import gradio as gr
import os
import json
import pandas as pd
import openai
from pathlib import Path
from typing import Dict, Any, Optional, List, Tuple
import logging
from dataclasses import dataclass
import re
from urllib.parse import urlparse
from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type
import threading
import time
import uuid
from concurrent.futures import ThreadPoolExecutor, as_completed
from queue import Queue

# å¯é€‰ä¾èµ–
try:
    import docx
    HAS_DOCX = True
except ImportError:
    HAS_DOCX = False

try:
    import PyPDF2
    HAS_PDF = True
except ImportError:
    HAS_PDF = False

try:
    import chardet
    HAS_CHARDET = True
except ImportError:
    HAS_CHARDET = False

# é…ç½®æ—¥å¿— - åŒæ—¶è¾“å‡ºåˆ°æ§åˆ¶å°å’Œæ–‡ä»¶
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),  # è¾“å‡ºåˆ°æ§åˆ¶å°
        logging.FileHandler('ai_text_processing.log', encoding='utf-8')  # è¾“å‡ºåˆ°æ–‡ä»¶
    ]
)
logger = logging.getLogger(__name__)

# ==================== AIæ¨¡å‹é…ç½® ====================

@dataclass
class ModelConfig:
    """AIæ¨¡å‹é…ç½®ç±»"""
    name: str
    base_url: str
    api_key: str
    model_name: str
    timeout: int = 60
    max_retries: int = 3
    temperature: float = 0.7
    max_tokens: Optional[int] = None
    
    def __post_init__(self):
        """é…ç½®éªŒè¯"""
        if not self.base_url or not self.api_key or not self.model_name:
            raise ValueError("base_url, api_key, model_name ä¸èƒ½ä¸ºç©º")
        
        # éªŒè¯URLæ ¼å¼
        parsed = urlparse(self.base_url)
        if not parsed.scheme or not parsed.netloc:
            raise ValueError("base_url æ ¼å¼æ— æ•ˆ")

class AIModelClient:
    """AIæ¨¡å‹å®¢æˆ·ç«¯"""
    @classmethod
    def load_custom_configs(cls) -> Dict[str, Dict]:
        """åŠ è½½è‡ªå®šä¹‰æ¨¡å‹é…ç½®"""
        config_file = "model_configs.json"
        try:
            if os.path.exists(config_file):
                with open(config_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
        except Exception as e:
            logger.warning(f"åŠ è½½æ¨¡å‹é…ç½®å¤±è´¥: {str(e)}")
        return {}
    
    @classmethod
    def save_custom_config(cls, name: str, config: Dict[str, Any]):
        """ä¿å­˜è‡ªå®šä¹‰æ¨¡å‹é…ç½®"""
        config_file = "model_configs.json"
        try:
            configs = cls.load_custom_configs()
            configs[name] = config
            with open(config_file, 'w', encoding='utf-8') as f:
                json.dump(configs, f, ensure_ascii=False, indent=2)
            logger.info(f"æ¨¡å‹é…ç½® {name} å·²ä¿å­˜")
        except Exception as e:
            logger.error(f"ä¿å­˜æ¨¡å‹é…ç½®å¤±è´¥: {str(e)}")
    
    @classmethod
    def get_all_configs(cls) -> Dict[str, Dict]:
        """è·å–æ‰€æœ‰æ¨¡å‹é…ç½®ï¼ˆé¢„è®¾+è‡ªå®šä¹‰ï¼‰"""
        # é¢„è®¾æ¨¡å‹é…ç½®
        all_configs = {
            "OpenAI GPT-4": {
                "base_url": "https://api.openai.com/v1",
                "model_name": "gpt-4",
                "api_key": "your-openai-api-key"
            },
            "OpenAI GPT-3.5": {
                "base_url": "https://api.openai.com/v1",
                "model_name": "gpt-3.5-turbo",
                "api_key": "your-openai-api-key"
            },
            "Claude-3": {
                "base_url": "https://api.anthropic.com/v1",
                "model_name": "claude-3-sonnet-20240229",
                "api_key": "your-anthropic-api-key"
            }
        }
        
        # åŠ è½½è‡ªå®šä¹‰é…ç½®å¹¶åˆå¹¶
        custom_configs = cls.load_custom_configs()
        all_configs.update(custom_configs)
        return all_configs
    
    def __init__(self, config: ModelConfig):
        self.config = config
        self.client = openai.OpenAI(
            base_url=config.base_url,
            api_key=config.api_key,
            timeout=config.timeout,
            max_retries=config.max_retries
        )
        logger.info(f"AIæ¨¡å‹å®¢æˆ·ç«¯å·²åˆå§‹åŒ–: {config.name}")
    
    @retry(
        stop=stop_after_attempt(3),
        wait=wait_exponential(multiplier=1, min=4, max=10),
        retry=retry_if_exception_type((openai.RateLimitError, openai.APITimeoutError))
    )
    def process_text(self, text: str, prompt: str) -> str:
        """ä½¿ç”¨AIæ¨¡å‹å¤„ç†æ–‡æœ¬"""
        try:
            messages = [
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡æœ¬å¤„ç†åŠ©æ‰‹ï¼Œè¯·æ ¹æ®ç”¨æˆ·çš„è¦æ±‚å¤„ç†æ–‡æœ¬å†…å®¹ã€‚"},
                {"role": "user", "content": f"è¯·æ ¹æ®ä»¥ä¸‹è¦æ±‚å¤„ç†æ–‡æœ¬ï¼š\n\nè¦æ±‚ï¼š{prompt}\n\næ–‡æœ¬å†…å®¹ï¼š{text}"}
            ]
            
            response = self.client.chat.completions.create(
                model=self.config.model_name,
                messages=messages,
                temperature=self.config.temperature,
                max_tokens=self.config.max_tokens
            )
            
            return response.choices[0].message.content.strip()
            
        except Exception as e:
            logger.error(f"AIå¤„ç†å¤±è´¥: {str(e)}")
            raise
    
    def test_connection(self) -> bool:
        """æµ‹è¯•è¿æ¥"""
        try:
            response = self.process_text("æµ‹è¯•", "è¯·å›å¤'è¿æ¥æˆåŠŸ'")
            return "è¿æ¥æˆåŠŸ" in response or "æˆåŠŸ" in response
        except Exception:
            return False

# ==================== æ–‡ä»¶å¤„ç†æ¨¡å— ====================

class FileProcessor:
    """æ–‡ä»¶å¤„ç†å™¨"""
    
    @staticmethod
    def detect_encoding(file_path: str) -> str:
        """æ£€æµ‹æ–‡ä»¶ç¼–ç """
        # å¦‚æœæœ‰chardetåº“ï¼Œä¼˜å…ˆä½¿ç”¨
        if HAS_CHARDET:
            try:
                with open(file_path, 'rb') as f:
                    raw_data = f.read(10000)  # è¯»å–å‰10KBç”¨äºæ£€æµ‹
                result = chardet.detect(raw_data)
                if result['encoding'] and result['confidence'] > 0.7:
                    return result['encoding']
            except Exception:
                pass
        
        # å¤‡ç”¨æ–¹æ¡ˆï¼šå°è¯•å¸¸è§ç¼–ç 
        encodings = ['utf-8', 'utf-8-sig', 'gbk', 'gb2312', 'gb18030', 'big5', 'latin1']
        
        for encoding in encodings:
            try:
                with open(file_path, 'r', encoding=encoding) as f:
                    f.read()
                return encoding
            except (UnicodeDecodeError, UnicodeError):
                continue
        
        return 'utf-8'  # é»˜è®¤è¿”å›utf-8
    
    @staticmethod
    def read_txt_file(file_path: str) -> pd.DataFrame:
        """è¯»å–TXTæ–‡ä»¶"""
        try:
            encoding = FileProcessor.detect_encoding(file_path)
            with open(file_path, 'r', encoding=encoding) as f:
                content = f.read()
            
            # æŒ‰è¡Œåˆ†å‰²ï¼Œåˆ›å»ºDataFrame
            lines = [line.strip() for line in content.split('\n') if line.strip()]
            df = pd.DataFrame({'content': lines})
            return df
            
        except Exception as e:
            raise Exception(f"è¯»å–TXTæ–‡ä»¶å¤±è´¥: {str(e)}")
    
    @staticmethod
    def read_md_file(file_path: str) -> pd.DataFrame:
        """è¯»å–Markdownæ–‡ä»¶"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # æŒ‰æ®µè½åˆ†å‰²ï¼ˆåŒæ¢è¡Œç¬¦ï¼‰
            paragraphs = [p.strip() for p in content.split('\n\n') if p.strip()]
            df = pd.DataFrame({'content': paragraphs})
            return df
            
        except Exception as e:
            raise Exception(f"è¯»å–Markdownæ–‡ä»¶å¤±è´¥: {str(e)}")
    
    @staticmethod
    def read_excel_file(file_path: str) -> Tuple[pd.DataFrame, List[str]]:
        """è¯»å–Excelæ–‡ä»¶ï¼Œè¿”å›æ•°æ®å’Œå·¥ä½œè¡¨åç§°"""
        try:
            # è·å–æ‰€æœ‰å·¥ä½œè¡¨åç§°
            excel_file = pd.ExcelFile(file_path)
            sheet_names = excel_file.sheet_names
            
            # è¯»å–ç¬¬ä¸€ä¸ªå·¥ä½œè¡¨
            df = pd.read_excel(file_path, sheet_name=sheet_names[0])
            
            return df, sheet_names
            
        except Exception as e:
            raise Exception(f"è¯»å–Excelæ–‡ä»¶å¤±è´¥: {str(e)}")
    
    @staticmethod
    def read_csv_file(file_path: str) -> pd.DataFrame:
        """è¯»å–CSVæ–‡ä»¶"""
        try:
            encoding = FileProcessor.detect_encoding(file_path)
            
            # å°è¯•ä¸åŒçš„åˆ†éš”ç¬¦
            separators = [',', ';', '\t', '|']
            
            for sep in separators:
                try:
                    df = pd.read_csv(file_path, encoding=encoding, sep=sep)
                    # æ£€æŸ¥æ˜¯å¦æˆåŠŸè§£æï¼ˆè‡³å°‘æœ‰2åˆ—æˆ–å¤šè¡Œï¼‰
                    if len(df.columns) > 1 or len(df) > 1:
                        return df
                except Exception:
                    continue
            
            # å¦‚æœéƒ½å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤é€—å·åˆ†éš”ç¬¦
            df = pd.read_csv(file_path, encoding=encoding)
            return df
            
        except Exception as e:
            raise Exception(f"è¯»å–CSVæ–‡ä»¶å¤±è´¥: {str(e)}")
    
    @staticmethod
    def read_pdf_file(file_path: str) -> pd.DataFrame:
        """è¯»å–PDFæ–‡ä»¶"""
        if not HAS_PDF:
            raise Exception("éœ€è¦å®‰è£…PyPDF2åº“æ¥è¯»å–PDFæ–‡ä»¶: pip install PyPDF2")
        
        try:
            with open(file_path, 'rb') as f:
                pdf_reader = PyPDF2.PdfReader(f)
                text_content = []
                
                for page_num in range(len(pdf_reader.pages)):
                    page = pdf_reader.pages[page_num]
                    text = page.extract_text()
                    if text.strip():
                        text_content.append(text.strip())
                
                df = pd.DataFrame({'content': text_content})
                return df
                
        except Exception as e:
            raise Exception(f"è¯»å–PDFæ–‡ä»¶å¤±è´¥: {str(e)}")
    
    @staticmethod
    def read_docx_file(file_path: str) -> pd.DataFrame:
        """è¯»å–Wordæ–‡æ¡£"""
        if not HAS_DOCX:
            raise Exception("éœ€è¦å®‰è£…python-docxåº“æ¥è¯»å–Wordæ–‡æ¡£: pip install python-docx")
        
        try:
            doc = docx.Document(file_path)
            paragraphs = []
            
            for paragraph in doc.paragraphs:
                text = paragraph.text.strip()
                if text:
                    paragraphs.append(text)
            
            df = pd.DataFrame({'content': paragraphs})
            return df
            
        except Exception as e:
            raise Exception(f"è¯»å–Wordæ–‡æ¡£å¤±è´¥: {str(e)}")
    
    @staticmethod
    def save_file(df: pd.DataFrame, original_path: str, suffix: str = "_processed") -> str:
        """ä¿å­˜å¤„ç†åçš„æ–‡ä»¶åˆ°åŸæ–‡ä»¶ç›®å½•"""
        try:
            original_path = Path(original_path)
            output_dir = original_path.parent
            
            # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶å
            base_name = original_path.stem
            extension = original_path.suffix.lower()
            
            # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
            output_dir.mkdir(parents=True, exist_ok=True)
            
            output_file = output_dir / f"{base_name}{suffix}{extension}"
            
            # æ ¹æ®æ–‡ä»¶ç±»å‹ä¿å­˜
            if extension == '.csv':
                df.to_csv(output_file, index=False, encoding='utf-8-sig')
            elif extension in ['.xlsx', '.xls']:
                df.to_excel(output_file, index=False)
            else:  # txt, mdç­‰æ–‡æœ¬æ–‡ä»¶
                # å¦‚æœåªæœ‰ä¸€åˆ—ï¼Œç›´æ¥ä¿å­˜å†…å®¹
                if len(df.columns) == 1:
                    content = '\n'.join(df.iloc[:, 0].astype(str))
                else:
                    # å¤šåˆ—æ—¶ä¿å­˜ä¸ºCSVæ ¼å¼
                    content = df.to_csv(index=False, sep='\t')
                
                with open(output_file, 'w', encoding='utf-8') as f:
                    f.write(content)
            
            return str(output_file)
            
        except Exception as e:
            raise Exception(f"ä¿å­˜æ–‡ä»¶å¤±è´¥: {str(e)}")

# ==================== æç¤ºè¯ä»»åŠ¡ç®¡ç† ====================

class PromptTaskManager:
    """æç¤ºè¯ä»»åŠ¡ç®¡ç†å™¨"""
    
    def __init__(self):
        self.tasks_file = "prompt_tasks.json"
        self.default_tasks = {
            "æ–‡æœ¬æ‘˜è¦": "è¯·ä¸ºä»¥ä¸‹æ–‡æœ¬ç”Ÿæˆä¸€ä¸ªç®€æ´çš„æ‘˜è¦ï¼Œçªå‡ºä¸»è¦å†…å®¹å’Œå…³é”®ä¿¡æ¯ã€‚",
            "å…³é”®è¯æå–": "è¯·ä»ä»¥ä¸‹æ–‡æœ¬ä¸­æå–5-10ä¸ªå…³é”®è¯ï¼Œç”¨é€—å·åˆ†éš”ã€‚",
            "æƒ…æ„Ÿåˆ†æ": "è¯·åˆ†æä»¥ä¸‹æ–‡æœ¬çš„æƒ…æ„Ÿå€¾å‘ï¼Œå›ç­”ç§¯æã€æ¶ˆææˆ–ä¸­æ€§ï¼Œå¹¶ç®€è¦è¯´æ˜ç†ç”±ã€‚",
            "æ–‡æœ¬åˆ†ç±»": "è¯·å¯¹ä»¥ä¸‹æ–‡æœ¬è¿›è¡Œåˆ†ç±»ï¼Œå¹¶è¯´æ˜åˆ†ç±»ç†ç”±ã€‚",
            "é—®é¢˜ç”Ÿæˆ": "è¯·æ ¹æ®ä»¥ä¸‹æ–‡æœ¬å†…å®¹ç”Ÿæˆ3-5ä¸ªç›¸å…³é—®é¢˜ã€‚",
            "æ–‡æœ¬ç¿»è¯‘": "è¯·å°†ä»¥ä¸‹æ–‡æœ¬ç¿»è¯‘æˆè‹±æ–‡ã€‚",
            "å†…å®¹æ‰©å±•": "è¯·åŸºäºä»¥ä¸‹æ–‡æœ¬å†…å®¹è¿›è¡Œæ‰©å±•ï¼Œå¢åŠ æ›´å¤šç»†èŠ‚å’Œä¿¡æ¯ã€‚",
            "æ ¼å¼åŒ–": "è¯·å°†ä»¥ä¸‹æ–‡æœ¬è¿›è¡Œæ ¼å¼åŒ–æ•´ç†ï¼Œä½¿å…¶æ›´åŠ è§„èŒƒå’Œæ˜“è¯»ã€‚"
        }
        self.tasks = self.load_tasks()
    
    def load_tasks(self) -> Dict[str, str]:
        """åŠ è½½æç¤ºè¯ä»»åŠ¡"""
        tasks = self.default_tasks.copy()
        
        try:
            if os.path.exists(self.tasks_file):
                with open(self.tasks_file, 'r', encoding='utf-8') as f:
                    saved_tasks = json.load(f)
                # åˆå¹¶é»˜è®¤ä»»åŠ¡å’Œä¿å­˜çš„ä»»åŠ¡
                tasks.update(saved_tasks)
                logger.info(f"å·²åŠ è½½ {len(saved_tasks)} ä¸ªè‡ªå®šä¹‰ä»»åŠ¡")
        except Exception as e:
            logger.warning(f"åŠ è½½ä»»åŠ¡æ–‡ä»¶å¤±è´¥: {str(e)}")
        
        return tasks
    
    def save_tasks(self):
        """ä¿å­˜æç¤ºè¯ä»»åŠ¡ï¼ˆä¿å­˜æ‰€æœ‰è¢«ä¿®æ”¹çš„ä»»åŠ¡ï¼‰"""
        try:
            # ä¿å­˜æ‰€æœ‰ä»»åŠ¡ï¼ˆåŒ…æ‹¬è¢«ä¿®æ”¹çš„é»˜è®¤ä»»åŠ¡ï¼‰
            custom_tasks = {k: v for k, v in self.tasks.items() if k not in self.default_tasks or v != self.default_tasks.get(k)}
            with open(self.tasks_file, 'w', encoding='utf-8') as f:
                json.dump(custom_tasks, f, ensure_ascii=False, indent=2)
            logger.info(f"å·²ä¿å­˜ {len(custom_tasks)} ä¸ªä»»åŠ¡ï¼ˆåŒ…æ‹¬ä¿®æ”¹çš„é»˜è®¤ä»»åŠ¡ï¼‰")
        except Exception as e:
            logger.error(f"ä¿å­˜ä»»åŠ¡æ–‡ä»¶å¤±è´¥: {str(e)}")
    
    def add_task(self, name: str, prompt: str):
        """æ·»åŠ æˆ–æ›´æ–°ä»»åŠ¡"""
        self.tasks[name] = prompt
        self.save_tasks()
        logger.info(f"ä»»åŠ¡ '{name}' å·²ä¿å­˜")
    
    def delete_task(self, name: str) -> bool:
        """åˆ é™¤ä»»åŠ¡ï¼ˆé»˜è®¤ä»»åŠ¡ä¼šé‡ç½®ä¸ºåŸå§‹å€¼ï¼‰"""
        if name in self.default_tasks:
            # å¦‚æœæ˜¯é»˜è®¤ä»»åŠ¡ï¼Œé‡ç½®ä¸ºåŸå§‹å€¼
            self.tasks[name] = self.default_tasks[name]
            self.save_tasks()
            logger.info(f"é»˜è®¤ä»»åŠ¡ '{name}' å·²é‡ç½®ä¸ºåŸå§‹å€¼")
            return True
        
        if name in self.tasks:
            del self.tasks[name]
            self.save_tasks()
            logger.info(f"è‡ªå®šä¹‰ä»»åŠ¡ '{name}' å·²åˆ é™¤")
            return True
        return False
    
    def get_task_names(self) -> List[str]:
        """è·å–æ‰€æœ‰ä»»åŠ¡åç§°"""
        return list(self.tasks.keys())
    
    def get_task_prompt(self, name: str) -> str:
        """è·å–ä»»åŠ¡æç¤ºè¯"""
        return self.tasks.get(name, "")
    
    def is_default_task(self, name: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºé»˜è®¤ä»»åŠ¡"""
        return name in self.default_tasks
    
    def reload_tasks(self):
        """é‡æ–°åŠ è½½ä»»åŠ¡"""
        self.tasks = self.load_tasks()

# ==================== å…¨å±€å˜é‡ ====================

# å…¨å±€å˜é‡
current_model_client = None
current_dataframe = None
original_file_path = None
task_manager = PromptTaskManager()

# ==================== Gradioç•Œé¢å‡½æ•° ====================

def load_model(preset_name: str, custom_name: str, custom_base_url: str, 
               custom_api_key: str, custom_model_name: str, save_custom: bool = False) -> Tuple[str, str]:
    """åŠ è½½AIæ¨¡å‹"""
    global current_model_client
    
    try:
        if preset_name != "è‡ªå®šä¹‰":
            # ä½¿ç”¨é¢„è®¾æˆ–å·²ä¿å­˜çš„é…ç½®
            all_configs = AIModelClient.get_all_configs()
            if preset_name not in all_configs:
                return f"é”™è¯¯ï¼šæœªçŸ¥çš„æ¨¡å‹é…ç½® {preset_name}", ""
            
            config_dict = all_configs[preset_name].copy()
            config = ModelConfig(**config_dict)
        else:
            # ä½¿ç”¨è‡ªå®šä¹‰é…ç½®
            if not all([custom_name, custom_base_url, custom_api_key, custom_model_name]):
                return "é”™è¯¯ï¼šè‡ªå®šä¹‰é…ç½®ä¿¡æ¯ä¸å®Œæ•´", ""
            
            config = ModelConfig(
                name=custom_name,
                base_url=custom_base_url,
                api_key=custom_api_key,
                model_name=custom_model_name
            )
            
            # ä¿å­˜è‡ªå®šä¹‰é…ç½®
            if save_custom and custom_name:
                config_dict = {
                    "name": custom_name,
                    "base_url": custom_base_url,
                    "api_key": custom_api_key,
                    "model_name": custom_model_name
                }
                AIModelClient.save_custom_config(custom_name, config_dict)
        
        # åˆ›å»ºå®¢æˆ·ç«¯
        current_model_client = AIModelClient(config)
        
        # æµ‹è¯•è¿æ¥
        logger.info(f"æ­£åœ¨æµ‹è¯•æ¨¡å‹è¿æ¥: {config.name}")
        if current_model_client.test_connection():
            message = f"âœ… æ¨¡å‹ {config.name} åŠ è½½æˆåŠŸå¹¶è¿æ¥æ­£å¸¸"
            logger.info(message)
            # æ›´æ–°é…ç½®é€‰æ‹©åˆ—è¡¨
            updated_choices = list(AIModelClient.get_all_configs().keys()) + ["è‡ªå®šä¹‰"]
            return message, gr.Dropdown(choices=updated_choices)
        else:
            message = f"âš ï¸ æ¨¡å‹ {config.name} åŠ è½½æˆåŠŸä½†è¿æ¥æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥é…ç½®"
            logger.warning(message)
            updated_choices = list(AIModelClient.get_all_configs().keys()) + ["è‡ªå®šä¹‰"]
            return message, gr.Dropdown(choices=updated_choices)
            
    except Exception as e:
        error_msg = f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}"
        logger.error(error_msg)
        return error_msg, ""

def handle_file_upload(file) -> Tuple[str, str, gr.Dropdown]:
    """ä¸Šä¼ å¹¶è¯»å–æ–‡ä»¶"""
    global current_dataframe, original_file_path
    
    if file is None:
        return "è¯·é€‰æ‹©æ–‡ä»¶", "", gr.Dropdown(choices=[], visible=False)
    
    try:
        file_path = file.name
        original_file_path = file_path
        file_extension = Path(file_path).suffix.lower()
        
        # æ ¹æ®æ–‡ä»¶ç±»å‹è¯»å–
        if file_extension == '.txt':
            df = FileProcessor.read_txt_file(file_path)
        elif file_extension == '.md':
            df = FileProcessor.read_md_file(file_path)
        elif file_extension in ['.xlsx', '.xls']:
            df, sheet_names = FileProcessor.read_excel_file(file_path)
        elif file_extension == '.csv':
            df = FileProcessor.read_csv_file(file_path)
        elif file_extension == '.pdf':
            df = FileProcessor.read_pdf_file(file_path)
        elif file_extension in ['.docx', '.doc']:
            df = FileProcessor.read_docx_file(file_path)
        else:
            return f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file_extension}\næ”¯æŒçš„æ ¼å¼: .txt, .md, .csv, .xlsx, .xls, .pdf, .docx", "", gr.Dropdown(choices=[], visible=False)
        
        current_dataframe = df
        
        # ç”Ÿæˆåˆ—é€‰æ‹©é€‰é¡¹ï¼ˆæ”¯æŒå¤šé€‰ï¼‰
        columns = df.columns.tolist()
        
        # å¯¹äºè¡¨æ ¼æ–‡ä»¶ï¼ˆExcel/CSVï¼‰ï¼Œæ˜¾ç¤ºåˆ—é€‰æ‹©ï¼›å¯¹äºæ–‡æœ¬æ–‡ä»¶ï¼Œéšè—åˆ—é€‰æ‹©
        if file_extension in ['.xlsx', '.xls', '.csv']:
            column_dropdown = gr.Dropdown(
                choices=columns, 
                value=None,
                label="é€‰æ‹©è¦å¤„ç†çš„åˆ—",
                multiselect=True,
                visible=True
            )
        else:
            # å¯¹äºæ–‡æœ¬æ–‡ä»¶ï¼Œé»˜è®¤é€‰æ‹©ç¬¬ä¸€åˆ—
            column_dropdown = gr.Dropdown(
                choices=columns,
                value=[columns[0]] if columns else None,
                label="é€‰æ‹©è¦å¤„ç†çš„åˆ—",
                multiselect=True,
                visible=False
            )
        
        # ç”Ÿæˆé¢„è§ˆ
        preview = df.head(10).to_string(index=False, max_cols=5, max_colwidth=50)
        
        # æ·»åŠ æ–‡ä»¶ä¿¡æ¯
        file_info = f"æ–‡ä»¶ç±»å‹: {file_extension}\næ–‡ä»¶å¤§å°: {os.path.getsize(file_path) / 1024:.1f} KB\næ•°æ®è¡Œæ•°: {len(df)}\næ•°æ®åˆ—æ•°: {len(df.columns)}"
        
        return f"âœ… æ–‡ä»¶è¯»å–æˆåŠŸ\n{file_info}", preview, column_dropdown
        
    except Exception as e:
        return f"âŒ æ–‡ä»¶è¯»å–å¤±è´¥: {str(e)}", "", gr.Dropdown(choices=[], visible=False)

def get_task_prompt(task_name: str) -> str:
    """è·å–ä»»åŠ¡æç¤ºè¯"""
    return task_manager.get_task_prompt(task_name)

def add_custom_task(task_name: str, task_prompt: str) -> Tuple[str, gr.Dropdown, gr.Dropdown]:
    """æ·»åŠ è‡ªå®šä¹‰ä»»åŠ¡"""
    if not task_name or not task_prompt:
        task_choices = task_manager.get_task_names()
        return "ä»»åŠ¡åç§°å’Œæç¤ºè¯ä¸èƒ½ä¸ºç©º", gr.Dropdown(choices=task_choices), gr.Dropdown(choices=task_choices)
    
    try:
        task_manager.add_task(task_name, task_prompt)
        task_choices = task_manager.get_task_names()
        updated_dropdown1 = gr.Dropdown(choices=task_choices, value=task_name)
        updated_dropdown2 = gr.Dropdown(choices=task_choices, value=task_name)
        return f"âœ… ä»»åŠ¡ '{task_name}' æ·»åŠ æˆåŠŸ", updated_dropdown1, updated_dropdown2
    except Exception as e:
        task_choices = task_manager.get_task_names()
        return f"âŒ æ·»åŠ ä»»åŠ¡å¤±è´¥: {str(e)}", gr.Dropdown(choices=task_choices), gr.Dropdown(choices=task_choices)

def delete_task(task_name: str) -> Tuple[str, gr.Dropdown, gr.Dropdown]:
    """åˆ é™¤ä»»åŠ¡ï¼ˆé»˜è®¤ä»»åŠ¡ä¼šé‡ç½®ä¸ºåŸå§‹å€¼ï¼‰"""
    if not task_name:
        task_choices = task_manager.get_task_names()
        return "è¯·é€‰æ‹©è¦åˆ é™¤çš„ä»»åŠ¡", gr.Dropdown(choices=task_choices), gr.Dropdown(choices=task_choices)
    
    try:
        is_default = task_manager.is_default_task(task_name)
        if task_manager.delete_task(task_name):
            task_choices = task_manager.get_task_names()
            updated_dropdown1 = gr.Dropdown(choices=task_choices)
            updated_dropdown2 = gr.Dropdown(choices=task_choices)
            if is_default:
                return f"âœ… é»˜è®¤ä»»åŠ¡ '{task_name}' å·²é‡ç½®ä¸ºåŸå§‹å€¼", updated_dropdown1, updated_dropdown2
            else:
                return f"âœ… è‡ªå®šä¹‰ä»»åŠ¡ '{task_name}' åˆ é™¤æˆåŠŸ", updated_dropdown1, updated_dropdown2
        else:
            task_choices = task_manager.get_task_names()
            return f"âŒ æ— æ³•åˆ é™¤ä»»åŠ¡ '{task_name}'", gr.Dropdown(choices=task_choices), gr.Dropdown(choices=task_choices)
    except Exception as e:
        task_choices = task_manager.get_task_names()
        return f"âŒ åˆ é™¤ä»»åŠ¡å¤±è´¥: {str(e)}", gr.Dropdown(choices=task_choices), gr.Dropdown(choices=task_choices)

def edit_task(task_name: str, new_prompt: str) -> Tuple[str, gr.Dropdown, gr.Dropdown]:
    """ç¼–è¾‘ä»»åŠ¡æç¤ºè¯"""
    if not task_name:
        task_choices = task_manager.get_task_names()
        return "è¯·é€‰æ‹©è¦ç¼–è¾‘çš„ä»»åŠ¡", gr.Dropdown(choices=task_choices), gr.Dropdown(choices=task_choices)
    
    if not new_prompt.strip():
        task_choices = task_manager.get_task_names()
        return "æç¤ºè¯ä¸èƒ½ä¸ºç©º", gr.Dropdown(choices=task_choices), gr.Dropdown(choices=task_choices)
    
    try:
        task_manager.add_task(task_name, new_prompt.strip())
        task_choices = task_manager.get_task_names()
        updated_dropdown1 = gr.Dropdown(choices=task_choices, value=task_name)
        updated_dropdown2 = gr.Dropdown(choices=task_choices, value=task_name)
        is_default = task_manager.is_default_task(task_name)
        if is_default:
            return f"âœ… é»˜è®¤ä»»åŠ¡ '{task_name}' ä¿®æ”¹æˆåŠŸ", updated_dropdown1, updated_dropdown2
        else:
            return f"âœ… ä»»åŠ¡ '{task_name}' ä¿®æ”¹æˆåŠŸ", updated_dropdown1, updated_dropdown2
    except Exception as e:
        task_choices = task_manager.get_task_names()
        return f"âŒ ç¼–è¾‘ä»»åŠ¡å¤±è´¥: {str(e)}", gr.Dropdown(choices=task_choices), gr.Dropdown(choices=task_choices)

def reload_tasks() -> Tuple[str, gr.Dropdown, gr.Dropdown]:
    """é‡æ–°åŠ è½½ä»»åŠ¡"""
    try:
        task_manager.reload_tasks()
        task_choices = task_manager.get_task_names()
        updated_dropdown1 = gr.Dropdown(choices=task_choices)
        updated_dropdown2 = gr.Dropdown(choices=task_choices)
        return "âœ… ä»»åŠ¡åˆ—è¡¨å·²é‡æ–°åŠ è½½", updated_dropdown1, updated_dropdown2
    except Exception as e:
        task_choices = task_manager.get_task_names()
        return f"âŒ é‡æ–°åŠ è½½å¤±è´¥: {str(e)}", gr.Dropdown(choices=task_choices), gr.Dropdown(choices=task_choices)

def process_data_stream(file_upload, selected_columns, task_name: str, 
                        batch_size: int = 10, max_workers: int = 3, 
                        save_location: str = "å½“å‰æ–‡ä»¶çš„outputç›®å½•", custom_save_path: str = ""):
    """æµå¼å¤„ç†æ•°æ®ï¼ˆæ”¯æŒå¤šçº¿ç¨‹å’Œå¤šåˆ—é€‰æ‹©ï¼Œå®æ—¶è¿›åº¦æ˜¾ç¤ºï¼‰"""
    global current_model_client, current_dataframe, original_file_path
    
    if current_model_client is None:
        yield "âŒ è¯·å…ˆåŠ è½½AIæ¨¡å‹", "", 0.0
        return
    
    if current_dataframe is None:
        yield "âŒ è¯·å…ˆä¸Šä¼ æ–‡ä»¶", "", 0.0
        return
    
    if not selected_columns:
        yield "âŒ è¯·é€‰æ‹©è¦å¤„ç†çš„åˆ—", "", 0.0
        return
    
    if not task_name:
        yield "âŒ è¯·é€‰æ‹©å¤„ç†ä»»åŠ¡", "", 0.0
        return
    
    try:
        # è·å–ä»»åŠ¡æç¤ºè¯
        prompt = task_manager.get_task_prompt(task_name)
        if not prompt:
            yield "âŒ é€‰æ‹©çš„ä»»åŠ¡æ— æ•ˆ", "", 0.0
            return
        
        # å¤„ç†å¤šåˆ—é€‰æ‹©
        if isinstance(selected_columns, str):
            columns_to_process = [selected_columns]
        else:
            columns_to_process = selected_columns if selected_columns else []
        
        if not columns_to_process:
            yield "âŒ è¯·é€‰æ‹©è¦å¤„ç†çš„åˆ—", "", 0.0
            return
        
        # éªŒè¯åˆ—æ˜¯å¦å­˜åœ¨
        missing_columns = [col for col in columns_to_process if col not in current_dataframe.columns]
        if missing_columns:
            yield f"âŒ ä»¥ä¸‹åˆ—ä¸å­˜åœ¨: {', '.join(missing_columns)}", "", 0.0
            return
        
        total_processed = 0
        processing_log = []
        overall_progress = 0.0
        total_start_time = time.time()  # è®°å½•æ€»å¼€å§‹æ—¶é—´
        
        # è®¡ç®—æ€»çš„å¤„ç†é¡¹ç›®æ•°
        total_items_all_columns = 0
        for column in columns_to_process:
            data_to_process = current_dataframe[column].astype(str).tolist()
            indexed_data = [(i, item) for i, item in enumerate(data_to_process) if item.strip()]
            total_items_all_columns += len(indexed_data)
        
        # åˆå§‹åŒ–ä¿¡æ¯
        processing_log.append(f"ğŸ“Š å¼€å§‹å¤„ç† {len(columns_to_process)} åˆ—ï¼Œå…± {total_items_all_columns} æ¡æ•°æ®")
        processing_log.append(f"âš™ï¸ ä½¿ç”¨ {max_workers} ä¸ªçº¿ç¨‹å¹¶å‘å¤„ç†")
        processing_log.append(f"ğŸ“ é»˜è®¤è¾“å‡ºç›®å½•: {Path(__file__).parent / 'output'}")
        processing_log.append(f"ğŸ“‚ ä¿å­˜ä½ç½®è®¾ç½®: {save_location}")
        if save_location == "è‡ªå®šä¹‰ç›®å½•" and custom_save_path.strip():
            processing_log.append(f"ğŸ“ è‡ªå®šä¹‰è·¯å¾„: {custom_save_path.strip()}")
        processing_log.append(f"ğŸ“ å¤„ç†ä»»åŠ¡: {task_name}")
        processing_log.append("" + "="*50)
        
        # è¾“å‡ºåˆå§‹çŠ¶æ€
        yield "\n".join(processing_log), "", 0.0
        
        global_processed_count = 0
        
        # å¤„ç†æ¯ä¸€åˆ—
        for col_index, column in enumerate(columns_to_process):
            processing_log.append(f"\nğŸ”„ æ­£åœ¨å¤„ç†åˆ—: {column} ({col_index + 1}/{len(columns_to_process)})")
            processing_log.append(f"â° å¼€å§‹æ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}")
            
            # è·å–è¦å¤„ç†çš„æ•°æ®
            data_to_process = current_dataframe[column].astype(str).tolist()
            
            # è¿‡æ»¤ç©ºå€¼å¹¶ä¿å­˜åŸå§‹ç´¢å¼•
            indexed_data = [(i, item) for i, item in enumerate(data_to_process) if item.strip()]
            
            if not indexed_data:
                processing_log.append(f"âš ï¸ åˆ— {column} ä¸­æ²¡æœ‰æœ‰æ•ˆæ•°æ®ï¼Œè·³è¿‡")
                yield "\n".join(processing_log), "", overall_progress
                continue
            
            total_items = len(indexed_data)
            processing_log.append(f"ğŸ“ è¯¥åˆ—æœ‰æ•ˆæ•°æ®: {total_items} æ¡")
            processing_log.append(f"ğŸ“Š æ•°æ®é•¿åº¦ç»Ÿè®¡: å¹³å‡ {sum(len(str(item[1])) for item in indexed_data) / len(indexed_data):.0f} å­—ç¬¦")
            processing_log.append(f"ğŸš€ å¼€å§‹å¹¶å‘å¤„ç†ï¼Œçº¿ç¨‹æ•°: {max_workers}")
            processing_log.append("-" * 40)
            
            # è¾“å‡ºåˆ—å¼€å§‹å¤„ç†çŠ¶æ€
            yield "\n".join(processing_log), "", overall_progress
            
            # åˆ›å»ºç»“æœå­—å…¸
            results_dict = {}
            processed_count = 0
            start_time = time.time()
            
            def process_single_item(indexed_item):
                """å¤„ç†å•ä¸ªæ–‡æœ¬é¡¹"""
                index, item = indexed_item
                try:
                    # é™åˆ¶å•ä¸ªæ–‡æœ¬é•¿åº¦
                    if len(item) > 10000:
                        item = item[:10000] + "...[æ–‡æœ¬è¿‡é•¿ï¼Œå·²æˆªæ–­]"
                    
                    result = current_model_client.process_text(item, prompt)
                    return index, result, True
                except Exception as e:
                    error_msg = f"å¤„ç†å¤±è´¥: {str(e)}"
                    logger.error(f"å¤„ç†ç¬¬ {index+1} é¡¹å¤±è´¥: {str(e)}")
                    return index, error_msg, False
            
            # ä½¿ç”¨çº¿ç¨‹æ± è¿›è¡Œå¹¶å‘å¤„ç†
            with ThreadPoolExecutor(max_workers=max_workers) as executor:
                # æäº¤æ‰€æœ‰ä»»åŠ¡
                future_to_index = {executor.submit(process_single_item, item): item[0] for item in indexed_data}
                
                # æ”¶é›†ç»“æœ
                for future in as_completed(future_to_index):
                    try:
                        index, result, success = future.result()
                        results_dict[index] = result
                        processed_count += 1
                        global_processed_count += 1
                        
                        # è®¡ç®—è¿›åº¦
                        column_progress = processed_count / total_items * 100
                        overall_progress = global_processed_count / total_items_all_columns * 100
                        
                        # è®¡ç®—å¤„ç†é€Ÿåº¦
                        elapsed_time = time.time() - start_time
                        if elapsed_time > 0:
                            speed = processed_count / elapsed_time
                            remaining_items = total_items - processed_count
                            eta = remaining_items / speed if speed > 0 else 0
                            
                            # å®æ—¶è¯¦ç»†çš„è¿›åº¦æ˜¾ç¤ºï¼ˆæ¯å¤„ç†ä¸€ä¸ªå°±æ›´æ–°ï¼‰
                            if processed_count % 1 == 0 or processed_count == total_items:
                                current_log = processing_log.copy()
                                status_msg = (
                                    f"ğŸ”„ å®æ—¶çŠ¶æ€ | åˆ—: {column} ({col_index + 1}/{len(columns_to_process)}) | "
                                    f"å½“å‰åˆ—è¿›åº¦: {processed_count}/{total_items} ({column_progress:.1f}%) | "
                                    f"æ€»ä½“è¿›åº¦: {global_processed_count}/{total_items_all_columns} ({overall_progress:.1f}%) | "
                                    f"å¤„ç†é€Ÿåº¦: {speed:.1f}æ¡/ç§’ | é¢„è®¡å‰©ä½™æ—¶é—´: {eta:.0f}ç§’"
                                )
                                current_log.append(status_msg)
                                
                                # æ˜¾ç¤ºæœ€è¿‘å¤„ç†çš„å†…å®¹é¢„è§ˆï¼ˆæˆåŠŸçš„æƒ…å†µï¼‰
                                if success and len(result) > 0:
                                    preview_text = result[:50] + "..." if len(result) > 50 else result
                                    current_log.append(f"   âœ… æœ€æ–°å¤„ç†ç»“æœé¢„è§ˆ: {preview_text}")
                                elif not success:
                                    current_log.append(f"   âŒ å¤„ç†å¤±è´¥: {result}")
                                
                                # å®æ—¶è¾“å‡ºçŠ¶æ€
                                yield "\n".join(current_log), "", overall_progress
                        
                    except Exception as e:
                        logger.error(f"è·å–å¤„ç†ç»“æœå¤±è´¥: {str(e)}")
                        global_processed_count += 1
            
            # æ„å»ºå®Œæ•´çš„ç»“æœåˆ—è¡¨
            full_results = []
            for i, original_item in enumerate(data_to_process):
                if i in results_dict:
                    full_results.append(results_dict[i])
                else:
                    full_results.append("" if not original_item.strip() else "å¤„ç†å¤±è´¥")
            
            # æ·»åŠ ç»“æœåˆ—åˆ°DataFrame
            result_column_name = f"{column}_processed"
            current_dataframe[result_column_name] = full_results
            
            successful_count = len([r for r in full_results if r and not r.startswith('å¤„ç†å¤±è´¥')])
            failed_count = total_items - successful_count
            total_processed += successful_count
            
            # è®¡ç®—è¯¥åˆ—çš„å¤„ç†æ—¶é—´
            column_end_time = time.time()
            column_duration = column_end_time - start_time
            
            processing_log.append("-" * 40)
            processing_log.append(f"âœ… åˆ— {column} å¤„ç†å®Œæˆç»Ÿè®¡:")
            processing_log.append(f"   ğŸ“Š æˆåŠŸå¤„ç†: {successful_count} æ¡ ({successful_count/total_items*100:.1f}%)")
            processing_log.append(f"   âŒ å¤„ç†å¤±è´¥: {failed_count} æ¡ ({failed_count/total_items*100:.1f}%)")
            processing_log.append(f"   â±ï¸ å¤„ç†è€—æ—¶: {column_duration:.1f} ç§’")
            processing_log.append(f"   ğŸš€ å¹³å‡é€Ÿåº¦: {total_items/column_duration:.1f} æ¡/ç§’")
            processing_log.append(f"   â° å®Œæˆæ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}")
            processing_log.append(f"   ğŸ“ˆ ç´¯è®¡å®Œæˆ: {total_processed}/{total_items_all_columns} æ¡")
            
            # è¾“å‡ºåˆ—å®ŒæˆçŠ¶æ€
            yield "\n".join(processing_log), "", overall_progress
        
        # ä¿å­˜ç»“æœ
        processing_log.append("\n" + "="*50)
        processing_log.append("ğŸ’¾ æ­£åœ¨ä¿å­˜å¤„ç†ç»“æœ...")
        processing_log.append(f"ğŸ“ è¾“å‡ºç›®å½•: {Path(__file__).parent / 'output'}")
        
        # è¾“å‡ºä¿å­˜å¼€å§‹çŠ¶æ€
        yield "\n".join(processing_log), "", overall_progress
        
        if original_file_path:
            try:
                if save_location == "è‡ªå®šä¹‰ç›®å½•" and custom_save_path.strip():
                    # ä½¿ç”¨è‡ªå®šä¹‰ä¿å­˜è·¯å¾„
                    custom_dir = Path(custom_save_path.strip())
                    custom_dir.mkdir(parents=True, exist_ok=True)
                    
                    original_name = Path(original_file_path).stem
                    original_ext = Path(original_file_path).suffix
                    timestamp = time.strftime("%Y%m%d_%H%M%S")
                    output_filename = f"{original_name}_processed_{timestamp}{original_ext}"
                    output_path = custom_dir / output_filename
                    
                    processing_log.append(f"ğŸ“‚ ä½¿ç”¨è‡ªå®šä¹‰ç›®å½•: {custom_dir}")
                    
                    if original_ext.lower() in ['.xlsx', '.xls']:
                        current_dataframe.to_excel(str(output_path), index=False)
                        processing_log.append(f"ğŸ“Š Excelæ–‡ä»¶ä¿å­˜ä¸­...")
                    else:
                        current_dataframe.to_csv(str(output_path), index=False, encoding='utf-8-sig')
                        processing_log.append(f"ğŸ“„ CSVæ–‡ä»¶ä¿å­˜ä¸­...")
                else:
                    # ä½¿ç”¨é»˜è®¤outputç›®å½•ï¼ˆå½“å‰ä»£ç æ–‡ä»¶æ‰€åœ¨ç›®å½•ä¸‹çš„outputæ–‡ä»¶å¤¹ï¼‰
                    processing_log.append(f"ğŸ“‚ ä½¿ç”¨é»˜è®¤è¾“å‡ºç›®å½•: {Path(__file__).parent / 'output'}")
                    output_path = save_to_output_dir(current_dataframe, original_file_path)
                    processing_log.append(f"ğŸ’¾ æ–‡ä»¶ä¿å­˜ä¸­...")
                
                processing_log.append(f"âœ… ç»“æœå·²æˆåŠŸä¿å­˜åˆ°: {output_path}")
                processing_log.append(f"ğŸ“Š ä¿å­˜çš„æ•°æ®è¡Œæ•°: {len(current_dataframe)}")
                processing_log.append(f"ğŸ“‹ ä¿å­˜çš„æ•°æ®åˆ—æ•°: {len(current_dataframe.columns)}")
                
                # ç”Ÿæˆç»“æœé¢„è§ˆ
                result_preview = generate_enhanced_result_preview(current_dataframe, columns_to_process, total_processed)
                
                # è®¡ç®—æ€»å¤„ç†æ—¶é—´
                total_end_time = time.time()
                total_duration = total_end_time - total_start_time
                
                # ç”Ÿæˆæœ€ç»ˆç»Ÿè®¡ä¿¡æ¯
                processing_log.append("\n" + "="*50)
                processing_log.append("ğŸ¯ === æœ€ç»ˆå¤„ç†ç»Ÿè®¡æŠ¥å‘Š === ğŸ¯")
                processing_log.append(f"ğŸ“‹ å¤„ç†ä»»åŠ¡: {task_name}")
                processing_log.append(f"ğŸ“Š å¤„ç†åˆ—æ•°: {len(columns_to_process)} åˆ—")
                processing_log.append(f"ğŸ“ˆ æ€»æ•°æ®é‡: {total_items_all_columns} æ¡")
                processing_log.append(f"âœ… æˆåŠŸå¤„ç†: {total_processed} æ¡")
                processing_log.append(f"âŒ å¤±è´¥æ•°é‡: {total_items_all_columns - total_processed} æ¡")
                processing_log.append(f"ğŸ“Š æ€»æˆåŠŸç‡: {(total_processed/total_items_all_columns*100):.1f}%")
                processing_log.append(f"ğŸ”§ å¹¶å‘çº¿ç¨‹: {max_workers} ä¸ª")
                processing_log.append(f"â±ï¸ æ€»å¤„ç†æ—¶é—´: {total_duration:.1f} ç§’")
                processing_log.append(f"ğŸš€ å¹³å‡å¤„ç†é€Ÿåº¦: {total_items_all_columns/total_duration:.1f} æ¡/ç§’")
                processing_log.append(f"ğŸ’¾ è¾“å‡ºä½ç½®: {output_path}")
                processing_log.append(f"â° å®Œæˆæ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}")
                processing_log.append("="*50)
                processing_log.append(f"ğŸ‰ æ‰€æœ‰å¤„ç†ä»»åŠ¡å·²æˆåŠŸå®Œæˆï¼")
                
                final_message = "\n".join(processing_log)
                
                # æœ€ç»ˆè¾“å‡º
                yield final_message, result_preview, 100.0
                
            except Exception as e:
                logger.error(f"ä¿å­˜æ–‡ä»¶å¤±è´¥: {str(e)}")
                processing_log.append(f"âŒ ä¿å­˜å¤±è´¥: {str(e)}")
                
                result_preview = generate_enhanced_result_preview(current_dataframe, columns_to_process, total_processed)
                final_message = "\n".join(processing_log)
                final_message += f"\n\nâš ï¸ å¤„ç†å®Œæˆä½†ä¿å­˜å¤±è´¥ï¼Œå…±å¤„ç† {total_processed} æ¡æ•°æ®"
                
                yield final_message, result_preview, overall_progress
        else:
            processing_log.append(f"ğŸ‰ æ€»è®¡å¤„ç†å®Œæˆ {total_processed} æ¡æ•°æ®")
            result_preview = generate_enhanced_result_preview(current_dataframe, columns_to_process, total_processed)
            yield "\n".join(processing_log), result_preview, overall_progress
            
    except Exception as e:
        logger.error(f"æ•°æ®å¤„ç†å¤±è´¥: {str(e)}")
        yield f"âŒ å¤„ç†å¤±è´¥: {str(e)}", "", 0.0

def process_data(file_upload, selected_columns, task_name: str, 
                batch_size: int = 10, max_workers: int = 3, 
                save_location: str = "å½“å‰æ–‡ä»¶çš„outputç›®å½•", custom_save_path: str = "") -> Tuple[str, str, float]:
    """å¤„ç†æ•°æ®ï¼ˆæ”¯æŒå¤šçº¿ç¨‹å’Œå¤šåˆ—é€‰æ‹©ï¼Œå¸¦è¿›åº¦æ˜¾ç¤ºï¼‰"""
    global current_model_client, current_dataframe, original_file_path
    
    if current_model_client is None:
        return "âŒ è¯·å…ˆåŠ è½½AIæ¨¡å‹", "", 0.0
    
    if current_dataframe is None:
        return "âŒ è¯·å…ˆä¸Šä¼ æ–‡ä»¶", "", 0.0
    
    if not selected_columns:
        return "âŒ è¯·é€‰æ‹©è¦å¤„ç†çš„åˆ—", "", 0.0
    
    if not task_name:
        return "âŒ è¯·é€‰æ‹©å¤„ç†ä»»åŠ¡", "", 0.0
    
    try:
        # è·å–ä»»åŠ¡æç¤ºè¯
        prompt = task_manager.get_task_prompt(task_name)
        if not prompt:
            return "âŒ é€‰æ‹©çš„ä»»åŠ¡æ— æ•ˆ", "", 0.0
        
        # å¤„ç†å¤šåˆ—é€‰æ‹©
        if isinstance(selected_columns, str):
            columns_to_process = [selected_columns]
        else:
            columns_to_process = selected_columns if selected_columns else []
        
        if not columns_to_process:
            return "âŒ è¯·é€‰æ‹©è¦å¤„ç†çš„åˆ—", "", 0.0
        
        # éªŒè¯åˆ—æ˜¯å¦å­˜åœ¨
        missing_columns = [col for col in columns_to_process if col not in current_dataframe.columns]
        if missing_columns:
            return f"âŒ ä»¥ä¸‹åˆ—ä¸å­˜åœ¨: {', '.join(missing_columns)}", "", 0.0
        
        total_processed = 0
        processing_log = []
        overall_progress = 0.0
        total_start_time = time.time()  # è®°å½•æ€»å¼€å§‹æ—¶é—´
        
        # è®¡ç®—æ€»çš„å¤„ç†é¡¹ç›®æ•°
        total_items_all_columns = 0
        for column in columns_to_process:
            data_to_process = current_dataframe[column].astype(str).tolist()
            indexed_data = [(i, item) for i, item in enumerate(data_to_process) if item.strip()]
            total_items_all_columns += len(indexed_data)
        
        processing_log.append(f"ğŸ“Š å¼€å§‹å¤„ç† {len(columns_to_process)} åˆ—ï¼Œå…± {total_items_all_columns} æ¡æ•°æ®")
        processing_log.append(f"âš™ï¸ ä½¿ç”¨ {max_workers} ä¸ªçº¿ç¨‹å¹¶å‘å¤„ç†")
        processing_log.append(f"ğŸ“ é»˜è®¤è¾“å‡ºç›®å½•: {Path(__file__).parent / 'output'}")
        processing_log.append(f"ğŸ“‚ ä¿å­˜ä½ç½®è®¾ç½®: {save_location}")
        if save_location == "è‡ªå®šä¹‰ç›®å½•" and custom_save_path.strip():
            processing_log.append(f"ğŸ“ è‡ªå®šä¹‰è·¯å¾„: {custom_save_path.strip()}")
        processing_log.append(f"ğŸ“ å¤„ç†ä»»åŠ¡: {task_name}")
        processing_log.append("" + "="*50)
        
        global_processed_count = 0
        
        # å¤„ç†æ¯ä¸€åˆ—
        for col_index, column in enumerate(columns_to_process):
            processing_log.append(f"\nğŸ”„ æ­£åœ¨å¤„ç†åˆ—: {column} ({col_index + 1}/{len(columns_to_process)})")
            processing_log.append(f"â° å¼€å§‹æ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}")
            
            # è·å–è¦å¤„ç†çš„æ•°æ®
            data_to_process = current_dataframe[column].astype(str).tolist()
            
            # è¿‡æ»¤ç©ºå€¼å¹¶ä¿å­˜åŸå§‹ç´¢å¼•
            indexed_data = [(i, item) for i, item in enumerate(data_to_process) if item.strip()]
            
            if not indexed_data:
                processing_log.append(f"âš ï¸ åˆ— {column} ä¸­æ²¡æœ‰æœ‰æ•ˆæ•°æ®ï¼Œè·³è¿‡")
                continue
            
            total_items = len(indexed_data)
            processing_log.append(f"ğŸ“ è¯¥åˆ—æœ‰æ•ˆæ•°æ®: {total_items} æ¡")
            processing_log.append(f"ğŸ“Š æ•°æ®é•¿åº¦ç»Ÿè®¡: å¹³å‡ {sum(len(str(item[1])) for item in indexed_data) / len(indexed_data):.0f} å­—ç¬¦")
            processing_log.append(f"ğŸš€ å¼€å§‹å¹¶å‘å¤„ç†ï¼Œçº¿ç¨‹æ•°: {max_workers}")
            processing_log.append("-" * 40)
            
            # åˆ›å»ºç»“æœå­—å…¸
            results_dict = {}
            processed_count = 0
            start_time = time.time()
            
            def process_single_item(indexed_item):
                """å¤„ç†å•ä¸ªæ–‡æœ¬é¡¹"""
                index, item = indexed_item
                try:
                    # é™åˆ¶å•ä¸ªæ–‡æœ¬é•¿åº¦
                    if len(item) > 10000:
                        item = item[:10000] + "...[æ–‡æœ¬è¿‡é•¿ï¼Œå·²æˆªæ–­]"
                    
                    result = current_model_client.process_text(item, prompt)
                    return index, result, True
                except Exception as e:
                    error_msg = f"å¤„ç†å¤±è´¥: {str(e)}"
                    logger.error(f"å¤„ç†ç¬¬ {index+1} é¡¹å¤±è´¥: {str(e)}")
                    return index, error_msg, False
            
            # ä½¿ç”¨çº¿ç¨‹æ± è¿›è¡Œå¹¶å‘å¤„ç†
            with ThreadPoolExecutor(max_workers=max_workers) as executor:
                # æäº¤æ‰€æœ‰ä»»åŠ¡
                future_to_index = {executor.submit(process_single_item, item): item[0] for item in indexed_data}
                
                # æ”¶é›†ç»“æœ
                for future in as_completed(future_to_index):
                    try:
                        index, result, success = future.result()
                        results_dict[index] = result
                        processed_count += 1
                        global_processed_count += 1
                        
                        # è®¡ç®—è¿›åº¦
                        column_progress = processed_count / total_items * 100
                        overall_progress = global_processed_count / total_items_all_columns * 100
                        
                        # è®¡ç®—å¤„ç†é€Ÿåº¦
                        elapsed_time = time.time() - start_time
                        if elapsed_time > 0:
                            speed = processed_count / elapsed_time
                            remaining_items = total_items - processed_count
                            eta = remaining_items / speed if speed > 0 else 0
                            
                            # å®æ—¶è¯¦ç»†çš„è¿›åº¦æ˜¾ç¤º
                            if processed_count % 2 == 0 or processed_count == total_items:
                                status_msg = (
                                    f"ğŸ”„ å®æ—¶çŠ¶æ€ | åˆ—: {column} ({col_index + 1}/{len(columns_to_process)}) | "
                                    f"å½“å‰åˆ—è¿›åº¦: {processed_count}/{total_items} ({column_progress:.1f}%) | "
                                    f"æ€»ä½“è¿›åº¦: {global_processed_count}/{total_items_all_columns} ({overall_progress:.1f}%) | "
                                    f"å¤„ç†é€Ÿåº¦: {speed:.1f}æ¡/ç§’ | é¢„è®¡å‰©ä½™æ—¶é—´: {eta:.0f}ç§’"
                                )
                                processing_log.append(status_msg)
                                
                                # æ˜¾ç¤ºæœ€è¿‘å¤„ç†çš„å†…å®¹é¢„è§ˆï¼ˆæˆåŠŸçš„æƒ…å†µï¼‰
                                if success and len(result) > 0:
                                    preview_text = result[:50] + "..." if len(result) > 50 else result
                                    processing_log.append(f"   âœ… æœ€æ–°å¤„ç†ç»“æœé¢„è§ˆ: {preview_text}")
                                elif not success:
                                    processing_log.append(f"   âŒ å¤„ç†å¤±è´¥: {result}")
                        
                    except Exception as e:
                        logger.error(f"è·å–å¤„ç†ç»“æœå¤±è´¥: {str(e)}")
                        global_processed_count += 1
            
            # æ„å»ºå®Œæ•´çš„ç»“æœåˆ—è¡¨
            full_results = []
            for i, original_item in enumerate(data_to_process):
                if i in results_dict:
                    full_results.append(results_dict[i])
                else:
                    full_results.append("" if not original_item.strip() else "å¤„ç†å¤±è´¥")
            
            # æ·»åŠ ç»“æœåˆ—åˆ°DataFrame
            result_column_name = f"{column}_processed"
            current_dataframe[result_column_name] = full_results
            
            successful_count = len([r for r in full_results if r and not r.startswith('å¤„ç†å¤±è´¥')])
            failed_count = total_items - successful_count
            total_processed += successful_count
            
            # è®¡ç®—è¯¥åˆ—çš„å¤„ç†æ—¶é—´
            column_end_time = time.time()
            column_duration = column_end_time - start_time
            
            processing_log.append("-" * 40)
            processing_log.append(f"âœ… åˆ— {column} å¤„ç†å®Œæˆç»Ÿè®¡:")
            processing_log.append(f"   ğŸ“Š æˆåŠŸå¤„ç†: {successful_count} æ¡ ({successful_count/total_items*100:.1f}%)")
            processing_log.append(f"   âŒ å¤„ç†å¤±è´¥: {failed_count} æ¡ ({failed_count/total_items*100:.1f}%)")
            processing_log.append(f"   â±ï¸ å¤„ç†è€—æ—¶: {column_duration:.1f} ç§’")
            processing_log.append(f"   ğŸš€ å¹³å‡é€Ÿåº¦: {total_items/column_duration:.1f} æ¡/ç§’")
            processing_log.append(f"   â° å®Œæˆæ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}")
            processing_log.append(f"   ğŸ“ˆ ç´¯è®¡å®Œæˆ: {total_processed}/{total_items_all_columns} æ¡")
        
        # ä¿å­˜ç»“æœ
        processing_log.append("\n" + "="*50)
        processing_log.append("ğŸ’¾ æ­£åœ¨ä¿å­˜å¤„ç†ç»“æœ...")
        processing_log.append(f"ğŸ“ è¾“å‡ºç›®å½•: {Path(__file__).parent / 'output'}")
        
        if original_file_path:
            try:
                if save_location == "è‡ªå®šä¹‰ç›®å½•" and custom_save_path.strip():
                    # ä½¿ç”¨è‡ªå®šä¹‰ä¿å­˜è·¯å¾„
                    custom_dir = Path(custom_save_path.strip())
                    custom_dir.mkdir(parents=True, exist_ok=True)
                    
                    original_name = Path(original_file_path).stem
                    original_ext = Path(original_file_path).suffix
                    timestamp = time.strftime("%Y%m%d_%H%M%S")
                    output_filename = f"{original_name}_processed_{timestamp}{original_ext}"
                    output_path = custom_dir / output_filename
                    
                    processing_log.append(f"ğŸ“‚ ä½¿ç”¨è‡ªå®šä¹‰ç›®å½•: {custom_dir}")
                    
                    if original_ext.lower() in ['.xlsx', '.xls']:
                        current_dataframe.to_excel(str(output_path), index=False)
                        processing_log.append(f"ğŸ“Š Excelæ–‡ä»¶ä¿å­˜ä¸­...")
                    else:
                        current_dataframe.to_csv(str(output_path), index=False, encoding='utf-8-sig')
                        processing_log.append(f"ğŸ“„ CSVæ–‡ä»¶ä¿å­˜ä¸­...")
                else:
                    # ä½¿ç”¨é»˜è®¤outputç›®å½•ï¼ˆå½“å‰ä»£ç æ–‡ä»¶æ‰€åœ¨ç›®å½•ä¸‹çš„outputæ–‡ä»¶å¤¹ï¼‰
                    processing_log.append(f"ğŸ“‚ ä½¿ç”¨é»˜è®¤è¾“å‡ºç›®å½•: {Path(__file__).parent / 'output'}")
                    output_path = save_to_output_dir(current_dataframe, original_file_path)
                    processing_log.append(f"ğŸ’¾ æ–‡ä»¶ä¿å­˜ä¸­...")
                
                processing_log.append(f"âœ… ç»“æœå·²æˆåŠŸä¿å­˜åˆ°: {output_path}")
                processing_log.append(f"ğŸ“Š ä¿å­˜çš„æ•°æ®è¡Œæ•°: {len(current_dataframe)}")
                processing_log.append(f"ğŸ“‹ ä¿å­˜çš„æ•°æ®åˆ—æ•°: {len(current_dataframe.columns)}")
                
                # ç”Ÿæˆç»“æœé¢„è§ˆ
                result_preview = generate_enhanced_result_preview(current_dataframe, columns_to_process, total_processed)
                
                # è®¡ç®—æ€»å¤„ç†æ—¶é—´
                total_end_time = time.time()
                total_duration = total_end_time - total_start_time
                
                # ç”Ÿæˆæœ€ç»ˆç»Ÿè®¡ä¿¡æ¯
                processing_log.append("\n" + "="*50)
                processing_log.append("ğŸ¯ === æœ€ç»ˆå¤„ç†ç»Ÿè®¡æŠ¥å‘Š === ğŸ¯")
                processing_log.append(f"ğŸ“‹ å¤„ç†ä»»åŠ¡: {task_name}")
                processing_log.append(f"ğŸ“Š å¤„ç†åˆ—æ•°: {len(columns_to_process)} åˆ—")
                processing_log.append(f"ğŸ“ˆ æ€»æ•°æ®é‡: {total_items_all_columns} æ¡")
                processing_log.append(f"âœ… æˆåŠŸå¤„ç†: {total_processed} æ¡")
                processing_log.append(f"âŒ å¤±è´¥æ•°é‡: {total_items_all_columns - total_processed} æ¡")
                processing_log.append(f"ğŸ“Š æ€»æˆåŠŸç‡: {(total_processed/total_items_all_columns*100):.1f}%")
                processing_log.append(f"ğŸ”§ å¹¶å‘çº¿ç¨‹: {max_workers} ä¸ª")
                processing_log.append(f"â±ï¸ æ€»å¤„ç†æ—¶é—´: {total_duration:.1f} ç§’")
                processing_log.append(f"ğŸš€ å¹³å‡å¤„ç†é€Ÿåº¦: {total_items_all_columns/total_duration:.1f} æ¡/ç§’")
                processing_log.append(f"ğŸ’¾ è¾“å‡ºä½ç½®: {output_path}")
                processing_log.append(f"â° å®Œæˆæ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}")
                processing_log.append("="*50)
                processing_log.append(f"ğŸ‰ æ‰€æœ‰å¤„ç†ä»»åŠ¡å·²æˆåŠŸå®Œæˆï¼")
                
                final_message = "\n".join(processing_log)
                
                return final_message, result_preview, 100.0
                
            except Exception as e:
                logger.error(f"ä¿å­˜æ–‡ä»¶å¤±è´¥: {str(e)}")
                processing_log.append(f"âŒ ä¿å­˜å¤±è´¥: {str(e)}")
                
                result_preview = generate_enhanced_result_preview(current_dataframe, columns_to_process, total_processed)
                final_message = "\n".join(processing_log)
                final_message += f"\n\nâš ï¸ å¤„ç†å®Œæˆä½†ä¿å­˜å¤±è´¥ï¼Œå…±å¤„ç† {total_processed} æ¡æ•°æ®"
                
                return final_message, result_preview, overall_progress
        else:
            processing_log.append(f"ğŸ‰ æ€»è®¡å¤„ç†å®Œæˆ {total_processed} æ¡æ•°æ®")
            result_preview = generate_enhanced_result_preview(current_dataframe, columns_to_process, total_processed)
            return "\n".join(processing_log), result_preview, overall_progress
            
    except Exception as e:
        logger.error(f"æ•°æ®å¤„ç†å¤±è´¥: {str(e)}")
        return f"âŒ å¤„ç†å¤±è´¥: {str(e)}", "", 0.0

def generate_result_preview(df: pd.DataFrame, processed_columns: List[str]) -> str:
    """ç”Ÿæˆå¤„ç†ç»“æœé¢„è§ˆ"""
    try:
        preview_lines = []
        preview_lines.append("=== å¤„ç†ç»“æœé¢„è§ˆ ===")
        preview_lines.append(f"æ•°æ®æ€»è¡Œæ•°: {len(df)}")
        preview_lines.append(f"æ•°æ®æ€»åˆ—æ•°: {len(df.columns)}")
        preview_lines.append("")
        
        # æ˜¾ç¤ºå¤„ç†çš„åˆ—ä¿¡æ¯
        for col in processed_columns:
            result_col = f"{col}_processed"
            if result_col in df.columns:
                non_empty_count = df[result_col].astype(str).str.strip().ne('').sum()
                preview_lines.append(f"åˆ— '{col}' -> '{result_col}': {non_empty_count} æ¡æœ‰æ•ˆç»“æœ")
        
        preview_lines.append("")
        preview_lines.append("=== å‰5è¡Œæ•°æ®é¢„è§ˆ ===")
        
        # æ˜¾ç¤ºå‰5è¡Œçš„åŸå§‹æ•°æ®å’Œå¤„ç†ç»“æœ
        for i in range(min(5, len(df))):
            preview_lines.append(f"--- ç¬¬ {i+1} è¡Œ ---")
            for col in processed_columns:
                original_value = str(df.iloc[i][col])[:100]
                result_col = f"{col}_processed"
                if result_col in df.columns:
                    processed_value = str(df.iloc[i][result_col])[:100]
                    preview_lines.append(f"{col}: {original_value}")
                    preview_lines.append(f"{result_col}: {processed_value}")
                    preview_lines.append("")
        
        return "\n".join(preview_lines)
        
    except Exception as e:
        return f"ç”Ÿæˆé¢„è§ˆå¤±è´¥: {str(e)}"

def generate_enhanced_result_preview(df: pd.DataFrame, processed_columns: List[str], total_processed: int) -> str:
    """ç”Ÿæˆå¢å¼ºçš„å¤„ç†ç»“æœé¢„è§ˆ"""
    try:
        preview_lines = []
        preview_lines.append("ğŸ“Š === å¤„ç†ç»“æœè¯¦ç»†é¢„è§ˆ === ğŸ“Š")
        preview_lines.append(f"ğŸ“‹ æ•°æ®æ€»è¡Œæ•°: {len(df)}")
        preview_lines.append(f"ğŸ“‹ æ•°æ®æ€»åˆ—æ•°: {len(df.columns)}")
        preview_lines.append(f"âœ… æˆåŠŸå¤„ç†: {total_processed} æ¡")
        preview_lines.append("")
        
        # æ˜¾ç¤ºæ¯åˆ—çš„è¯¦ç»†ç»Ÿè®¡
        preview_lines.append("ğŸ“ˆ å„åˆ—å¤„ç†ç»Ÿè®¡:")
        for col in processed_columns:
            result_col = f"{col}_processed"
            if result_col in df.columns:
                # ç»Ÿè®¡æœ‰æ•ˆç»“æœ
                valid_results = df[result_col].astype(str).str.strip()
                non_empty_count = valid_results.ne('').sum()
                failed_count = valid_results.str.startswith('å¤„ç†å¤±è´¥').sum()
                success_count = non_empty_count - failed_count
                
                # è®¡ç®—å¹³å‡é•¿åº¦
                valid_lengths = valid_results[valid_results.ne('')].str.len()
                avg_length = valid_lengths.mean() if len(valid_lengths) > 0 else 0
                
                preview_lines.append(f"   ğŸ”¹ åˆ— '{col}':")
                preview_lines.append(f"      âœ… æˆåŠŸ: {success_count} æ¡")
                preview_lines.append(f"      âŒ å¤±è´¥: {failed_count} æ¡")
                preview_lines.append(f"      ğŸ“ å¹³å‡ç»“æœé•¿åº¦: {avg_length:.0f} å­—ç¬¦")
                preview_lines.append("")
        
        preview_lines.append("ğŸ” === æ•°æ®æ ·æœ¬é¢„è§ˆ === ğŸ”")
        
        # æ˜¾ç¤ºå‰3è¡Œçš„åŸå§‹æ•°æ®å’Œå¤„ç†ç»“æœ
        for i in range(min(3, len(df))):
            preview_lines.append(f"ğŸ“„ ç¬¬ {i+1} è¡Œæ ·æœ¬:")
            for col in processed_columns:
                original_value = str(df.iloc[i][col])
                result_col = f"{col}_processed"
                if result_col in df.columns:
                    processed_value = str(df.iloc[i][result_col])
                    
                    # é™åˆ¶æ˜¾ç¤ºé•¿åº¦
                    original_display = original_value[:150] + "..." if len(original_value) > 150 else original_value
                    processed_display = processed_value[:150] + "..." if len(processed_value) > 150 else processed_value
                    
                    preview_lines.append(f"   ğŸ“ åŸå§‹[{col}]: {original_display}")
                    preview_lines.append(f"   ğŸ¤– å¤„ç†[{result_col}]: {processed_display}")
                    preview_lines.append("   " + "-"*50)
            preview_lines.append("")
        
        # æ·»åŠ æ•°æ®è´¨é‡åˆ†æ
        preview_lines.append("ğŸ“Š === æ•°æ®è´¨é‡åˆ†æ === ğŸ“Š")
        for col in processed_columns:
            result_col = f"{col}_processed"
            if result_col in df.columns:
                results = df[result_col].astype(str)
                
                # åˆ†æç»“æœç±»å‹
                empty_count = results.str.strip().eq('').sum()
                error_count = results.str.contains('å¤„ç†å¤±è´¥|é”™è¯¯|å¤±è´¥', na=False).sum()
                valid_count = len(results) - empty_count - error_count
                
                preview_lines.append(f"   ğŸ“Š åˆ— '{col}' è´¨é‡åˆ†æ:")
                preview_lines.append(f"      ğŸŸ¢ æœ‰æ•ˆç»“æœ: {valid_count} æ¡ ({valid_count/len(results)*100:.1f}%)")
                preview_lines.append(f"      ğŸ”´ å¤„ç†é”™è¯¯: {error_count} æ¡ ({error_count/len(results)*100:.1f}%)")
                preview_lines.append(f"      âšª ç©ºç™½ç»“æœ: {empty_count} æ¡ ({empty_count/len(results)*100:.1f}%)")
                preview_lines.append("")
        
        return "\n".join(preview_lines)
        
    except Exception as e:
        return f"ç”Ÿæˆå¢å¼ºé¢„è§ˆå¤±è´¥: {str(e)}"

def save_to_output_dir(df: pd.DataFrame, original_path: str, suffix: str = "_processed") -> str:
    """ä¿å­˜æ–‡ä»¶åˆ°å½“å‰ä»£ç æ–‡ä»¶æ‰€åœ¨çš„outputç›®å½•"""
    try:
        # è·å–å½“å‰ä»£ç æ–‡ä»¶æ‰€åœ¨ç›®å½•
        current_script_dir = Path(__file__).parent
        
        # åˆ›å»ºoutputç›®å½•ï¼ˆåœ¨å½“å‰ä»£ç æ–‡ä»¶ç›®å½•ä¸‹ï¼‰
        output_dir = current_script_dir / "output"
        output_dir.mkdir(parents=True, exist_ok=True)
        
        original_path = Path(original_path)
        
        # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶å
        base_name = original_path.stem
        extension = original_path.suffix.lower()
        timestamp = time.strftime("%Y%m%d_%H%M%S")
        
        output_file = output_dir / f"{base_name}{suffix}_{timestamp}{extension}"
        
        # æ ¹æ®æ–‡ä»¶ç±»å‹ä¿å­˜
        if extension == '.csv':
            df.to_csv(output_file, index=False, encoding='utf-8-sig')
        elif extension in ['.xlsx', '.xls']:
            df.to_excel(output_file, index=False)
        else:  # txt, mdç­‰æ–‡æœ¬æ–‡ä»¶
            # å¦‚æœåªæœ‰ä¸€åˆ—ï¼Œç›´æ¥ä¿å­˜å†…å®¹
            if len(df.columns) == 1:
                content = '\n'.join(df.iloc[:, 0].astype(str))
            else:
                # å¤šåˆ—æ—¶ä¿å­˜ä¸ºCSVæ ¼å¼
                content = df.to_csv(index=False, sep='\t')
            
            with open(output_file, 'w', encoding='utf-8') as f:
                f.write(content)
        
        return str(output_file)
        
    except Exception as e:
        raise Exception(f"ä¿å­˜æ–‡ä»¶å¤±è´¥: {str(e)}")

def get_processed_data() -> str:
    """è·å–å¤„ç†åçš„æ•°æ®é¢„è§ˆ"""
    global current_dataframe
    
    if current_dataframe is None:
        return "æš‚æ— æ•°æ®"
    
    try:
        # æ˜¾ç¤ºå‰10è¡Œæ•°æ®
        preview = current_dataframe.head(10).to_string(index=False, max_cols=10)
        return preview
    except Exception as e:
        return f"è·å–æ•°æ®é¢„è§ˆå¤±è´¥: {str(e)}"

# ==================== Gradioç•Œé¢ ====================

def create_interface():
    """åˆ›å»ºGradioç•Œé¢"""
    
    with gr.Blocks(title="AIæ–‡æœ¬å¤„ç†å·¥å…·", theme=gr.themes.Ocean()) as interface:
        gr.Markdown("# ğŸ¤– AIæ–‡æœ¬å¤„ç†å·¥å…·")
        gr.Markdown("æ”¯æŒå¤šç§æ–‡ä»¶æ ¼å¼çš„AIæ™ºèƒ½æ–‡æœ¬å¤„ç†")
        
        with gr.Tabs() as tabs:
            # ç¬¬ä¸€ä¸ªæ ‡ç­¾é¡µï¼šæ¨¡å‹é…ç½®
            with gr.TabItem("ğŸ”§ æ¨¡å‹é…ç½®"):
                with gr.Row():
                    with gr.Column(scale=1):
                        gr.Markdown("### AIæ¨¡å‹é…ç½®")
                        preset_dropdown = gr.Dropdown(
                            choices=list(AIModelClient.get_all_configs().keys()) + ["è‡ªå®šä¹‰"],
                            value="deepseek",
                            label="é€‰æ‹©æ¨¡å‹é…ç½®"
                        )
                        
                        # è‡ªå®šä¹‰é…ç½®ï¼ˆé»˜è®¤éšè—ï¼‰
                        with gr.Group(visible=False) as custom_config:
                            custom_name = gr.Textbox(label="æ¨¡å‹åç§°", placeholder="ä¾‹å¦‚ï¼šæˆ‘çš„æ¨¡å‹")
                            custom_base_url = gr.Textbox(label="APIåœ°å€", placeholder="ä¾‹å¦‚ï¼šhttps://api.example.com/v1")
                            custom_api_key = gr.Textbox(label="APIå¯†é’¥", type="password")
                            custom_model_name = gr.Textbox(label="æ¨¡å‹åç§°", placeholder="ä¾‹å¦‚ï¼šgpt-3.5-turbo")
                            save_custom_config = gr.Checkbox(label="ä¿å­˜æ­¤é…ç½®", value=False)
                        
                        load_model_btn = gr.Button("ğŸš€ åŠ è½½æ¨¡å‹", variant="primary")
                        model_status = gr.Textbox(label="æ¨¡å‹çŠ¶æ€", interactive=False)
                    
                    with gr.Column(scale=1):
                        gr.Markdown("### æ¨¡å‹é…ç½®è¯´æ˜")
                        gr.Markdown("""
                        **é¢„è®¾é…ç½®ï¼š**
                        - DeepSeek: é«˜æ€§èƒ½ä¸­æ–‡å¤§æ¨¡å‹
                        - OpenAI: GPTç³»åˆ—æ¨¡å‹
                        - æœ¬åœ°æ¨¡å‹: æœ¬åœ°éƒ¨ç½²çš„æ¨¡å‹
                        
                        **è‡ªå®šä¹‰é…ç½®ï¼š**
                        - æ”¯æŒä»»ä½•å…¼å®¹OpenAI APIçš„æ¨¡å‹
                        - å¯ä¿å­˜é…ç½®ä¾›ä¸‹æ¬¡ä½¿ç”¨
                        - è‡ªåŠ¨æµ‹è¯•è¿æ¥çŠ¶æ€
                        """)
            
            # ç¬¬äºŒä¸ªæ ‡ç­¾é¡µï¼šä»»åŠ¡ç®¡ç†
            with gr.TabItem("ğŸ“ ä»»åŠ¡ç®¡ç†"):
                with gr.Row():
                    with gr.Column(scale=1):
                        gr.Markdown("### æç¤ºè¯ä»»åŠ¡ç®¡ç†")
                        task_dropdown = gr.Dropdown(
                            choices=task_manager.get_task_names(),
                            value=task_manager.get_task_names()[0] if task_manager.get_task_names() else None,
                            label="é€‰æ‹©ä»»åŠ¡"
                        )
                        
                        task_prompt_display = gr.Textbox(
                            label="å½“å‰ä»»åŠ¡æç¤ºè¯",
                            lines=5,
                            interactive=True
                        )
                        
                        # ä»»åŠ¡ç®¡ç†æŒ‰é’®
                        with gr.Row():
                            edit_task_btn = gr.Button("âœï¸ ä¿å­˜ä¿®æ”¹", variant="primary")
                            delete_task_btn = gr.Button("ğŸ—‘ï¸ åˆ é™¤/é‡ç½®", variant="secondary")
                        with gr.Row():
                            reload_tasks_btn = gr.Button("ğŸ”„ é‡æ–°åŠ è½½", variant="secondary")
                        
                        task_status = gr.Textbox(label="ä»»åŠ¡çŠ¶æ€", interactive=False)
                    
                    with gr.Column(scale=1):
                        # è‡ªå®šä¹‰ä»»åŠ¡
                        gr.Markdown("### æ·»åŠ è‡ªå®šä¹‰ä»»åŠ¡")
                        new_task_name = gr.Textbox(label="ä»»åŠ¡åç§°", placeholder="è¾“å…¥æ–°ä»»åŠ¡åç§°")
                        new_task_prompt = gr.Textbox(label="ä»»åŠ¡æç¤ºè¯", lines=8, placeholder="è¾“å…¥ä»»åŠ¡æç¤ºè¯")
                        add_task_btn = gr.Button("â• æ·»åŠ ä»»åŠ¡", variant="primary")
                        
                        gr.Markdown("### ä»»åŠ¡è¯´æ˜")
                        gr.Markdown("""
                        **é»˜è®¤ä»»åŠ¡ï¼š**
                        - æ–‡æœ¬æ‘˜è¦ã€å…³é”®è¯æå–ã€æƒ…æ„Ÿåˆ†æç­‰
                        - å¯ä»¥ä¿®æ”¹æç¤ºè¯å†…å®¹
                        - åˆ é™¤æ—¶ä¼šé‡ç½®ä¸ºåŸå§‹å€¼
                        
                        **è‡ªå®šä¹‰ä»»åŠ¡ï¼š**
                        - å¯æ·»åŠ ã€åˆ é™¤ã€ä¿®æ”¹
                        - æ”¯æŒå¤æ‚çš„æç¤ºè¯æ¨¡æ¿
                        - åˆ é™¤æ—¶ä¼šå®Œå…¨ç§»é™¤
                        
                        **æ“ä½œè¯´æ˜ï¼š**
                        - é€‰æ‹©ä»»åŠ¡åå¯ç›´æ¥ç¼–è¾‘æç¤ºè¯
                        - ç‚¹å‡»"ä¿å­˜ä¿®æ”¹"åº”ç”¨æ›´æ”¹
                        - "åˆ é™¤/é‡ç½®"å¯¹é»˜è®¤ä»»åŠ¡æ˜¯é‡ç½®ï¼Œå¯¹è‡ªå®šä¹‰ä»»åŠ¡æ˜¯åˆ é™¤
                        """)
            
            # ç¬¬ä¸‰ä¸ªæ ‡ç­¾é¡µï¼šæ–‡ä»¶å¤„ç†ä¸ç»“æœ
            with gr.TabItem("ğŸ“ æ–‡ä»¶å¤„ç†"):
                with gr.Row():
                    with gr.Column(scale=1):
                        # æ–‡ä»¶ä¸Šä¼ 
                        gr.Markdown("### æ–‡ä»¶ä¸Šä¼ ")
                        file_upload = gr.File(
                            label="ä¸Šä¼ æ–‡ä»¶",
                            file_types=[".txt", ".md", ".csv", ".xlsx", ".xls", ".pdf", ".docx", ".doc"]
                        )
                        
                        file_info = gr.Textbox(label="æ–‡ä»¶ä¿¡æ¯", interactive=False)
                        
                        # åˆ—é€‰æ‹©ï¼ˆä»…å¯¹è¡¨æ ¼æ–‡ä»¶æ˜¾ç¤ºï¼‰
                        column_dropdown = gr.Dropdown(
                            label="é€‰æ‹©è¦å¤„ç†çš„åˆ—",
                            visible=False,
                            multiselect=True
                        )
                        
                        # ä»»åŠ¡é€‰æ‹©
                        gr.Markdown("### å¤„ç†ä»»åŠ¡")
                        selected_task = gr.Dropdown(
                            choices=task_manager.get_task_names(),
                            value=task_manager.get_task_names()[0] if task_manager.get_task_names() else None,
                            label="é€‰æ‹©å¤„ç†ä»»åŠ¡"
                        )
                        
                        # å¤„ç†å‚æ•°
                        gr.Markdown("### å¤„ç†å‚æ•°")
                        with gr.Row():
                            batch_size = gr.Slider(
                                minimum=1, maximum=50, value=10, step=1,
                                label="æ‰¹æ¬¡å¤§å°"
                            )
                            max_workers = gr.Slider(
                                minimum=1, maximum=10, value=3, step=1,
                                label="å¹¶å‘æ•°"
                            )
                        
                        # ä¿å­˜è®¾ç½®
                        gr.Markdown("### ä¿å­˜è®¾ç½®")
                        save_location = gr.Radio(
                            choices=["å½“å‰æ–‡ä»¶çš„outputç›®å½•", "è‡ªå®šä¹‰ç›®å½•"],
                            value="å½“å‰æ–‡ä»¶çš„outputç›®å½•",
                            label="ä¿å­˜ä½ç½®"
                        )
                        
                        custom_save_path = gr.Textbox(
                            label="è‡ªå®šä¹‰ä¿å­˜è·¯å¾„",
                            placeholder="è¾“å…¥è‡ªå®šä¹‰ä¿å­˜è·¯å¾„...",
                            visible=False
                        )
                        
                        # å¤„ç†æŒ‰é’®
                        with gr.Row():
                            process_btn = gr.Button("ğŸ”„ å¼€å§‹å¤„ç†", variant="primary", size="lg")
                            clear_btn = gr.Button("ğŸ—‘ï¸ æ¸…é™¤ç»“æœ", variant="secondary", size="lg")
                    
                    with gr.Column(scale=1):
                        # æ–‡ä»¶é¢„è§ˆ
                        gr.Markdown("### æ–‡ä»¶é¢„è§ˆ")
                        file_preview = gr.Textbox(label="æ–‡ä»¶å†…å®¹é¢„è§ˆ", lines=8, interactive=False)
                        
                        # å¤„ç†çŠ¶æ€
                        gr.Markdown("### å¤„ç†çŠ¶æ€")
                        
                        # æ·»åŠ è¿›åº¦æ¡
                        progress_bar = gr.Progress()
                        processing_progress = gr.Slider(
                            minimum=0, maximum=100, value=0, step=0.1,
                            label="å¤„ç†è¿›åº¦ (%)",
                            interactive=False,
                            visible=True
                        )
                        
                        process_output = gr.Textbox(label="å¤„ç†æ—¥å¿—", lines=10, interactive=False)
                        
                        # ç»“æœé¢„è§ˆ
                        gr.Markdown("### ç»“æœé¢„è§ˆ")
                        result_preview = gr.Textbox(label="å¤„ç†ç»“æœè¯¦æƒ…", lines=12, interactive=False)
        
        # æ˜¾ç¤º/éšè—è‡ªå®šä¹‰é…ç½®
        def toggle_custom_config(preset):
            return gr.Group(visible=(preset == "è‡ªå®šä¹‰"))
        
        preset_dropdown.change(
            toggle_custom_config,
            inputs=[preset_dropdown],
            outputs=[custom_config]
        )
        
        # æ˜¾ç¤º/éšè—è‡ªå®šä¹‰ä¿å­˜è·¯å¾„
        save_location.change(
            fn=lambda x: gr.update(visible=(x == "è‡ªå®šä¹‰ç›®å½•")),
            inputs=[save_location],
            outputs=[custom_save_path]
        )
        
        # åŠ è½½æ¨¡å‹
        load_model_btn.click(
            load_model,
            inputs=[preset_dropdown, custom_name, custom_base_url, custom_api_key, custom_model_name, save_custom_config],
            outputs=[model_status, preset_dropdown]
        )
        
        # æ–‡ä»¶ä¸Šä¼ å¤„ç†
        file_upload.change(
            fn=handle_file_upload,
            inputs=[file_upload],
            outputs=[file_info, file_preview, column_dropdown]
        )
        
        # ä»»åŠ¡é€‰æ‹©æ—¶æ˜¾ç¤ºæç¤ºè¯
        task_dropdown.change(
            get_task_prompt,
            inputs=[task_dropdown],
            outputs=[task_prompt_display]
        )
        
        # ç¼–è¾‘ä»»åŠ¡
        edit_task_btn.click(
            edit_task,
            inputs=[task_dropdown, task_prompt_display],
            outputs=[task_status, task_dropdown, selected_task]
        )
        
        # æ·»åŠ è‡ªå®šä¹‰ä»»åŠ¡
        add_task_btn.click(
            add_custom_task,
            inputs=[new_task_name, new_task_prompt],
            outputs=[task_status, task_dropdown, selected_task]
        )
        
        # åˆ é™¤ä»»åŠ¡
        delete_task_btn.click(
            delete_task,
            inputs=[task_dropdown],
            outputs=[task_status, task_dropdown, selected_task]
        )
        
        # é‡æ–°åŠ è½½ä»»åŠ¡
        reload_tasks_btn.click(
            reload_tasks,
            outputs=[task_status, task_dropdown, selected_task]
        )
        
        # å¤„ç†æ•°æ®
        def process_with_progress(file_upload, column_dropdown, selected_task, batch_size, max_workers, save_location, custom_save_path):
            """å¸¦è¿›åº¦æ›´æ–°çš„å¤„ç†å‡½æ•°"""
            log, preview, progress = process_data(file_upload, column_dropdown, selected_task, batch_size, max_workers, save_location, custom_save_path)
            return log, preview, progress
        
        # å®æ—¶è¿›åº¦æ›´æ–°å‡½æ•°
        def start_processing(file_upload, column_dropdown, selected_task, batch_size, max_workers, save_location, custom_save_path):
            """å¼€å§‹å¤„ç†å¹¶æ˜¾ç¤ºå®æ—¶è¿›åº¦"""
            # é‡ç½®è¿›åº¦
            yield "ğŸš€ å¼€å§‹å¤„ç†...", "", 0.0
            
            # è°ƒç”¨æµå¼å¤„ç†å‡½æ•°
            try:
                for log, preview, progress in process_data_stream(file_upload, column_dropdown, selected_task, batch_size, max_workers, save_location, custom_save_path):
                    yield log, preview, progress
            except Exception as e:
                yield f"âŒ å¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}", "", 0.0
        
        process_btn.click(
             fn=start_processing,
             inputs=[file_upload, column_dropdown, selected_task, batch_size, max_workers, save_location, custom_save_path],
             outputs=[process_output, result_preview, processing_progress]
         )
         
         # æ¸…é™¤ç»“æœåŠŸèƒ½
        def clear_results():
            return "", "", 0.0
            
        clear_btn.click(
            fn=clear_results,
            outputs=[process_output, result_preview, processing_progress]
        )
    
    return interface

# ==================== ä¸»ç¨‹åº ====================

if __name__ == "__main__":
    # åˆ›å»ºå¹¶å¯åŠ¨ç•Œé¢
    interface = create_interface()
    
    # å¯åŠ¨æœåŠ¡
    interface.launch(
        server_name="0.0.0.0",
        server_port=7863,
        share=False,
        debug=True
    )
