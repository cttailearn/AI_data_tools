import gradio as gr
import os
import json
import pandas as pd
import openai
from pathlib import Path
from typing import Dict, Any, Optional, List, Tuple
import logging
from dataclasses import dataclass
import re
from urllib.parse import urlparse
from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type
import threading
import time
import uuid
from concurrent.futures import ThreadPoolExecutor, as_completed
from queue import Queue
import fnmatch
import glob

# å¯é€‰ä¾èµ–
try:
    import docx
    HAS_DOCX = True
except ImportError:
    HAS_DOCX = False

try:
    import PyPDF2
    HAS_PDF = True
except ImportError:
    HAS_PDF = False

try:
    import chardet
    HAS_CHARDET = True
except ImportError:
    HAS_CHARDET = False

# é…ç½®æ—¥å¿— - åŒæ—¶è¾“å‡ºåˆ°æ§åˆ¶å°å’Œæ–‡ä»¶
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),  # è¾“å‡ºåˆ°æ§åˆ¶å°
        logging.FileHandler('ai_text_processing.log', encoding='utf-8')  # è¾“å‡ºåˆ°æ–‡ä»¶
    ]
)
logger = logging.getLogger(__name__)

# ==================== AIæ¨¡å‹é…ç½® ====================

@dataclass
class ModelConfig:
    """AIæ¨¡å‹é…ç½®ç±»"""
    name: str
    base_url: str
    api_key: str
    model_name: str
    timeout: int = 60
    max_retries: int = 3
    temperature: float = 0.7
    max_tokens: Optional[int] = None
    
    def __post_init__(self):
        """é…ç½®éªŒè¯"""
        if not self.base_url or not self.api_key or not self.model_name:
            raise ValueError("base_url, api_key, model_name ä¸èƒ½ä¸ºç©º")
        
        # éªŒè¯URLæ ¼å¼
        parsed = urlparse(self.base_url)
        if not parsed.scheme or not parsed.netloc:
            raise ValueError("base_url æ ¼å¼æ— æ•ˆ")

class AIModelClient:
    """AIæ¨¡å‹å®¢æˆ·ç«¯"""
    @classmethod
    def load_custom_configs(cls) -> Dict[str, Dict]:
        """åŠ è½½è‡ªå®šä¹‰æ¨¡å‹é…ç½®"""
        config_file = "model_configs.json"
        try:
            if os.path.exists(config_file):
                with open(config_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
        except Exception as e:
            logger.warning(f"åŠ è½½æ¨¡å‹é…ç½®å¤±è´¥: {str(e)}")
        return {}
    
    @classmethod
    def save_custom_config(cls, name: str, config: Dict[str, Any]):
        """ä¿å­˜è‡ªå®šä¹‰æ¨¡å‹é…ç½®"""
        config_file = "model_configs.json"
        try:
            configs = cls.load_custom_configs()
            configs[name] = config
            with open(config_file, 'w', encoding='utf-8') as f:
                json.dump(configs, f, ensure_ascii=False, indent=2)
            logger.info(f"æ¨¡å‹é…ç½® {name} å·²ä¿å­˜")
        except Exception as e:
            logger.error(f"ä¿å­˜æ¨¡å‹é…ç½®å¤±è´¥: {str(e)}")
    
    @classmethod
    def get_all_configs(cls) -> Dict[str, Dict]:
        """è·å–æ‰€æœ‰æ¨¡å‹é…ç½®ï¼ˆé¢„è®¾+è‡ªå®šä¹‰ï¼‰"""
        # é¢„è®¾æ¨¡å‹é…ç½®
        all_configs = {
            "OpenAI GPT-4": {
                "base_url": "https://api.openai.com/v1",
                "model_name": "gpt-4",
                "api_key": "your-openai-api-key"
            },
            "OpenAI GPT-3.5": {
                "base_url": "https://api.openai.com/v1",
                "model_name": "gpt-3.5-turbo",
                "api_key": "your-openai-api-key"
            },
            "Claude-3": {
                "base_url": "https://api.anthropic.com/v1",
                "model_name": "claude-3-sonnet-20240229",
                "api_key": "your-anthropic-api-key"
            }
        }
        
        # åŠ è½½è‡ªå®šä¹‰é…ç½®å¹¶åˆå¹¶
        custom_configs = cls.load_custom_configs()
        all_configs.update(custom_configs)
        return all_configs
    
    def __init__(self, config: ModelConfig):
        self.config = config
        self.client = openai.OpenAI(
            base_url=config.base_url,
            api_key=config.api_key,
            timeout=config.timeout,
            max_retries=config.max_retries
        )
        logger.info(f"AIæ¨¡å‹å®¢æˆ·ç«¯å·²åˆå§‹åŒ–: {config.name}")
    
    @retry(
        stop=stop_after_attempt(5),
        wait=wait_exponential(multiplier=1, min=4, max=10),
        retry=retry_if_exception_type((
            openai.RateLimitError, 
            openai.APITimeoutError,
            openai.APIConnectionError,
            ConnectionError,
            TimeoutError,
            Exception  # æ•è·æ‰€æœ‰å¼‚å¸¸è¿›è¡Œé‡è¯•
        ))
    )
    def process_text(self, text: str, prompt: str) -> str:
        """ä½¿ç”¨AIæ¨¡å‹å¤„ç†æ–‡æœ¬"""
        try:
            messages = [
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡æœ¬å¤„ç†åŠ©æ‰‹ï¼Œè¯·æ ¹æ®ç”¨æˆ·çš„è¦æ±‚å¤„ç†æ–‡æœ¬å†…å®¹ã€‚"},
                {"role": "user", "content": f"è¯·æ ¹æ®ä»¥ä¸‹è¦æ±‚å¤„ç†æ–‡æœ¬ï¼š\n\nè¦æ±‚ï¼š{prompt}\n\næ–‡æœ¬å†…å®¹ï¼š{text}"}
            ]
            
            response = self.client.chat.completions.create(
                model=self.config.model_name,
                messages=messages,
                temperature=self.config.temperature,
                max_tokens=self.config.max_tokens
            )
            
            return response.choices[0].message.content.strip()
            
        except (openai.APIConnectionError, ConnectionError) as e:
            logger.warning(f"è¿æ¥é”™è¯¯ï¼Œæ­£åœ¨é‡è¯•: {str(e)}")
            raise
        except (openai.APITimeoutError, TimeoutError) as e:
            logger.warning(f"è¯·æ±‚è¶…æ—¶ï¼Œæ­£åœ¨é‡è¯•: {str(e)}")
            raise
        except openai.RateLimitError as e:
            logger.warning(f"è¯·æ±‚é¢‘ç‡é™åˆ¶ï¼Œæ­£åœ¨é‡è¯•: {str(e)}")
            raise
        except Exception as e:
            logger.error(f"AIå¤„ç†å¤±è´¥: {str(e)}ï¼Œæ­£åœ¨é‡è¯•")
            raise
    
    def test_connection(self) -> bool:
        """æµ‹è¯•è¿æ¥"""
        try:
            response = self.process_text("æµ‹è¯•", "è¯·å›å¤'è¿æ¥æˆåŠŸ'")
            return "è¿æ¥æˆåŠŸ" in response or "æˆåŠŸ" in response
        except Exception:
            return False

# ==================== æ–‡ä»¶å¤„ç†æ¨¡å— ====================

class FileProcessor:
    """æ–‡ä»¶å¤„ç†å™¨"""
    
    @staticmethod
    def detect_encoding(file_path: str) -> str:
        """æ£€æµ‹æ–‡ä»¶ç¼–ç """
        # å¦‚æœæœ‰chardetåº“ï¼Œä¼˜å…ˆä½¿ç”¨
        if HAS_CHARDET:
            try:
                with open(file_path, 'rb') as f:
                    raw_data = f.read(10000)  # è¯»å–å‰10KBç”¨äºæ£€æµ‹
                result = chardet.detect(raw_data)
                if result['encoding'] and result['confidence'] > 0.7:
                    return result['encoding']
            except Exception:
                pass
        
        # å¤‡ç”¨æ–¹æ¡ˆï¼šå°è¯•å¸¸è§ç¼–ç 
        encodings = ['utf-8', 'utf-8-sig', 'gbk', 'gb2312', 'gb18030', 'big5', 'latin1']
        
        for encoding in encodings:
            try:
                with open(file_path, 'r', encoding=encoding) as f:
                    f.read()
                return encoding
            except (UnicodeDecodeError, UnicodeError):
                continue
        
        return 'utf-8'  # é»˜è®¤è¿”å›utf-8
    
    @staticmethod
    def read_txt_file(file_path: str) -> pd.DataFrame:
        """è¯»å–TXTæ–‡ä»¶"""
        try:
            encoding = FileProcessor.detect_encoding(file_path)
            with open(file_path, 'r', encoding=encoding) as f:
                content = f.read()
            
            # æŒ‰è¡Œåˆ†å‰²ï¼Œåˆ›å»ºDataFrame
            lines = [line.strip() for line in content.split('\n') if line.strip()]
            df = pd.DataFrame({'content': lines})
            return df
            
        except Exception as e:
            raise Exception(f"è¯»å–TXTæ–‡ä»¶å¤±è´¥: {str(e)}")
    
    @staticmethod
    def read_md_file(file_path: str) -> pd.DataFrame:
        """è¯»å–Markdownæ–‡ä»¶"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # æŒ‰æ®µè½åˆ†å‰²ï¼ˆåŒæ¢è¡Œç¬¦ï¼‰
            paragraphs = [p.strip() for p in content.split('\n\n') if p.strip()]
            df = pd.DataFrame({'content': paragraphs})
            return df
            
        except Exception as e:
            raise Exception(f"è¯»å–Markdownæ–‡ä»¶å¤±è´¥: {str(e)}")
    
    @staticmethod
    def read_excel_file(file_path: str) -> Tuple[pd.DataFrame, List[str]]:
        """è¯»å–Excelæ–‡ä»¶ï¼Œè¿”å›æ•°æ®å’Œå·¥ä½œè¡¨åç§°"""
        try:
            # è·å–æ‰€æœ‰å·¥ä½œè¡¨åç§°
            excel_file = pd.ExcelFile(file_path)
            sheet_names = excel_file.sheet_names
            
            # è¯»å–ç¬¬ä¸€ä¸ªå·¥ä½œè¡¨
            df = pd.read_excel(file_path, sheet_name=sheet_names[0])
            
            return df, sheet_names
            
        except Exception as e:
            raise Exception(f"è¯»å–Excelæ–‡ä»¶å¤±è´¥: {str(e)}")
    
    @staticmethod
    def read_csv_file(file_path: str) -> pd.DataFrame:
        """è¯»å–CSVæ–‡ä»¶"""
        try:
            encoding = FileProcessor.detect_encoding(file_path)
            
            # å°è¯•ä¸åŒçš„åˆ†éš”ç¬¦
            separators = [',', ';', '\t', '|']
            
            for sep in separators:
                try:
                    df = pd.read_csv(file_path, encoding=encoding, sep=sep)
                    # æ£€æŸ¥æ˜¯å¦æˆåŠŸè§£æï¼ˆè‡³å°‘æœ‰2åˆ—æˆ–å¤šè¡Œï¼‰
                    if len(df.columns) > 1 or len(df) > 1:
                        return df
                except Exception:
                    continue
            
            # å¦‚æœéƒ½å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤é€—å·åˆ†éš”ç¬¦
            df = pd.read_csv(file_path, encoding=encoding)
            return df
            
        except Exception as e:
            raise Exception(f"è¯»å–CSVæ–‡ä»¶å¤±è´¥: {str(e)}")
    
    @staticmethod
    def read_pdf_file(file_path: str) -> pd.DataFrame:
        """è¯»å–PDFæ–‡ä»¶"""
        if not HAS_PDF:
            raise Exception("éœ€è¦å®‰è£…PyPDF2åº“æ¥è¯»å–PDFæ–‡ä»¶: pip install PyPDF2")
        
        try:
            with open(file_path, 'rb') as f:
                pdf_reader = PyPDF2.PdfReader(f)
                text_content = []
                
                for page_num in range(len(pdf_reader.pages)):
                    page = pdf_reader.pages[page_num]
                    text = page.extract_text()
                    if text.strip():
                        text_content.append(text.strip())
                
                df = pd.DataFrame({'content': text_content})
                return df
                
        except Exception as e:
            raise Exception(f"è¯»å–PDFæ–‡ä»¶å¤±è´¥: {str(e)}")
    
    @staticmethod
    def read_docx_file(file_path: str) -> pd.DataFrame:
        """è¯»å–Wordæ–‡æ¡£"""
        if not HAS_DOCX:
            raise Exception("éœ€è¦å®‰è£…python-docxåº“æ¥è¯»å–Wordæ–‡æ¡£: pip install python-docx")
        
        try:
            doc = docx.Document(file_path)
            paragraphs = []
            
            for paragraph in doc.paragraphs:
                text = paragraph.text.strip()
                if text:
                    paragraphs.append(text)
            
            df = pd.DataFrame({'content': paragraphs})
            return df
            
        except Exception as e:
            raise Exception(f"è¯»å–Wordæ–‡æ¡£å¤±è´¥: {str(e)}")
    
    @staticmethod
    def save_file(df: pd.DataFrame, original_path: str, suffix: str = "_processed") -> str:
        """ä¿å­˜å¤„ç†åçš„æ–‡ä»¶åˆ°åŸæ–‡ä»¶ç›®å½•"""
        try:
            original_path = Path(original_path)
            output_dir = original_path.parent
            
            # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶å
            base_name = original_path.stem
            extension = original_path.suffix.lower()
            
            # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
            output_dir.mkdir(parents=True, exist_ok=True)
            
            output_file = output_dir / f"{base_name}{suffix}{extension}"
            
            # æ ¹æ®æ–‡ä»¶ç±»å‹ä¿å­˜
            if extension == '.csv':
                df.to_csv(output_file, index=False, encoding='utf-8-sig')
            elif extension in ['.xlsx', '.xls']:
                df.to_excel(output_file, index=False)
            else:  # txt, mdç­‰æ–‡æœ¬æ–‡ä»¶
                # å¦‚æœåªæœ‰ä¸€åˆ—ï¼Œç›´æ¥ä¿å­˜å†…å®¹
                if len(df.columns) == 1:
                    content = '\n'.join(df.iloc[:, 0].astype(str))
                else:
                    # å¤šåˆ—æ—¶ä¿å­˜ä¸ºCSVæ ¼å¼
                    content = df.to_csv(index=False, sep='\t')
                
                with open(output_file, 'w', encoding='utf-8') as f:
                    f.write(content)
            
            return str(output_file)
            
        except Exception as e:
            raise Exception(f"ä¿å­˜æ–‡ä»¶å¤±è´¥: {str(e)}")

# ==================== æç¤ºè¯ä»»åŠ¡ç®¡ç† ====================

class PromptTaskManager:
    """æç¤ºè¯ä»»åŠ¡ç®¡ç†å™¨"""
    
    def __init__(self):
        self.tasks_file = "prompt_tasks.json"
        self.default_tasks = {
            "æ–‡æœ¬æ‘˜è¦": "è¯·ä¸ºä»¥ä¸‹æ–‡æœ¬ç”Ÿæˆä¸€ä¸ªç®€æ´çš„æ‘˜è¦ï¼Œçªå‡ºä¸»è¦å†…å®¹å’Œå…³é”®ä¿¡æ¯ã€‚",
            "å…³é”®è¯æå–": "è¯·ä»ä»¥ä¸‹æ–‡æœ¬ä¸­æå–5-10ä¸ªå…³é”®è¯ï¼Œç”¨é€—å·åˆ†éš”ã€‚",
            "æƒ…æ„Ÿåˆ†æ": "è¯·åˆ†æä»¥ä¸‹æ–‡æœ¬çš„æƒ…æ„Ÿå€¾å‘ï¼Œå›ç­”ç§¯æã€æ¶ˆææˆ–ä¸­æ€§ï¼Œå¹¶ç®€è¦è¯´æ˜ç†ç”±ã€‚",
            "æ–‡æœ¬åˆ†ç±»": "è¯·å¯¹ä»¥ä¸‹æ–‡æœ¬è¿›è¡Œåˆ†ç±»ï¼Œå¹¶è¯´æ˜åˆ†ç±»ç†ç”±ã€‚",
            "é—®é¢˜ç”Ÿæˆ": "è¯·æ ¹æ®ä»¥ä¸‹æ–‡æœ¬å†…å®¹ç”Ÿæˆ3-5ä¸ªç›¸å…³é—®é¢˜ã€‚",
            "æ–‡æœ¬ç¿»è¯‘": "è¯·å°†ä»¥ä¸‹æ–‡æœ¬ç¿»è¯‘æˆè‹±æ–‡ã€‚",
            "å†…å®¹æ‰©å±•": "è¯·åŸºäºä»¥ä¸‹æ–‡æœ¬å†…å®¹è¿›è¡Œæ‰©å±•ï¼Œå¢åŠ æ›´å¤šç»†èŠ‚å’Œä¿¡æ¯ã€‚",
            "æ ¼å¼åŒ–": "è¯·å°†ä»¥ä¸‹æ–‡æœ¬è¿›è¡Œæ ¼å¼åŒ–æ•´ç†ï¼Œä½¿å…¶æ›´åŠ è§„èŒƒå’Œæ˜“è¯»ã€‚"
        }
        self.tasks = self.load_tasks()
    
    def load_tasks(self) -> Dict[str, str]:
        """åŠ è½½æç¤ºè¯ä»»åŠ¡"""
        tasks = self.default_tasks.copy()
        
        try:
            if os.path.exists(self.tasks_file):
                with open(self.tasks_file, 'r', encoding='utf-8') as f:
                    saved_tasks = json.load(f)
                # åˆå¹¶é»˜è®¤ä»»åŠ¡å’Œä¿å­˜çš„ä»»åŠ¡
                tasks.update(saved_tasks)
                logger.info(f"å·²åŠ è½½ {len(saved_tasks)} ä¸ªè‡ªå®šä¹‰ä»»åŠ¡")
        except Exception as e:
            logger.warning(f"åŠ è½½ä»»åŠ¡æ–‡ä»¶å¤±è´¥: {str(e)}")
        
        return tasks
    
    def save_tasks(self):
        """ä¿å­˜æç¤ºè¯ä»»åŠ¡ï¼ˆä¿å­˜æ‰€æœ‰è¢«ä¿®æ”¹çš„ä»»åŠ¡ï¼‰"""
        try:
            # ä¿å­˜æ‰€æœ‰ä»»åŠ¡ï¼ˆåŒ…æ‹¬è¢«ä¿®æ”¹çš„é»˜è®¤ä»»åŠ¡ï¼‰
            custom_tasks = {k: v for k, v in self.tasks.items() if k not in self.default_tasks or v != self.default_tasks.get(k)}
            with open(self.tasks_file, 'w', encoding='utf-8') as f:
                json.dump(custom_tasks, f, ensure_ascii=False, indent=2)
            logger.info(f"å·²ä¿å­˜ {len(custom_tasks)} ä¸ªä»»åŠ¡ï¼ˆåŒ…æ‹¬ä¿®æ”¹çš„é»˜è®¤ä»»åŠ¡ï¼‰")
        except Exception as e:
            logger.error(f"ä¿å­˜ä»»åŠ¡æ–‡ä»¶å¤±è´¥: {str(e)}")
    
    def add_task(self, name: str, prompt: str):
        """æ·»åŠ æˆ–æ›´æ–°ä»»åŠ¡"""
        self.tasks[name] = prompt
        self.save_tasks()
        logger.info(f"ä»»åŠ¡ '{name}' å·²ä¿å­˜")
    
    def delete_task(self, name: str) -> bool:
        """åˆ é™¤ä»»åŠ¡ï¼ˆé»˜è®¤ä»»åŠ¡ä¼šé‡ç½®ä¸ºåŸå§‹å€¼ï¼‰"""
        if name in self.default_tasks:
            # å¦‚æœæ˜¯é»˜è®¤ä»»åŠ¡ï¼Œé‡ç½®ä¸ºåŸå§‹å€¼
            self.tasks[name] = self.default_tasks[name]
            self.save_tasks()
            logger.info(f"é»˜è®¤ä»»åŠ¡ '{name}' å·²é‡ç½®ä¸ºåŸå§‹å€¼")
            return True
        
        if name in self.tasks:
            del self.tasks[name]
            self.save_tasks()
            logger.info(f"è‡ªå®šä¹‰ä»»åŠ¡ '{name}' å·²åˆ é™¤")
            return True
        return False
    
    def get_task_names(self) -> List[str]:
        """è·å–æ‰€æœ‰ä»»åŠ¡åç§°"""
        return list(self.tasks.keys())
    
    def get_task_prompt(self, name: str) -> str:
        """è·å–ä»»åŠ¡æç¤ºè¯"""
        return self.tasks.get(name, "")
    
    def is_default_task(self, name: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºé»˜è®¤ä»»åŠ¡"""
        return name in self.default_tasks
    
    def reload_tasks(self):
        """é‡æ–°åŠ è½½ä»»åŠ¡"""
        self.tasks = self.load_tasks()

# ==================== å…¨å±€å˜é‡ ====================

# å…¨å±€å˜é‡
current_model_client = None
current_dataframe = None
original_file_path = None
task_manager = PromptTaskManager()

# ä¸­æ–­å¤„ç†ç›¸å…³å˜é‡
processing_interrupted = False
processing_lock = threading.Lock()

# ==================== ä¸­æ–­å¤„ç†å‡½æ•° ====================

def set_processing_interrupted(interrupted: bool = True):
    """è®¾ç½®å¤„ç†ä¸­æ–­æ ‡å¿—"""
    global processing_interrupted
    with processing_lock:
        processing_interrupted = interrupted
        if interrupted:
            logger.info("ç”¨æˆ·è¯·æ±‚ä¸­æ–­å¤„ç†")
        else:
            logger.info("é‡ç½®ä¸­æ–­æ ‡å¿—")

def is_processing_interrupted() -> bool:
    """æ£€æŸ¥æ˜¯å¦è¯·æ±‚ä¸­æ–­å¤„ç†"""
    global processing_interrupted
    with processing_lock:
        return processing_interrupted

def interrupt_processing() -> str:
    """ä¸­æ–­å½“å‰å¤„ç†"""
    set_processing_interrupted(True)
    return "â¹ï¸ å·²è¯·æ±‚ä¸­æ–­å¤„ç†ï¼Œæ­£åœ¨åœæ­¢..."

# ==================== Gradioç•Œé¢å‡½æ•° ====================

def load_model(preset_name: str, custom_name: str, custom_base_url: str, 
               custom_api_key: str, custom_model_name: str, save_custom: bool = False) -> Tuple[str, str]:
    """åŠ è½½AIæ¨¡å‹"""
    global current_model_client
    
    try:
        if preset_name != "è‡ªå®šä¹‰":
            # ä½¿ç”¨é¢„è®¾æˆ–å·²ä¿å­˜çš„é…ç½®
            all_configs = AIModelClient.get_all_configs()
            if preset_name not in all_configs:
                return f"é”™è¯¯ï¼šæœªçŸ¥çš„æ¨¡å‹é…ç½® {preset_name}", ""
            
            config_dict = all_configs[preset_name].copy()
            config = ModelConfig(**config_dict)
        else:
            # ä½¿ç”¨è‡ªå®šä¹‰é…ç½®
            if not all([custom_name, custom_base_url, custom_api_key, custom_model_name]):
                return "é”™è¯¯ï¼šè‡ªå®šä¹‰é…ç½®ä¿¡æ¯ä¸å®Œæ•´", ""
            
            config = ModelConfig(
                name=custom_name,
                base_url=custom_base_url,
                api_key=custom_api_key,
                model_name=custom_model_name
            )
            
            # ä¿å­˜è‡ªå®šä¹‰é…ç½®
            if save_custom and custom_name:
                config_dict = {
                    "name": custom_name,
                    "base_url": custom_base_url,
                    "api_key": custom_api_key,
                    "model_name": custom_model_name
                }
                AIModelClient.save_custom_config(custom_name, config_dict)
        
        # åˆ›å»ºå®¢æˆ·ç«¯
        current_model_client = AIModelClient(config)
        
        # æµ‹è¯•è¿æ¥
        logger.info(f"æ­£åœ¨æµ‹è¯•æ¨¡å‹è¿æ¥: {config.name}")
        if current_model_client.test_connection():
            message = f"âœ… æ¨¡å‹ {config.name} åŠ è½½æˆåŠŸå¹¶è¿æ¥æ­£å¸¸"
            logger.info(message)
            # æ›´æ–°é…ç½®é€‰æ‹©åˆ—è¡¨
            updated_choices = list(AIModelClient.get_all_configs().keys()) + ["è‡ªå®šä¹‰"]
            return message, gr.Dropdown(choices=updated_choices)
        else:
            message = f"âš ï¸ æ¨¡å‹ {config.name} åŠ è½½æˆåŠŸä½†è¿æ¥æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥é…ç½®"
            logger.warning(message)
            updated_choices = list(AIModelClient.get_all_configs().keys()) + ["è‡ªå®šä¹‰"]
            return message, gr.Dropdown(choices=updated_choices)
            
    except Exception as e:
        error_msg = f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}"
        logger.error(error_msg)
        return error_msg, ""

def filter_files_by_pattern(files: List[str], pattern: str, allowed_extensions: List[str]) -> List[str]:
    """æ ¹æ®æ¨¡å¼å’Œæ‰©å±•åè¿‡æ»¤æ–‡ä»¶åˆ—è¡¨"""
    if not files:
        return []
    
    filtered_files = []
    
    for file_path in files:
        file_obj = Path(file_path)
        
        # æ£€æŸ¥æ‰©å±•å
        if file_obj.suffix.lower() not in allowed_extensions:
            continue
        
        # æ£€æŸ¥æ–‡ä»¶åæ¨¡å¼
        if pattern and pattern.strip():
            # æ”¯æŒé€šé…ç¬¦åŒ¹é…
            if not fnmatch.fnmatch(file_obj.name, pattern.strip()):
                continue
        
        filtered_files.append(file_path)
    
    return filtered_files

def handle_directory_upload(files, pattern: str = "", allowed_extensions: List[str] = None) -> Tuple[str, str, gr.Dropdown]:
    """å¤„ç†ç›®å½•ä¸Šä¼ å’Œæ–‡ä»¶è¿‡æ»¤"""
    global current_dataframe, original_file_path
    
    if not files:
        return "è¯·é€‰æ‹©æ–‡ä»¶", "", gr.update(choices=[], visible=False)
    
    if allowed_extensions is None:
        allowed_extensions = [".txt", ".md", ".csv", ".xlsx", ".xls", ".pdf", ".docx", ".doc"]
    
    # è·å–æ‰€æœ‰ä¸Šä¼ çš„æ–‡ä»¶è·¯å¾„
    file_paths = [f.name for f in files]
    
    # åº”ç”¨è¿‡æ»¤å™¨
    filtered_files = filter_files_by_pattern(file_paths, pattern, allowed_extensions)
    
    if not filtered_files:
        return "âŒ æ²¡æœ‰æ‰¾åˆ°åŒ¹é…çš„æ–‡ä»¶", "", gr.update(choices=[], visible=False)
    
    # ç”Ÿæˆæ–‡ä»¶ä¿¡æ¯
    total_files = len(file_paths)
    matched_files = len(filtered_files)
    
    file_info = f"ğŸ“ æ€»æ–‡ä»¶æ•°: {total_files}\nâœ… åŒ¹é…æ–‡ä»¶æ•°: {matched_files}\nğŸ” è¿‡æ»¤æ¨¡å¼: {pattern if pattern else 'æ— '}\nğŸ“‹ å…è®¸ç±»å‹: {', '.join(allowed_extensions)}"
    
    # ç”ŸæˆåŒ¹é…æ–‡ä»¶åˆ—è¡¨æ˜¾ç¤º
    files_display = "\n".join([f"â€¢ {Path(f).name}" for f in filtered_files[:10]])
    if len(filtered_files) > 10:
        files_display += f"\n... è¿˜æœ‰ {len(filtered_files) - 10} ä¸ªæ–‡ä»¶"
    
    file_info += f"\n\nğŸ“‹ åŒ¹é…çš„æ–‡ä»¶:\n{files_display}"
    
    # é¢„è§ˆç¬¬ä¸€ä¸ªåŒ¹é…çš„æ–‡ä»¶
    preview_content = ""
    column_choices = []
    
    if filtered_files:
        try:
            first_file = filtered_files[0]
            df = read_single_file(first_file)
            if df is not None:
                # è®¾ç½®å…¨å±€å˜é‡
                current_dataframe = df
                original_file_path = first_file
                
                # è·å–åˆ—å
                column_choices = df.columns.tolist()
                
                preview_content = f"ğŸ“„ é¢„è§ˆæ–‡ä»¶: {Path(first_file).name}\n\n{df.head(3).to_string(max_cols=3, max_colwidth=50)}"
            else:
                preview_content = f"ğŸ“„ é¢„è§ˆæ–‡ä»¶: {Path(first_file).name}\nâŒ æ— æ³•è¯»å–æ–‡ä»¶å†…å®¹"
        except Exception as e:
            preview_content = f"âŒ é¢„è§ˆå¤±è´¥: {str(e)}"
    
    # è¿”å›åˆ—é€‰æ‹©ä¸‹æ‹‰èœå•çš„æ›´æ–°
    if column_choices:
        dropdown_update = gr.update(choices=column_choices, visible=True, value=None)
    else:
        dropdown_update = gr.update(choices=[], visible=False)
    
    return file_info, preview_content, dropdown_update

def read_single_file(file_path: str) -> Optional[pd.DataFrame]:
    """è¯»å–å•ä¸ªæ–‡ä»¶å¹¶è¿”å›DataFrame"""
    try:
        file_extension = Path(file_path).suffix.lower()
        
        if file_extension == '.txt':
            return FileProcessor.read_txt_file(file_path)
        elif file_extension == '.md':
            return FileProcessor.read_md_file(file_path)
        elif file_extension in ['.xlsx', '.xls']:
            df, _ = FileProcessor.read_excel_file(file_path)
            return df
        elif file_extension == '.csv':
            return FileProcessor.read_csv_file(file_path)
        elif file_extension == '.pdf':
            return FileProcessor.read_pdf_file(file_path)
        elif file_extension in ['.docx', '.doc']:
            return FileProcessor.read_docx_file(file_path)
        else:
            return None
    except Exception as e:
        logger.error(f"è¯»å–æ–‡ä»¶å¤±è´¥ {file_path}: {str(e)}")
        return None

def handle_file_upload(file) -> Tuple[str, str, gr.Dropdown]:
    """ä¸Šä¼ å¹¶è¯»å–æ–‡ä»¶"""
    global current_dataframe, original_file_path
    
    if file is None:
        return "è¯·é€‰æ‹©æ–‡ä»¶", "", gr.Dropdown(choices=[], visible=False)
    
    try:
        file_path = file.name
        original_file_path = file_path
        file_extension = Path(file_path).suffix.lower()
        
        # æ ¹æ®æ–‡ä»¶ç±»å‹è¯»å–
        if file_extension == '.txt':
            df = FileProcessor.read_txt_file(file_path)
        elif file_extension == '.md':
            df = FileProcessor.read_md_file(file_path)
        elif file_extension in ['.xlsx', '.xls']:
            df, sheet_names = FileProcessor.read_excel_file(file_path)
        elif file_extension == '.csv':
            df = FileProcessor.read_csv_file(file_path)
        elif file_extension == '.pdf':
            df = FileProcessor.read_pdf_file(file_path)
        elif file_extension in ['.docx', '.doc']:
            df = FileProcessor.read_docx_file(file_path)
        else:
            return f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file_extension}\næ”¯æŒçš„æ ¼å¼: .txt, .md, .csv, .xlsx, .xls, .pdf, .docx", "", gr.Dropdown(choices=[], visible=False)
        
        current_dataframe = df
        
        # ç”Ÿæˆåˆ—é€‰æ‹©é€‰é¡¹ï¼ˆæ”¯æŒå¤šé€‰ï¼‰
        columns = df.columns.tolist()
        
        # å¯¹äºè¡¨æ ¼æ–‡ä»¶ï¼ˆExcel/CSVï¼‰ï¼Œæ˜¾ç¤ºåˆ—é€‰æ‹©ï¼›å¯¹äºæ–‡æœ¬æ–‡ä»¶ï¼Œéšè—åˆ—é€‰æ‹©
        if file_extension in ['.xlsx', '.xls', '.csv']:
            column_dropdown = gr.Dropdown(
                choices=columns, 
                value=None,
                label="é€‰æ‹©è¦å¤„ç†çš„åˆ—",
                multiselect=True,
                visible=True
            )
        else:
            # å¯¹äºæ–‡æœ¬æ–‡ä»¶ï¼Œé»˜è®¤é€‰æ‹©ç¬¬ä¸€åˆ—
            column_dropdown = gr.Dropdown(
                choices=columns,
                value=[columns[0]] if columns else None,
                label="é€‰æ‹©è¦å¤„ç†çš„åˆ—",
                multiselect=True,
                visible=False
            )
        
        # ç”Ÿæˆé¢„è§ˆ
        preview = df.head(10).to_string(index=False, max_cols=5, max_colwidth=50)
        
        # æ·»åŠ æ–‡ä»¶ä¿¡æ¯
        file_info = f"æ–‡ä»¶ç±»å‹: {file_extension}\næ–‡ä»¶å¤§å°: {os.path.getsize(file_path) / 1024:.1f} KB\næ•°æ®è¡Œæ•°: {len(df)}\næ•°æ®åˆ—æ•°: {len(df.columns)}"
        
        return f"âœ… æ–‡ä»¶è¯»å–æˆåŠŸ\n{file_info}", preview, column_dropdown
        
    except Exception as e:
        return f"âŒ æ–‡ä»¶è¯»å–å¤±è´¥: {str(e)}", "", gr.Dropdown(choices=[], visible=False)

def get_task_prompt(task_name: str) -> str:
    """è·å–ä»»åŠ¡æç¤ºè¯"""
    return task_manager.get_task_prompt(task_name)

def add_custom_task(task_name: str, task_prompt: str) -> Tuple[str, gr.Dropdown, gr.Dropdown, gr.Dropdown]:
    """æ·»åŠ è‡ªå®šä¹‰ä»»åŠ¡"""
    if not task_name or not task_prompt:
        task_choices = task_manager.get_task_names()
        return "ä»»åŠ¡åç§°å’Œæç¤ºè¯ä¸èƒ½ä¸ºç©º", gr.Dropdown(choices=task_choices), gr.Dropdown(choices=task_choices), gr.Dropdown(choices=task_choices)
    
    try:
        task_manager.add_task(task_name, task_prompt)
        task_choices = task_manager.get_task_names()
        updated_dropdown1 = gr.Dropdown(choices=task_choices, value=task_name)
        updated_dropdown2 = gr.Dropdown(choices=task_choices, value=task_name)
        updated_dropdown3 = gr.Dropdown(choices=task_choices, value=task_name)
        return f"âœ… ä»»åŠ¡ '{task_name}' æ·»åŠ æˆåŠŸ", updated_dropdown1, updated_dropdown2, updated_dropdown3
    except Exception as e:
        task_choices = task_manager.get_task_names()
        return f"âŒ æ·»åŠ ä»»åŠ¡å¤±è´¥: {str(e)}", gr.Dropdown(choices=task_choices), gr.Dropdown(choices=task_choices), gr.Dropdown(choices=task_choices)

def delete_task(task_name: str) -> Tuple[str, gr.Dropdown, gr.Dropdown, gr.Dropdown]:
    """åˆ é™¤ä»»åŠ¡ï¼ˆé»˜è®¤ä»»åŠ¡ä¼šé‡ç½®ä¸ºåŸå§‹å€¼ï¼‰"""
    if not task_name:
        task_choices = task_manager.get_task_names()
        return "è¯·é€‰æ‹©è¦åˆ é™¤çš„ä»»åŠ¡", gr.Dropdown(choices=task_choices), gr.Dropdown(choices=task_choices), gr.Dropdown(choices=task_choices)
    
    try:
        is_default = task_manager.is_default_task(task_name)
        if task_manager.delete_task(task_name):
            task_choices = task_manager.get_task_names()
            updated_dropdown1 = gr.Dropdown(choices=task_choices)
            updated_dropdown2 = gr.Dropdown(choices=task_choices)
            updated_dropdown3 = gr.Dropdown(choices=task_choices)
            if is_default:
                return f"âœ… é»˜è®¤ä»»åŠ¡ '{task_name}' å·²é‡ç½®ä¸ºåŸå§‹å€¼", updated_dropdown1, updated_dropdown2, updated_dropdown3
            else:
                return f"âœ… è‡ªå®šä¹‰ä»»åŠ¡ '{task_name}' åˆ é™¤æˆåŠŸ", updated_dropdown1, updated_dropdown2, updated_dropdown3
        else:
            task_choices = task_manager.get_task_names()
            return f"âŒ æ— æ³•åˆ é™¤ä»»åŠ¡ '{task_name}'", gr.Dropdown(choices=task_choices), gr.Dropdown(choices=task_choices), gr.Dropdown(choices=task_choices)
    except Exception as e:
        task_choices = task_manager.get_task_names()
        return f"âŒ åˆ é™¤ä»»åŠ¡å¤±è´¥: {str(e)}", gr.Dropdown(choices=task_choices), gr.Dropdown(choices=task_choices), gr.Dropdown(choices=task_choices)

def edit_task(task_name: str, new_prompt: str) -> Tuple[str, gr.Dropdown, gr.Dropdown, gr.Dropdown]:
    """ç¼–è¾‘ä»»åŠ¡æç¤ºè¯"""
    if not task_name:
        task_choices = task_manager.get_task_names()
        return "è¯·é€‰æ‹©è¦ç¼–è¾‘çš„ä»»åŠ¡", gr.Dropdown(choices=task_choices), gr.Dropdown(choices=task_choices), gr.Dropdown(choices=task_choices)
    
    if not new_prompt.strip():
        task_choices = task_manager.get_task_names()
        return "æç¤ºè¯ä¸èƒ½ä¸ºç©º", gr.Dropdown(choices=task_choices), gr.Dropdown(choices=task_choices), gr.Dropdown(choices=task_choices)
    
    try:
        task_manager.add_task(task_name, new_prompt.strip())
        task_choices = task_manager.get_task_names()
        updated_dropdown1 = gr.Dropdown(choices=task_choices, value=task_name)
        updated_dropdown2 = gr.Dropdown(choices=task_choices, value=task_name)
        updated_dropdown3 = gr.Dropdown(choices=task_choices, value=task_name)
        is_default = task_manager.is_default_task(task_name)
        if is_default:
            return f"âœ… é»˜è®¤ä»»åŠ¡ '{task_name}' ä¿®æ”¹æˆåŠŸ", updated_dropdown1, updated_dropdown2, updated_dropdown3
        else:
            return f"âœ… ä»»åŠ¡ '{task_name}' ä¿®æ”¹æˆåŠŸ", updated_dropdown1, updated_dropdown2, updated_dropdown3
    except Exception as e:
        task_choices = task_manager.get_task_names()
        return f"âŒ ç¼–è¾‘ä»»åŠ¡å¤±è´¥: {str(e)}", gr.Dropdown(choices=task_choices), gr.Dropdown(choices=task_choices), gr.Dropdown(choices=task_choices)

def reload_tasks() -> Tuple[str, gr.Dropdown, gr.Dropdown, gr.Dropdown]:
    """é‡æ–°åŠ è½½ä»»åŠ¡"""
    try:
        task_manager.reload_tasks()
        task_choices = task_manager.get_task_names()
        updated_dropdown1 = gr.Dropdown(choices=task_choices)
        updated_dropdown2 = gr.Dropdown(choices=task_choices)
        updated_dropdown3 = gr.Dropdown(choices=task_choices)
        return "âœ… ä»»åŠ¡åˆ—è¡¨å·²é‡æ–°åŠ è½½", updated_dropdown1, updated_dropdown2, updated_dropdown3
    except Exception as e:
        task_choices = task_manager.get_task_names()
        return f"âŒ é‡æ–°åŠ è½½å¤±è´¥: {str(e)}", gr.Dropdown(choices=task_choices), gr.Dropdown(choices=task_choices), gr.Dropdown(choices=task_choices)

def process_multiple_files_stream(files, selected_columns, task_name: str, 
                                  batch_size: int = 10, max_workers: int = 3, 
                                  save_location: str = "å½“å‰æ–‡ä»¶çš„outputç›®å½•", custom_save_path: str = ""):
    """æµå¼å¤„ç†å¤šä¸ªæ–‡ä»¶ï¼ˆæ”¯æŒç›®å½•ä¸Šä¼ ï¼‰ï¼Œä½¿ç”¨æ‰¹æ¬¡å¤„ç†æ§åˆ¶å†…å­˜ä½¿ç”¨"""
    global current_model_client
    
    if current_model_client is None:
        yield "âŒ è¯·å…ˆåŠ è½½AIæ¨¡å‹", "", 0.0
        return
    
    if not files:
        yield "âŒ è¯·å…ˆä¸Šä¼ æ–‡ä»¶", "", 0.0
        return
    
    if not task_name:
        yield "âŒ è¯·é€‰æ‹©å¤„ç†ä»»åŠ¡", "", 0.0
        return
    
    try:
        # è·å–ä»»åŠ¡æç¤ºè¯
        prompt = task_manager.get_task_prompt(task_name)
        if not prompt:
            yield "âŒ é€‰æ‹©çš„ä»»åŠ¡æ— æ•ˆ", "", 0.0
            return
        
        # è·å–æ–‡ä»¶è·¯å¾„åˆ—è¡¨
        file_paths = [f.name for f in files]
        total_files = len(file_paths)
        
        processing_log = []
        processing_log.append(f"ğŸ“ å¼€å§‹å¤„ç† {total_files} ä¸ªæ–‡ä»¶")
        processing_log.append(f"ğŸ“¦ æ‰¹æ¬¡å¤§å°: {batch_size}, å¹¶å‘æ•°: {max_workers}")
        processing_log.append(f"ğŸ’¡ æ‰¹æ¬¡å¤„ç†ç”¨äºæ§åˆ¶å¤šæ–‡ä»¶å¤„ç†æ—¶çš„å†…å­˜ä½¿ç”¨")
        
        all_results = []
        total_processed_items = 0
        
        for file_idx, file_path in enumerate(file_paths):
            # æ£€æŸ¥ä¸­æ–­
            if is_processing_interrupted():
                processing_log.append("âš ï¸ å¤„ç†å·²è¢«ç”¨æˆ·ä¸­æ–­")
                yield "\n".join(processing_log), "", (file_idx / total_files) * 100
                return
            
            processing_log.append(f"\nğŸ“„ å¤„ç†æ–‡ä»¶ {file_idx + 1}/{total_files}: {os.path.basename(file_path)}")
            yield "\n".join(processing_log), "", (file_idx / total_files) * 100
            
            # è¯»å–å•ä¸ªæ–‡ä»¶
            df = read_single_file(file_path)
            if df is None:
                processing_log.append(f"âŒ æ— æ³•è¯»å–æ–‡ä»¶: {os.path.basename(file_path)}")
                continue
            
            # è®¾ç½®å…¨å±€å˜é‡ä»¥ä¾¿ç°æœ‰å‡½æ•°ä½¿ç”¨
            global current_dataframe, original_file_path
            current_dataframe = df
            original_file_path = file_path
            
            # å¤„ç†å•ä¸ªæ–‡ä»¶ï¼ˆå¤šæ–‡ä»¶æ¨¡å¼ä¸‹ä½¿ç”¨æ‰¹æ¬¡å¤„ç†ï¼‰
            file_processed = False
            for log, preview, progress in process_data_stream_single_file_with_batch(df, file_path, selected_columns, task_name, batch_size, max_workers, save_location, custom_save_path):
                if "âŒ" in log:
                    processing_log.append(f"âŒ æ–‡ä»¶å¤„ç†å¤±è´¥: {os.path.basename(file_path)}")
                    break
                elif "âœ…" in log:
                    file_processed = True
                    total_processed_items += 1
                    processing_log.append(f"âœ… æ–‡ä»¶å¤„ç†å®Œæˆ: {os.path.basename(file_path)}")
                    if preview:
                        all_results.append(f"æ–‡ä»¶: {os.path.basename(file_path)}\n{preview}")
                    break
                
                # æ›´æ–°è¿›åº¦
                file_progress = (file_idx + progress / 100) / total_files * 100
                yield "\n".join(processing_log), preview, file_progress
        
        # ç”Ÿæˆæœ€ç»ˆç»“æœ
        final_preview = "\n\n" + "="*50 + "\n\n".join(all_results) if all_results else "æ²¡æœ‰æˆåŠŸå¤„ç†çš„æ–‡ä»¶"
        processing_log.append(f"\nğŸ‰ æ‰¹é‡å¤„ç†å®Œæˆï¼æˆåŠŸå¤„ç† {total_processed_items}/{total_files} ä¸ªæ–‡ä»¶")
        
        yield "\n".join(processing_log), final_preview, 100.0
        
    except Exception as e:
        yield f"âŒ æ‰¹é‡å¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}", "", 0.0

def process_data_stream_single_file_with_batch(df, file_path, selected_columns, task_name: str, 
                                               batch_size: int = 10, max_workers: int = 3, 
                                               save_location: str = "å½“å‰æ–‡ä»¶çš„outputç›®å½•", custom_save_path: str = ""):
    """å¤„ç†å•ä¸ªæ–‡ä»¶çš„æµå¼å‡½æ•°ï¼Œä½¿ç”¨æ‰¹æ¬¡å¤„ç†æ§åˆ¶å†…å­˜ä½¿ç”¨ï¼ˆç”¨äºå¤šæ–‡ä»¶å¤„ç†ï¼‰"""
    global current_model_client
    
    if current_model_client is None:
        yield "âŒ è¯·å…ˆåŠ è½½AIæ¨¡å‹", "", 0.0
        return
    
    # å¤„ç†å¤šåˆ—é€‰æ‹©
    if isinstance(selected_columns, str):
        columns_to_process = [selected_columns]
    else:
        columns_to_process = selected_columns if selected_columns else []
    
    if not columns_to_process:
        yield "âŒ è¯·é€‰æ‹©è¦å¤„ç†çš„åˆ—", "", 0.0
        return
    
    # éªŒè¯åˆ—æ˜¯å¦å­˜åœ¨
    missing_columns = [col for col in columns_to_process if col not in df.columns]
    if missing_columns:
        yield f"âŒ ä»¥ä¸‹åˆ—ä¸å­˜åœ¨: {', '.join(missing_columns)}", "", 0.0
        return
    
    try:
        # è·å–ä»»åŠ¡æç¤ºè¯
        prompt = task_manager.get_task_prompt(task_name)
        if not prompt:
            yield "âŒ é€‰æ‹©çš„ä»»åŠ¡æ— æ•ˆ", "", 0.0
            return
        
        processing_log = []
        processing_log.append(f"ğŸ“ æ­£åœ¨å¤„ç†æ–‡ä»¶: {os.path.basename(file_path)}")
        processing_log.append(f"ğŸ“¦ æ‰¹æ¬¡å¤§å°: {batch_size}ï¼Œå¹¶å‘æ•°: {max_workers}")
        processing_log.append(f"ğŸ“ å¤„ç†ä»»åŠ¡: {task_name}")
        processing_log.append("-" * 40)
        
        # åˆ›å»ºå¤„ç†åçš„DataFrameå‰¯æœ¬
        processed_df = df.copy()
        total_processed = 0
        
        # å¤„ç†æ¯ä¸€åˆ—
        for col_index, column in enumerate(columns_to_process):
            processing_log.append(f"\nğŸ”„ æ­£åœ¨å¤„ç†åˆ—: {column} ({col_index + 1}/{len(columns_to_process)})")
            
            # è·å–è¦å¤„ç†çš„æ•°æ®
            data_to_process = df[column].astype(str).tolist()
            
            # è¿‡æ»¤ç©ºå€¼å¹¶ä¿å­˜åŸå§‹ç´¢å¼•
            indexed_data = [(i, item) for i, item in enumerate(data_to_process) if item.strip()]
            
            if not indexed_data:
                processing_log.append(f"âš ï¸ åˆ— {column} ä¸­æ²¡æœ‰æœ‰æ•ˆæ•°æ®ï¼Œè·³è¿‡")
                continue
            
            total_items = len(indexed_data)
            processing_log.append(f"ğŸ“ è¯¥åˆ—æœ‰æ•ˆæ•°æ®: {total_items} æ¡")
            
            # æ‰¹æ¬¡å¤„ç†æ•°æ®ï¼ˆç”¨äºå¤šæ–‡ä»¶å¤„ç†æ—¶æ§åˆ¶å†…å­˜ï¼‰
            processed_count = 0
            new_column_name = f"{column}_processed"
            processed_df[new_column_name] = processed_df[column]  # åˆå§‹åŒ–å¤„ç†åçš„åˆ—
            
            # åˆ†æ‰¹å¤„ç†æ•°æ®
            for batch_start in range(0, total_items, batch_size):
                # æ£€æŸ¥æ˜¯å¦è¯·æ±‚ä¸­æ–­
                if is_processing_interrupted():
                    processing_log.append(f"\nâ¹ï¸ ç”¨æˆ·è¯·æ±‚ä¸­æ–­ï¼Œæ­£åœ¨ä¿å­˜å½“å‰å·²å¤„ç†çš„ç»“æœ...")
                    break
                
                batch_end = min(batch_start + batch_size, total_items)
                batch_data = indexed_data[batch_start:batch_end]
                
                processing_log.append(f"ğŸ“¦ å¤„ç†æ‰¹æ¬¡ {batch_start//batch_size + 1}: {batch_start+1}-{batch_end}/{total_items}")
                
                # ä½¿ç”¨çº¿ç¨‹æ± å¤„ç†å½“å‰æ‰¹æ¬¡
                with ThreadPoolExecutor(max_workers=max_workers) as executor:
                    # æäº¤æ‰¹æ¬¡ä¸­çš„æ‰€æœ‰ä»»åŠ¡
                    future_to_index = {}
                    for index, item in batch_data:
                        if len(item) > 10000:
                            item = item[:10000] + "...[æ–‡æœ¬è¿‡é•¿ï¼Œå·²æˆªæ–­]"
                        future = executor.submit(current_model_client.process_text, item, prompt)
                        future_to_index[future] = index
                    
                    # æ”¶é›†æ‰¹æ¬¡ç»“æœ
                    batch_processed = 0
                    for future in as_completed(future_to_index):
                        try:
                            result = future.result(timeout=60)
                            index = future_to_index[future]
                            processed_df.loc[index, new_column_name] = result
                            batch_processed += 1
                            processed_count += 1
                            total_processed += 1
                        except Exception as e:
                            index = future_to_index[future]
                            processed_df.loc[index, new_column_name] = f"å¤„ç†å¤±è´¥: {str(e)}"
                            processing_log.append(f"âš ï¸ å¤„ç†å¤±è´¥ (è¡Œ{index}): {str(e)}")
                
                # æ›´æ–°è¿›åº¦
                column_progress = (processed_count / total_items) * 100
                overall_progress = ((col_index * 100) + column_progress) / len(columns_to_process)
                
                processing_log.append(f"âœ… æ‰¹æ¬¡å®Œæˆï¼Œå·²å¤„ç†: {processed_count}/{total_items}")
                yield "\n".join(processing_log), "", overall_progress
                
                # å¦‚æœè¢«ä¸­æ–­ï¼Œè·³å‡ºå¾ªç¯
                if is_processing_interrupted():
                    break
            
            # å¦‚æœè¢«ä¸­æ–­ï¼Œè·³å‡ºåˆ—å¾ªç¯
            if is_processing_interrupted():
                break
        
        # ä¿å­˜å¤„ç†ç»“æœ
        try:
            if save_location == "è‡ªå®šä¹‰ç›®å½•" and custom_save_path.strip():
                output_dir = Path(custom_save_path.strip())
            else:
                output_dir = Path(file_path).parent / "output"
            
            output_dir.mkdir(exist_ok=True)
            
            # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶å
            base_name = Path(file_path).stem
            output_file = output_dir / f"{base_name}_processed.xlsx"
            
            # ä¿å­˜æ–‡ä»¶
            processed_df.to_excel(output_file, index=False)
            
            processing_log.append(f"\nğŸ’¾ æ–‡ä»¶å·²ä¿å­˜: {output_file}")
            processing_log.append(f"âœ… å¤„ç†å®Œæˆï¼å…±å¤„ç† {total_processed} æ¡æ•°æ®")
            
            # ç”Ÿæˆé¢„è§ˆ
            preview = generate_result_preview(processed_df, [f"{col}_processed" for col in columns_to_process])
            
            yield "\n".join(processing_log), preview, 100.0
            
        except Exception as e:
            processing_log.append(f"âŒ ä¿å­˜æ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
            yield "\n".join(processing_log), "", 100.0
    
    except Exception as e:
        yield f"âŒ å¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}", "", 0.0

def process_data_stream_single_file(df, file_path, selected_columns, task_name: str, 
                                   batch_size: int = 10, max_workers: int = 3, 
                                   save_location: str = "å½“å‰æ–‡ä»¶çš„outputç›®å½•", custom_save_path: str = ""):
    """å¤„ç†å•ä¸ªæ–‡ä»¶çš„æµå¼å‡½æ•°ï¼Œå•ä¸ªæ–‡ä»¶ä¸ä½¿ç”¨æ‰¹æ¬¡å¤„ç†"""
    global current_model_client
    
    if current_model_client is None:
        yield "âŒ è¯·å…ˆåŠ è½½AIæ¨¡å‹", "", 0.0
        return
    
    # å¤„ç†å¤šåˆ—é€‰æ‹©
    if isinstance(selected_columns, str):
        columns_to_process = [selected_columns]
    else:
        columns_to_process = selected_columns if selected_columns else []
    
    if not columns_to_process:
        yield "âŒ è¯·é€‰æ‹©è¦å¤„ç†çš„åˆ—", "", 0.0
        return
    
    # éªŒè¯åˆ—æ˜¯å¦å­˜åœ¨
    missing_columns = [col for col in columns_to_process if col not in df.columns]
    if missing_columns:
        yield f"âŒ ä»¥ä¸‹åˆ—ä¸å­˜åœ¨: {', '.join(missing_columns)}", "", 0.0
        return
    
    try:
        # è·å–ä»»åŠ¡æç¤ºè¯
        prompt = task_manager.get_task_prompt(task_name)
        if not prompt:
            yield "âŒ é€‰æ‹©çš„ä»»åŠ¡æ— æ•ˆ", "", 0.0
            return
        
        processing_log = []
        processing_log.append(f"ğŸ“ æ­£åœ¨å¤„ç†æ–‡ä»¶: {os.path.basename(file_path)}")
        processing_log.append(f"âš™ï¸ å¹¶å‘æ•°: {max_workers}")
        processing_log.append(f"ğŸ“ å¤„ç†ä»»åŠ¡: {task_name}")
        processing_log.append("-" * 40)
        
        # åˆ›å»ºå¤„ç†åçš„DataFrameå‰¯æœ¬
        processed_df = df.copy()
        total_processed = 0
        
        # å¤„ç†æ¯ä¸€åˆ—
        for col_index, column in enumerate(columns_to_process):
            processing_log.append(f"\nğŸ”„ æ­£åœ¨å¤„ç†åˆ—: {column} ({col_index + 1}/{len(columns_to_process)})")
            
            # è·å–è¦å¤„ç†çš„æ•°æ®
            data_to_process = df[column].astype(str).tolist()
            
            # è¿‡æ»¤ç©ºå€¼å¹¶ä¿å­˜åŸå§‹ç´¢å¼•
            indexed_data = [(i, item) for i, item in enumerate(data_to_process) if item.strip()]
            
            if not indexed_data:
                processing_log.append(f"âš ï¸ åˆ— {column} ä¸­æ²¡æœ‰æœ‰æ•ˆæ•°æ®ï¼Œè·³è¿‡")
                continue
            
            total_items = len(indexed_data)
            processing_log.append(f"ğŸ“ è¯¥åˆ—æœ‰æ•ˆæ•°æ®: {total_items} æ¡")
            
            # å•ä¸ªæ–‡ä»¶ä¸€æ¬¡æ€§å¤„ç†æ‰€æœ‰æ•°æ®ï¼ˆä¸åˆ†æ‰¹ï¼‰
            processed_count = 0
            new_column_name = f"{column}_processed"
            processed_df[new_column_name] = processed_df[column]  # åˆå§‹åŒ–å¤„ç†åçš„åˆ—
            
            processing_log.append(f"ğŸš€ å¼€å§‹å¤„ç†æ‰€æœ‰æ•°æ®...")
            yield "\n".join(processing_log), "", (col_index / len(columns_to_process)) * 100
            
            # ä½¿ç”¨çº¿ç¨‹æ± ä¸€æ¬¡æ€§å¤„ç†æ‰€æœ‰æ•°æ®
            with ThreadPoolExecutor(max_workers=max_workers) as executor:
                # æäº¤æ‰€æœ‰ä»»åŠ¡
                future_to_index = {}
                for index, item in indexed_data:
                    if len(item) > 10000:
                        item = item[:10000] + "...[æ–‡æœ¬è¿‡é•¿ï¼Œå·²æˆªæ–­]"
                    future = executor.submit(current_model_client.process_text, item, prompt)
                    future_to_index[future] = index
                
                # æ”¶é›†æ‰€æœ‰ç»“æœ
                for future in as_completed(future_to_index):
                    # æ£€æŸ¥æ˜¯å¦è¯·æ±‚ä¸­æ–­
                    if is_processing_interrupted():
                        processing_log.append(f"\nâ¹ï¸ ç”¨æˆ·è¯·æ±‚ä¸­æ–­ï¼Œæ­£åœ¨ä¿å­˜å½“å‰å·²å¤„ç†çš„ç»“æœ...")
                        # å–æ¶ˆæœªå®Œæˆçš„ä»»åŠ¡
                        for f in future_to_index:
                            if not f.done():
                                f.cancel()
                        break
                    
                    try:
                        result = future.result(timeout=60)
                        index = future_to_index[future]
                        processed_df.loc[index, new_column_name] = result
                        processed_count += 1
                        total_processed += 1
                        
                        # æ¯å¤„ç†10ä¸ªé¡¹ç›®æ›´æ–°ä¸€æ¬¡è¿›åº¦
                        if processed_count % 10 == 0 or processed_count == total_items:
                            column_progress = (processed_count / total_items) * 100
                            overall_progress = ((col_index + column_progress / 100) / len(columns_to_process)) * 100
                            processing_log[-1] = f"ğŸ”„ å¤„ç†è¿›åº¦: {processed_count}/{total_items} ({column_progress:.1f}%)"
                            yield "\n".join(processing_log), "", overall_progress
                            
                    except Exception as e:
                        index = future_to_index[future]
                        processed_df.loc[index, new_column_name] = f"å¤„ç†å¤±è´¥: {str(e)}"
                        processing_log.append(f"âš ï¸ å¤„ç†å¤±è´¥ (è¡Œ{index}): {str(e)}")
            
            processing_log.append(f"âœ… åˆ— {column} å¤„ç†å®Œæˆï¼Œå·²å¤„ç†: {processed_count}/{total_items}")
            
            # å¦‚æœè¢«ä¸­æ–­ï¼Œè·³å‡ºåˆ—å¾ªç¯
            if is_processing_interrupted():
                break
        
        # ä¿å­˜å¤„ç†ç»“æœ
        try:
            if save_location == "è‡ªå®šä¹‰ç›®å½•" and custom_save_path.strip():
                output_dir = Path(custom_save_path.strip())
            else:
                output_dir = Path(file_path).parent / "output"
            
            output_dir.mkdir(exist_ok=True)
            
            # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶å
            base_name = Path(file_path).stem
            output_file = output_dir / f"{base_name}_processed.xlsx"
            
            # ä¿å­˜æ–‡ä»¶
            processed_df.to_excel(output_file, index=False)
            
            processing_log.append(f"\nğŸ’¾ æ–‡ä»¶å·²ä¿å­˜: {output_file}")
            processing_log.append(f"âœ… å¤„ç†å®Œæˆï¼å…±å¤„ç† {total_processed} æ¡æ•°æ®")
            
            # ç”Ÿæˆé¢„è§ˆ
            preview = generate_result_preview(processed_df, [f"{col}_processed" for col in columns_to_process])
            
            yield "\n".join(processing_log), preview, 100.0
            
        except Exception as e:
            processing_log.append(f"âŒ ä¿å­˜æ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
            yield "\n".join(processing_log), "", 100.0
    
    except Exception as e:
        yield f"âŒ å¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}", "", 0.0

def process_data_stream(file_upload, selected_columns, task_name: str, 
                        batch_size: int = 10, max_workers: int = 3, 
                        save_location: str = "å½“å‰æ–‡ä»¶çš„outputç›®å½•", custom_save_path: str = ""):
    """æµå¼å¤„ç†æ•°æ®ï¼ˆæ”¯æŒå¤šçº¿ç¨‹å’Œå¤šåˆ—é€‰æ‹©ï¼Œå®æ—¶è¿›åº¦æ˜¾ç¤ºï¼‰"""
    global current_model_client, current_dataframe, original_file_path
    
    # æ£€æŸ¥æ˜¯å¦ä¸ºå¤šæ–‡ä»¶ä¸Šä¼ 
    if isinstance(file_upload, list) and len(file_upload) > 1:
        # å¤šæ–‡ä»¶å¤„ç†
        for log, preview, progress in process_multiple_files_stream(file_upload, selected_columns, task_name, batch_size, max_workers, save_location, custom_save_path):
            yield log, preview, progress
        return
    
    # å•æ–‡ä»¶å¤„ç†ï¼ˆåŸæœ‰é€»è¾‘ï¼‰
    if current_model_client is None:
        yield "âŒ è¯·å…ˆåŠ è½½AIæ¨¡å‹", "", 0.0
        return
    
    if current_dataframe is None:
        yield "âŒ è¯·å…ˆä¸Šä¼ æ–‡ä»¶", "", 0.0
        return
    
    if not selected_columns:
        yield "âŒ è¯·é€‰æ‹©è¦å¤„ç†çš„åˆ—", "", 0.0
        return
    
    if not task_name:
        yield "âŒ è¯·é€‰æ‹©å¤„ç†ä»»åŠ¡", "", 0.0
        return
    
    try:
        # è·å–ä»»åŠ¡æç¤ºè¯
        prompt = task_manager.get_task_prompt(task_name)
        if not prompt:
            yield "âŒ é€‰æ‹©çš„ä»»åŠ¡æ— æ•ˆ", "", 0.0
            return
        
        # å¤„ç†å¤šåˆ—é€‰æ‹©
        if isinstance(selected_columns, str):
            columns_to_process = [selected_columns]
        else:
            columns_to_process = selected_columns if selected_columns else []
        
        if not columns_to_process:
            yield "âŒ è¯·é€‰æ‹©è¦å¤„ç†çš„åˆ—", "", 0.0
            return
        
        # éªŒè¯åˆ—æ˜¯å¦å­˜åœ¨
        missing_columns = [col for col in columns_to_process if col not in current_dataframe.columns]
        if missing_columns:
            yield f"âŒ ä»¥ä¸‹åˆ—ä¸å­˜åœ¨: {', '.join(missing_columns)}", "", 0.0
            return
        
        total_processed = 0
        processing_log = []
        overall_progress = 0.0
        total_start_time = time.time()  # è®°å½•æ€»å¼€å§‹æ—¶é—´
        
        # è®¡ç®—æ€»çš„å¤„ç†é¡¹ç›®æ•°
        total_items_all_columns = 0
        for column in columns_to_process:
            data_to_process = current_dataframe[column].astype(str).tolist()
            indexed_data = [(i, item) for i, item in enumerate(data_to_process) if item.strip()]
            total_items_all_columns += len(indexed_data)
        
        # åˆå§‹åŒ–ä¿¡æ¯
        processing_log.append(f"ğŸ“Š å¼€å§‹å¤„ç† {len(columns_to_process)} åˆ—ï¼Œå…± {total_items_all_columns} æ¡æ•°æ®")
        processing_log.append(f"âš™ï¸ ä½¿ç”¨ {max_workers} ä¸ªçº¿ç¨‹å¹¶å‘å¤„ç†")
        processing_log.append(f"ğŸ“ é»˜è®¤è¾“å‡ºç›®å½•: {Path(__file__).parent / 'output'}")
        processing_log.append(f"ğŸ“‚ ä¿å­˜ä½ç½®è®¾ç½®: {save_location}")
        if save_location == "è‡ªå®šä¹‰ç›®å½•" and custom_save_path.strip():
            processing_log.append(f"ğŸ“ è‡ªå®šä¹‰è·¯å¾„: {custom_save_path.strip()}")
        processing_log.append(f"ğŸ“ å¤„ç†ä»»åŠ¡: {task_name}")
        processing_log.append("" + "="*50)
        
        # è¾“å‡ºåˆå§‹çŠ¶æ€
        yield "\n".join(processing_log), "", 0.0
        
        global_processed_count = 0
        
        # é‡ç½®ä¸­æ–­æ ‡å¿—
        set_processing_interrupted(False)
        
        # å¤„ç†æ¯ä¸€åˆ—
        for col_index, column in enumerate(columns_to_process):
            # æ£€æŸ¥æ˜¯å¦è¯·æ±‚ä¸­æ–­
            if is_processing_interrupted():
                processing_log.append(f"\nâ¹ï¸ ç”¨æˆ·è¯·æ±‚ä¸­æ–­ï¼Œæ­£åœ¨ä¿å­˜å½“å‰å·²å¤„ç†çš„ç»“æœ...")
                # ç«‹å³ä¿å­˜å½“å‰å·²å¤„ç†çš„ç»“æœ
                yield "\n".join(processing_log), "", overall_progress
                break
                
            processing_log.append(f"\nğŸ”„ æ­£åœ¨å¤„ç†åˆ—: {column} ({col_index + 1}/{len(columns_to_process)})")
            processing_log.append(f"â° å¼€å§‹æ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}")
            
            # è·å–è¦å¤„ç†çš„æ•°æ®
            data_to_process = current_dataframe[column].astype(str).tolist()
            
            # è¿‡æ»¤ç©ºå€¼å¹¶ä¿å­˜åŸå§‹ç´¢å¼•
            indexed_data = [(i, item) for i, item in enumerate(data_to_process) if item.strip()]
            
            if not indexed_data:
                processing_log.append(f"âš ï¸ åˆ— {column} ä¸­æ²¡æœ‰æœ‰æ•ˆæ•°æ®ï¼Œè·³è¿‡")
                yield "\n".join(processing_log), "", overall_progress
                continue
            
            total_items = len(indexed_data)
            processing_log.append(f"ğŸ“ è¯¥åˆ—æœ‰æ•ˆæ•°æ®: {total_items} æ¡")
            processing_log.append(f"ğŸ“Š æ•°æ®é•¿åº¦ç»Ÿè®¡: å¹³å‡ {sum(len(str(item[1])) for item in indexed_data) / len(indexed_data):.0f} å­—ç¬¦")
            processing_log.append(f"ğŸš€ å¼€å§‹å¹¶å‘å¤„ç†ï¼Œçº¿ç¨‹æ•°: {max_workers}")
            processing_log.append("-" * 40)
            
            # è¾“å‡ºåˆ—å¼€å§‹å¤„ç†çŠ¶æ€
            yield "\n".join(processing_log), "", overall_progress
            
            # åˆ›å»ºç»“æœå­—å…¸
            results_dict = {}
            processed_count = 0
            start_time = time.time()
            
            def process_single_item(indexed_item):
                """å¤„ç†å•ä¸ªæ–‡æœ¬é¡¹ï¼ŒåŒ…å«é‡è¯•æœºåˆ¶"""
                index, item = indexed_item
                max_retries = 3
                retry_count = 0
                
                while retry_count <= max_retries:
                    try:
                        # é™åˆ¶å•ä¸ªæ–‡æœ¬é•¿åº¦
                        if len(item) > 10000:
                            item = item[:10000] + "...[æ–‡æœ¬è¿‡é•¿ï¼Œå·²æˆªæ–­]"
                        
                        result = current_model_client.process_text(item, prompt)
                        if retry_count > 0:
                            logger.info(f"ç¬¬ {index+1} é¡¹åœ¨ç¬¬ {retry_count+1} æ¬¡å°è¯•åæˆåŠŸå¤„ç†")
                        return index, result, True
                        
                    except (openai.APIConnectionError, ConnectionError) as e:
                        retry_count += 1
                        if retry_count <= max_retries:
                            wait_time = min(2 ** retry_count, 10)  # æŒ‡æ•°é€€é¿ï¼Œæœ€å¤§10ç§’
                            logger.warning(f"ç¬¬ {index+1} é¡¹è¿æ¥é”™è¯¯ (å°è¯• {retry_count}/{max_retries+1}): {str(e)}ï¼Œ{wait_time}ç§’åé‡è¯•")
                            time.sleep(wait_time)
                        else:
                            error_msg = f"å¤„ç†å¤±è´¥: è¿æ¥é”™è¯¯ - {str(e)} (å·²é‡è¯•{max_retries}æ¬¡)"
                            logger.error(f"ç¬¬ {index+1} é¡¹æœ€ç»ˆå¤±è´¥: {error_msg}")
                            return index, error_msg, False
                            
                    except (openai.APITimeoutError, TimeoutError) as e:
                        retry_count += 1
                        if retry_count <= max_retries:
                            wait_time = min(2 ** retry_count, 10)
                            logger.warning(f"ç¬¬ {index+1} é¡¹è¶…æ—¶é”™è¯¯ (å°è¯• {retry_count}/{max_retries+1}): {str(e)}ï¼Œ{wait_time}ç§’åé‡è¯•")
                            time.sleep(wait_time)
                        else:
                            error_msg = f"å¤„ç†å¤±è´¥: è¶…æ—¶é”™è¯¯ - {str(e)} (å·²é‡è¯•{max_retries}æ¬¡)"
                            logger.error(f"ç¬¬ {index+1} é¡¹æœ€ç»ˆå¤±è´¥: {error_msg}")
                            return index, error_msg, False
                            
                    except openai.RateLimitError as e:
                        retry_count += 1
                        if retry_count <= max_retries:
                            wait_time = min(5 * retry_count, 30)  # é€Ÿç‡é™åˆ¶éœ€è¦æ›´é•¿ç­‰å¾…æ—¶é—´
                            logger.warning(f"ç¬¬ {index+1} é¡¹é€Ÿç‡é™åˆ¶ (å°è¯• {retry_count}/{max_retries+1}): {str(e)}ï¼Œ{wait_time}ç§’åé‡è¯•")
                            time.sleep(wait_time)
                        else:
                            error_msg = f"å¤„ç†å¤±è´¥: é€Ÿç‡é™åˆ¶ - {str(e)} (å·²é‡è¯•{max_retries}æ¬¡)"
                            logger.error(f"ç¬¬ {index+1} é¡¹æœ€ç»ˆå¤±è´¥: {error_msg}")
                            return index, error_msg, False
                            
                    except Exception as e:
                        # å¯¹äºå…¶ä»–ç±»å‹çš„é”™è¯¯ï¼Œåªé‡è¯•ä¸€æ¬¡
                        if retry_count == 0:
                            retry_count += 1
                            wait_time = 2
                            logger.warning(f"ç¬¬ {index+1} é¡¹æœªçŸ¥é”™è¯¯ (å°è¯• {retry_count}/{max_retries+1}): {str(e)}ï¼Œ{wait_time}ç§’åé‡è¯•")
                            time.sleep(wait_time)
                        else:
                            error_msg = f"å¤„ç†å¤±è´¥: {str(e)}"
                            logger.error(f"ç¬¬ {index+1} é¡¹æœ€ç»ˆå¤±è´¥: {error_msg}")
                            return index, error_msg, False
            
            # å®ç°çœŸæ­£çš„æ‰¹æ¬¡å¤„ç†é€»è¾‘
            processing_log.append(f"ğŸ“¦ æ‰¹æ¬¡å¤§å°: {batch_size} æ¡/æ‰¹æ¬¡")
            total_batches = (total_items + batch_size - 1) // batch_size
            processing_log.append(f"ğŸ“Š æ€»æ‰¹æ¬¡æ•°: {total_batches} æ‰¹æ¬¡")
            yield "\n".join(processing_log), "", overall_progress
            
            # æŒ‰æ‰¹æ¬¡å¤„ç†æ•°æ®
            for batch_num in range(total_batches):
                # æ£€æŸ¥æ˜¯å¦è¯·æ±‚ä¸­æ–­
                if is_processing_interrupted():
                    processing_log.append(f"\nâ¹ï¸ æ£€æµ‹åˆ°ä¸­æ–­è¯·æ±‚ï¼Œåœæ­¢å½“å‰åˆ—çš„å¤„ç†...")
                    yield "\n".join(processing_log), "", overall_progress
                    break
                
                # è®¡ç®—å½“å‰æ‰¹æ¬¡çš„æ•°æ®èŒƒå›´
                batch_start = batch_num * batch_size
                batch_end = min(batch_start + batch_size, total_items)
                batch_data = indexed_data[batch_start:batch_end]
                
                processing_log.append(f"\nğŸ“¦ å¤„ç†æ‰¹æ¬¡ {batch_num + 1}/{total_batches} (æ•°æ® {batch_start + 1}-{batch_end})")
                yield "\n".join(processing_log), "", overall_progress
                
                # ä½¿ç”¨çº¿ç¨‹æ± å¤„ç†å½“å‰æ‰¹æ¬¡
                with ThreadPoolExecutor(max_workers=max_workers) as executor:
                    # æäº¤å½“å‰æ‰¹æ¬¡çš„ä»»åŠ¡
                    future_to_index = {executor.submit(process_single_item, item): item[0] for item in batch_data}
                    
                    # æ”¶é›†å½“å‰æ‰¹æ¬¡çš„ç»“æœ
                    batch_processed = 0
                    for future in as_completed(future_to_index):
                        # æ£€æŸ¥æ˜¯å¦è¯·æ±‚ä¸­æ–­
                        if is_processing_interrupted():
                            processing_log.append(f"\nâ¹ï¸ æ£€æµ‹åˆ°ä¸­æ–­è¯·æ±‚ï¼Œåœæ­¢å½“å‰æ‰¹æ¬¡çš„å¤„ç†...")
                            # å–æ¶ˆæœªå®Œæˆçš„ä»»åŠ¡
                            for f in future_to_index:
                                if not f.done():
                                    f.cancel()
                            yield "\n".join(processing_log), "", overall_progress
                            break
                            
                        try:
                            index, result, success = future.result()
                            results_dict[index] = result
                            processed_count += 1
                            global_processed_count += 1
                            batch_processed += 1
                            
                            # è®¡ç®—è¿›åº¦
                            column_progress = processed_count / total_items * 100
                            overall_progress = global_processed_count / total_items_all_columns * 100
                            
                            # è®¡ç®—å¤„ç†é€Ÿåº¦
                            elapsed_time = time.time() - start_time
                            if elapsed_time > 0:
                                speed = processed_count / elapsed_time
                                remaining_items = total_items - processed_count
                                eta = remaining_items / speed if speed > 0 else 0
                                
                                # æ‰¹æ¬¡å†…è¿›åº¦æ˜¾ç¤ºï¼ˆæ¯å¤„ç†5ä¸ªæˆ–æ‰¹æ¬¡å®Œæˆæ—¶æ›´æ–°ï¼‰
                                if batch_processed % 5 == 0 or batch_processed == len(batch_data):
                                    current_log = processing_log.copy()
                                    status_msg = (
                                        f"ğŸ”„ å®æ—¶çŠ¶æ€ | åˆ—: {column} ({col_index + 1}/{len(columns_to_process)}) | "
                                        f"æ‰¹æ¬¡: {batch_num + 1}/{total_batches} | "
                                        f"æ‰¹æ¬¡è¿›åº¦: {batch_processed}/{len(batch_data)} | "
                                        f"å½“å‰åˆ—è¿›åº¦: {processed_count}/{total_items} ({column_progress:.1f}%) | "
                                        f"æ€»ä½“è¿›åº¦: {global_processed_count}/{total_items_all_columns} ({overall_progress:.1f}%) | "
                                        f"å¤„ç†é€Ÿåº¦: {speed:.1f}æ¡/ç§’ | é¢„è®¡å‰©ä½™æ—¶é—´: {eta:.0f}ç§’"
                                    )
                                    current_log.append(status_msg)
                                    
                                    # æ˜¾ç¤ºæœ€è¿‘å¤„ç†çš„å†…å®¹é¢„è§ˆ
                                    if success and len(result) > 0:
                                        preview_text = result[:50] + "..." if len(result) > 50 else result
                                        current_log.append(f"   âœ… æœ€æ–°å¤„ç†ç»“æœé¢„è§ˆ: {preview_text}")
                                    elif not success:
                                        current_log.append(f"   âŒ å¤„ç†å¤±è´¥: {result}")
                                    
                                    # å®æ—¶è¾“å‡ºçŠ¶æ€
                                    yield "\n".join(current_log), "", overall_progress
                            
                        except Exception as e:
                            logger.error(f"è·å–å¤„ç†ç»“æœå¤±è´¥: {str(e)}")
                            global_processed_count += 1
                            batch_processed += 1
                
                # æ‰¹æ¬¡å®Œæˆåçš„çŠ¶æ€æ›´æ–°
                if not is_processing_interrupted():
                    processing_log.append(f"   âœ… æ‰¹æ¬¡ {batch_num + 1} å®Œæˆï¼Œå¤„ç†äº† {batch_processed} æ¡æ•°æ®")
                    yield "\n".join(processing_log), "", overall_progress
                else:
                    break
            
            # æ„å»ºå®Œæ•´çš„ç»“æœåˆ—è¡¨
            full_results = []
            for i, original_item in enumerate(data_to_process):
                if i in results_dict:
                    full_results.append(results_dict[i])
                else:
                    full_results.append("" if not original_item.strip() else "å¤„ç†å¤±è´¥")
            
            # æ·»åŠ ç»“æœåˆ—åˆ°DataFrame
            result_column_name = f"{column}_processed"
            current_dataframe[result_column_name] = full_results
            
            successful_count = len([r for r in full_results if r and not r.startswith('å¤„ç†å¤±è´¥')])
            failed_count = total_items - successful_count
            total_processed += successful_count
            
            # è®¡ç®—è¯¥åˆ—çš„å¤„ç†æ—¶é—´
            column_end_time = time.time()
            column_duration = column_end_time - start_time
            
            processing_log.append("-" * 40)
            processing_log.append(f"âœ… åˆ— {column} å¤„ç†å®Œæˆç»Ÿè®¡:")
            processing_log.append(f"   ğŸ“Š æˆåŠŸå¤„ç†: {successful_count} æ¡ ({successful_count/total_items*100:.1f}%)")
            processing_log.append(f"   âŒ å¤„ç†å¤±è´¥: {failed_count} æ¡ ({failed_count/total_items*100:.1f}%)")
            processing_log.append(f"   â±ï¸ å¤„ç†è€—æ—¶: {column_duration:.1f} ç§’")
            processing_log.append(f"   ğŸš€ å¹³å‡é€Ÿåº¦: {total_items/column_duration:.1f} æ¡/ç§’")
            processing_log.append(f"   â° å®Œæˆæ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}")
            processing_log.append(f"   ğŸ“ˆ ç´¯è®¡å®Œæˆ: {total_processed}/{total_items_all_columns} æ¡")
            
            # è¾“å‡ºåˆ—å®ŒæˆçŠ¶æ€
            yield "\n".join(processing_log), "", overall_progress
        
        # ä¿å­˜ç»“æœï¼ˆåŒ…æ‹¬ä¸­æ–­æƒ…å†µä¸‹çš„éƒ¨åˆ†ç»“æœï¼‰
        processing_log.append("\n" + "="*50)
        if is_processing_interrupted():
            processing_log.append("â¹ï¸ å¤„ç†å·²ä¸­æ–­ï¼Œæ­£åœ¨ä¿å­˜éƒ¨åˆ†å¤„ç†ç»“æœ...")
        else:
            processing_log.append("ğŸ’¾ æ­£åœ¨ä¿å­˜å¤„ç†ç»“æœ...")
        processing_log.append(f"ğŸ“ è¾“å‡ºç›®å½•: {Path(__file__).parent / 'output'}")
        
        # è¾“å‡ºä¿å­˜å¼€å§‹çŠ¶æ€
        yield "\n".join(processing_log), "", overall_progress
        
        if original_file_path:
            try:
                if save_location == "è‡ªå®šä¹‰ç›®å½•" and custom_save_path.strip():
                    # ä½¿ç”¨è‡ªå®šä¹‰ä¿å­˜è·¯å¾„
                    custom_dir = Path(custom_save_path.strip())
                    custom_dir.mkdir(parents=True, exist_ok=True)
                    
                    original_name = Path(original_file_path).stem
                    original_ext = Path(original_file_path).suffix
                    timestamp = time.strftime("%Y%m%d_%H%M%S")
                    output_filename = f"{original_name}_processed_{timestamp}{original_ext}"
                    output_path = custom_dir / output_filename
                    
                    processing_log.append(f"ğŸ“‚ ä½¿ç”¨è‡ªå®šä¹‰ç›®å½•: {custom_dir}")
                    
                    if original_ext.lower() in ['.xlsx', '.xls']:
                        current_dataframe.to_excel(str(output_path), index=False)
                        processing_log.append(f"ğŸ“Š Excelæ–‡ä»¶ä¿å­˜ä¸­...")
                    else:
                        current_dataframe.to_csv(str(output_path), index=False, encoding='utf-8-sig')
                        processing_log.append(f"ğŸ“„ CSVæ–‡ä»¶ä¿å­˜ä¸­...")
                else:
                    # ä½¿ç”¨é»˜è®¤outputç›®å½•ï¼ˆå½“å‰ä»£ç æ–‡ä»¶æ‰€åœ¨ç›®å½•ä¸‹çš„outputæ–‡ä»¶å¤¹ï¼‰
                    processing_log.append(f"ğŸ“‚ ä½¿ç”¨é»˜è®¤è¾“å‡ºç›®å½•: {Path(__file__).parent / 'output'}")
                    output_path = save_to_output_dir(current_dataframe, original_file_path)
                    processing_log.append(f"ğŸ’¾ æ–‡ä»¶ä¿å­˜ä¸­...")
                
                processing_log.append(f"âœ… ç»“æœå·²æˆåŠŸä¿å­˜åˆ°: {output_path}")
                processing_log.append(f"ğŸ“Š ä¿å­˜çš„æ•°æ®è¡Œæ•°: {len(current_dataframe)}")
                processing_log.append(f"ğŸ“‹ ä¿å­˜çš„æ•°æ®åˆ—æ•°: {len(current_dataframe.columns)}")
                
                # ç”Ÿæˆç»“æœé¢„è§ˆ
                result_preview = generate_enhanced_result_preview(current_dataframe, columns_to_process, total_processed)
                
                # è®¡ç®—æ€»å¤„ç†æ—¶é—´
                total_end_time = time.time()
                total_duration = total_end_time - total_start_time
                
                # ç”Ÿæˆæœ€ç»ˆç»Ÿè®¡ä¿¡æ¯
                processing_log.append("\n" + "="*50)
                if is_processing_interrupted():
                    processing_log.append("â¹ï¸ === ä¸­æ–­å¤„ç†ç»Ÿè®¡æŠ¥å‘Š === â¹ï¸")
                else:
                    processing_log.append("ğŸ¯ === æœ€ç»ˆå¤„ç†ç»Ÿè®¡æŠ¥å‘Š === ğŸ¯")
                processing_log.append(f"ğŸ“‹ å¤„ç†ä»»åŠ¡: {task_name}")
                processing_log.append(f"ğŸ“Š å¤„ç†åˆ—æ•°: {len(columns_to_process)} åˆ—")
                processing_log.append(f"ğŸ“ˆ æ€»æ•°æ®é‡: {total_items_all_columns} æ¡")
                processing_log.append(f"âœ… æˆåŠŸå¤„ç†: {total_processed} æ¡")
                processing_log.append(f"âŒ å¤±è´¥æ•°é‡: {total_items_all_columns - total_processed} æ¡")
                processing_log.append(f"ğŸ“Š æ€»æˆåŠŸç‡: {(total_processed/total_items_all_columns*100):.1f}%")
                processing_log.append(f"ğŸ”§ å¹¶å‘çº¿ç¨‹: {max_workers} ä¸ª")
                processing_log.append(f"â±ï¸ æ€»å¤„ç†æ—¶é—´: {total_duration:.1f} ç§’")
                processing_log.append(f"ğŸš€ å¹³å‡å¤„ç†é€Ÿåº¦: {total_items_all_columns/total_duration:.1f} æ¡/ç§’")
                processing_log.append(f"ğŸ’¾ è¾“å‡ºä½ç½®: {output_path}")
                processing_log.append(f"â° å®Œæˆæ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}")
                processing_log.append("="*50)
                if is_processing_interrupted():
                    processing_log.append(f"â¹ï¸ å¤„ç†å·²ä¸­æ–­ï¼Œå·²ä¿å­˜éƒ¨åˆ†ç»“æœï¼")
                else:
                    processing_log.append(f"ğŸ‰ æ‰€æœ‰å¤„ç†ä»»åŠ¡å·²æˆåŠŸå®Œæˆï¼")
                
                final_message = "\n".join(processing_log)
                
                # æœ€ç»ˆè¾“å‡º
                yield final_message, result_preview, 100.0
                
            except Exception as e:
                logger.error(f"ä¿å­˜æ–‡ä»¶å¤±è´¥: {str(e)}")
                processing_log.append(f"âŒ ä¿å­˜å¤±è´¥: {str(e)}")
                
                result_preview = generate_enhanced_result_preview(current_dataframe, columns_to_process, total_processed)
                final_message = "\n".join(processing_log)
                final_message += f"\n\nâš ï¸ å¤„ç†å®Œæˆä½†ä¿å­˜å¤±è´¥ï¼Œå…±å¤„ç† {total_processed} æ¡æ•°æ®"
                
                yield final_message, result_preview, overall_progress
        else:
            processing_log.append(f"ğŸ‰ æ€»è®¡å¤„ç†å®Œæˆ {total_processed} æ¡æ•°æ®")
            result_preview = generate_enhanced_result_preview(current_dataframe, columns_to_process, total_processed)
            yield "\n".join(processing_log), result_preview, overall_progress
            
    except Exception as e:
        logger.error(f"æ•°æ®å¤„ç†å¤±è´¥: {str(e)}")
        yield f"âŒ å¤„ç†å¤±è´¥: {str(e)}", "", 0.0

def process_data(file_upload, selected_columns, task_name: str, 
                batch_size: int = 10, max_workers: int = 3, 
                save_location: str = "å½“å‰æ–‡ä»¶çš„outputç›®å½•", custom_save_path: str = "") -> Tuple[str, str, float]:
    """å¤„ç†æ•°æ®ï¼ˆæ”¯æŒå¤šçº¿ç¨‹å’Œå¤šåˆ—é€‰æ‹©ï¼Œå¸¦è¿›åº¦æ˜¾ç¤ºï¼‰"""
    global current_model_client, current_dataframe, original_file_path
    
    if current_model_client is None:
        return "âŒ è¯·å…ˆåŠ è½½AIæ¨¡å‹", "", 0.0
    
    if current_dataframe is None:
        return "âŒ è¯·å…ˆä¸Šä¼ æ–‡ä»¶", "", 0.0
    
    if not selected_columns:
        return "âŒ è¯·é€‰æ‹©è¦å¤„ç†çš„åˆ—", "", 0.0
    
    if not task_name:
        return "âŒ è¯·é€‰æ‹©å¤„ç†ä»»åŠ¡", "", 0.0
    
    try:
        # è·å–ä»»åŠ¡æç¤ºè¯
        prompt = task_manager.get_task_prompt(task_name)
        if not prompt:
            return "âŒ é€‰æ‹©çš„ä»»åŠ¡æ— æ•ˆ", "", 0.0
        
        # å¤„ç†å¤šåˆ—é€‰æ‹©
        if isinstance(selected_columns, str):
            columns_to_process = [selected_columns]
        else:
            columns_to_process = selected_columns if selected_columns else []
        
        if not columns_to_process:
            return "âŒ è¯·é€‰æ‹©è¦å¤„ç†çš„åˆ—", "", 0.0
        
        # éªŒè¯åˆ—æ˜¯å¦å­˜åœ¨
        missing_columns = [col for col in columns_to_process if col not in current_dataframe.columns]
        if missing_columns:
            return f"âŒ ä»¥ä¸‹åˆ—ä¸å­˜åœ¨: {', '.join(missing_columns)}", "", 0.0
        
        total_processed = 0
        processing_log = []
        overall_progress = 0.0
        total_start_time = time.time()  # è®°å½•æ€»å¼€å§‹æ—¶é—´
        
        # è®¡ç®—æ€»çš„å¤„ç†é¡¹ç›®æ•°
        total_items_all_columns = 0
        for column in columns_to_process:
            data_to_process = current_dataframe[column].astype(str).tolist()
            indexed_data = [(i, item) for i, item in enumerate(data_to_process) if item.strip()]
            total_items_all_columns += len(indexed_data)
        
        processing_log.append(f"ğŸ“Š å¼€å§‹å¤„ç† {len(columns_to_process)} åˆ—ï¼Œå…± {total_items_all_columns} æ¡æ•°æ®")
        processing_log.append(f"âš™ï¸ ä½¿ç”¨ {max_workers} ä¸ªçº¿ç¨‹å¹¶å‘å¤„ç†")
        processing_log.append(f"ğŸ“ é»˜è®¤è¾“å‡ºç›®å½•: {Path(__file__).parent / 'output'}")
        processing_log.append(f"ğŸ“‚ ä¿å­˜ä½ç½®è®¾ç½®: {save_location}")
        if save_location == "è‡ªå®šä¹‰ç›®å½•" and custom_save_path.strip():
            processing_log.append(f"ğŸ“ è‡ªå®šä¹‰è·¯å¾„: {custom_save_path.strip()}")
        processing_log.append(f"ğŸ“ å¤„ç†ä»»åŠ¡: {task_name}")
        processing_log.append("" + "="*50)
        
        global_processed_count = 0
        
        # å¤„ç†æ¯ä¸€åˆ—
        for col_index, column in enumerate(columns_to_process):
            processing_log.append(f"\nğŸ”„ æ­£åœ¨å¤„ç†åˆ—: {column} ({col_index + 1}/{len(columns_to_process)})")
            processing_log.append(f"â° å¼€å§‹æ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}")
            
            # è·å–è¦å¤„ç†çš„æ•°æ®
            data_to_process = current_dataframe[column].astype(str).tolist()
            
            # è¿‡æ»¤ç©ºå€¼å¹¶ä¿å­˜åŸå§‹ç´¢å¼•
            indexed_data = [(i, item) for i, item in enumerate(data_to_process) if item.strip()]
            
            if not indexed_data:
                processing_log.append(f"âš ï¸ åˆ— {column} ä¸­æ²¡æœ‰æœ‰æ•ˆæ•°æ®ï¼Œè·³è¿‡")
                continue
            
            total_items = len(indexed_data)
            processing_log.append(f"ğŸ“ è¯¥åˆ—æœ‰æ•ˆæ•°æ®: {total_items} æ¡")
            processing_log.append(f"ğŸ“Š æ•°æ®é•¿åº¦ç»Ÿè®¡: å¹³å‡ {sum(len(str(item[1])) for item in indexed_data) / len(indexed_data):.0f} å­—ç¬¦")
            processing_log.append(f"ğŸš€ å¼€å§‹å¹¶å‘å¤„ç†ï¼Œçº¿ç¨‹æ•°: {max_workers}")
            processing_log.append("-" * 40)
            
            # åˆ›å»ºç»“æœå­—å…¸
            results_dict = {}
            processed_count = 0
            start_time = time.time()
            
            def process_single_item(indexed_item):
                """å¤„ç†å•ä¸ªæ–‡æœ¬é¡¹"""
                index, item = indexed_item
                try:
                    # é™åˆ¶å•ä¸ªæ–‡æœ¬é•¿åº¦
                    if len(item) > 10000:
                        item = item[:10000] + "...[æ–‡æœ¬è¿‡é•¿ï¼Œå·²æˆªæ–­]"
                    
                    result = current_model_client.process_text(item, prompt)
                    return index, result, True
                except Exception as e:
                    error_msg = f"å¤„ç†å¤±è´¥: {str(e)}"
                    logger.error(f"å¤„ç†ç¬¬ {index+1} é¡¹å¤±è´¥: {str(e)}")
                    return index, error_msg, False
            
            # ä½¿ç”¨çº¿ç¨‹æ± è¿›è¡Œå¹¶å‘å¤„ç†
            with ThreadPoolExecutor(max_workers=max_workers) as executor:
                # æäº¤æ‰€æœ‰ä»»åŠ¡
                future_to_index = {executor.submit(process_single_item, item): item[0] for item in indexed_data}
                
                # æ”¶é›†ç»“æœ
                for future in as_completed(future_to_index):
                    try:
                        index, result, success = future.result()
                        results_dict[index] = result
                        processed_count += 1
                        global_processed_count += 1
                        
                        # è®¡ç®—è¿›åº¦
                        column_progress = processed_count / total_items * 100
                        overall_progress = global_processed_count / total_items_all_columns * 100
                        
                        # è®¡ç®—å¤„ç†é€Ÿåº¦
                        elapsed_time = time.time() - start_time
                        if elapsed_time > 0:
                            speed = processed_count / elapsed_time
                            remaining_items = total_items - processed_count
                            eta = remaining_items / speed if speed > 0 else 0
                            
                            # å®æ—¶è¯¦ç»†çš„è¿›åº¦æ˜¾ç¤º
                            if processed_count % 2 == 0 or processed_count == total_items:
                                status_msg = (
                                    f"ğŸ”„ å®æ—¶çŠ¶æ€ | åˆ—: {column} ({col_index + 1}/{len(columns_to_process)}) | "
                                    f"å½“å‰åˆ—è¿›åº¦: {processed_count}/{total_items} ({column_progress:.1f}%) | "
                                    f"æ€»ä½“è¿›åº¦: {global_processed_count}/{total_items_all_columns} ({overall_progress:.1f}%) | "
                                    f"å¤„ç†é€Ÿåº¦: {speed:.1f}æ¡/ç§’ | é¢„è®¡å‰©ä½™æ—¶é—´: {eta:.0f}ç§’"
                                )
                                processing_log.append(status_msg)
                                
                                # æ˜¾ç¤ºæœ€è¿‘å¤„ç†çš„å†…å®¹é¢„è§ˆï¼ˆæˆåŠŸçš„æƒ…å†µï¼‰
                                if success and len(result) > 0:
                                    preview_text = result[:50] + "..." if len(result) > 50 else result
                                    processing_log.append(f"   âœ… æœ€æ–°å¤„ç†ç»“æœé¢„è§ˆ: {preview_text}")
                                elif not success:
                                    processing_log.append(f"   âŒ å¤„ç†å¤±è´¥: {result}")
                        
                    except Exception as e:
                        logger.error(f"è·å–å¤„ç†ç»“æœå¤±è´¥: {str(e)}")
                        global_processed_count += 1
            
            # æ„å»ºå®Œæ•´çš„ç»“æœåˆ—è¡¨
            full_results = []
            for i, original_item in enumerate(data_to_process):
                if i in results_dict:
                    full_results.append(results_dict[i])
                else:
                    full_results.append("" if not original_item.strip() else "å¤„ç†å¤±è´¥")
            
            # æ·»åŠ ç»“æœåˆ—åˆ°DataFrame
            result_column_name = f"{column}_processed"
            current_dataframe[result_column_name] = full_results
            
            successful_count = len([r for r in full_results if r and not r.startswith('å¤„ç†å¤±è´¥')])
            failed_count = total_items - successful_count
            total_processed += successful_count
            
            # è®¡ç®—è¯¥åˆ—çš„å¤„ç†æ—¶é—´
            column_end_time = time.time()
            column_duration = column_end_time - start_time
            
            processing_log.append("-" * 40)
            processing_log.append(f"âœ… åˆ— {column} å¤„ç†å®Œæˆç»Ÿè®¡:")
            processing_log.append(f"   ğŸ“Š æˆåŠŸå¤„ç†: {successful_count} æ¡ ({successful_count/total_items*100:.1f}%)")
            processing_log.append(f"   âŒ å¤„ç†å¤±è´¥: {failed_count} æ¡ ({failed_count/total_items*100:.1f}%)")
            processing_log.append(f"   â±ï¸ å¤„ç†è€—æ—¶: {column_duration:.1f} ç§’")
            processing_log.append(f"   ğŸš€ å¹³å‡é€Ÿåº¦: {total_items/column_duration:.1f} æ¡/ç§’")
            processing_log.append(f"   â° å®Œæˆæ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}")
            processing_log.append(f"   ğŸ“ˆ ç´¯è®¡å®Œæˆ: {total_processed}/{total_items_all_columns} æ¡")
        
        # ä¿å­˜ç»“æœ
        processing_log.append("\n" + "="*50)
        processing_log.append("ğŸ’¾ æ­£åœ¨ä¿å­˜å¤„ç†ç»“æœ...")
        processing_log.append(f"ğŸ“ è¾“å‡ºç›®å½•: {Path(__file__).parent / 'output'}")
        
        if original_file_path:
            try:
                if save_location == "è‡ªå®šä¹‰ç›®å½•" and custom_save_path.strip():
                    # ä½¿ç”¨è‡ªå®šä¹‰ä¿å­˜è·¯å¾„
                    custom_dir = Path(custom_save_path.strip())
                    custom_dir.mkdir(parents=True, exist_ok=True)
                    
                    original_name = Path(original_file_path).stem
                    original_ext = Path(original_file_path).suffix
                    timestamp = time.strftime("%Y%m%d_%H%M%S")
                    output_filename = f"{original_name}_processed_{timestamp}{original_ext}"
                    output_path = custom_dir / output_filename
                    
                    processing_log.append(f"ğŸ“‚ ä½¿ç”¨è‡ªå®šä¹‰ç›®å½•: {custom_dir}")
                    
                    if original_ext.lower() in ['.xlsx', '.xls']:
                        current_dataframe.to_excel(str(output_path), index=False)
                        processing_log.append(f"ğŸ“Š Excelæ–‡ä»¶ä¿å­˜ä¸­...")
                    else:
                        current_dataframe.to_csv(str(output_path), index=False, encoding='utf-8-sig')
                        processing_log.append(f"ğŸ“„ CSVæ–‡ä»¶ä¿å­˜ä¸­...")
                else:
                    # ä½¿ç”¨é»˜è®¤outputç›®å½•ï¼ˆå½“å‰ä»£ç æ–‡ä»¶æ‰€åœ¨ç›®å½•ä¸‹çš„outputæ–‡ä»¶å¤¹ï¼‰
                    processing_log.append(f"ğŸ“‚ ä½¿ç”¨é»˜è®¤è¾“å‡ºç›®å½•: {Path(__file__).parent / 'output'}")
                    output_path = save_to_output_dir(current_dataframe, original_file_path)
                    processing_log.append(f"ğŸ’¾ æ–‡ä»¶ä¿å­˜ä¸­...")
                
                processing_log.append(f"âœ… ç»“æœå·²æˆåŠŸä¿å­˜åˆ°: {output_path}")
                processing_log.append(f"ğŸ“Š ä¿å­˜çš„æ•°æ®è¡Œæ•°: {len(current_dataframe)}")
                processing_log.append(f"ğŸ“‹ ä¿å­˜çš„æ•°æ®åˆ—æ•°: {len(current_dataframe.columns)}")
                
                # ç”Ÿæˆç»“æœé¢„è§ˆ
                result_preview = generate_enhanced_result_preview(current_dataframe, columns_to_process, total_processed)
                
                # è®¡ç®—æ€»å¤„ç†æ—¶é—´
                total_end_time = time.time()
                total_duration = total_end_time - total_start_time
                
                # ç”Ÿæˆæœ€ç»ˆç»Ÿè®¡ä¿¡æ¯
                processing_log.append("\n" + "="*50)
                processing_log.append("ğŸ¯ === æœ€ç»ˆå¤„ç†ç»Ÿè®¡æŠ¥å‘Š === ğŸ¯")
                processing_log.append(f"ğŸ“‹ å¤„ç†ä»»åŠ¡: {task_name}")
                processing_log.append(f"ğŸ“Š å¤„ç†åˆ—æ•°: {len(columns_to_process)} åˆ—")
                processing_log.append(f"ğŸ“ˆ æ€»æ•°æ®é‡: {total_items_all_columns} æ¡")
                processing_log.append(f"âœ… æˆåŠŸå¤„ç†: {total_processed} æ¡")
                processing_log.append(f"âŒ å¤±è´¥æ•°é‡: {total_items_all_columns - total_processed} æ¡")
                processing_log.append(f"ğŸ“Š æ€»æˆåŠŸç‡: {(total_processed/total_items_all_columns*100):.1f}%")
                processing_log.append(f"ğŸ”§ å¹¶å‘çº¿ç¨‹: {max_workers} ä¸ª")
                processing_log.append(f"â±ï¸ æ€»å¤„ç†æ—¶é—´: {total_duration:.1f} ç§’")
                processing_log.append(f"ğŸš€ å¹³å‡å¤„ç†é€Ÿåº¦: {total_items_all_columns/total_duration:.1f} æ¡/ç§’")
                processing_log.append(f"ğŸ’¾ è¾“å‡ºä½ç½®: {output_path}")
                processing_log.append(f"â° å®Œæˆæ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}")
                processing_log.append("="*50)
                processing_log.append(f"ğŸ‰ æ‰€æœ‰å¤„ç†ä»»åŠ¡å·²æˆåŠŸå®Œæˆï¼")
                
                final_message = "\n".join(processing_log)
                
                return final_message, result_preview, 100.0
                
            except Exception as e:
                logger.error(f"ä¿å­˜æ–‡ä»¶å¤±è´¥: {str(e)}")
                processing_log.append(f"âŒ ä¿å­˜å¤±è´¥: {str(e)}")
                
                result_preview = generate_enhanced_result_preview(current_dataframe, columns_to_process, total_processed)
                final_message = "\n".join(processing_log)
                final_message += f"\n\nâš ï¸ å¤„ç†å®Œæˆä½†ä¿å­˜å¤±è´¥ï¼Œå…±å¤„ç† {total_processed} æ¡æ•°æ®"
                
                return final_message, result_preview, overall_progress
        else:
            processing_log.append(f"ğŸ‰ æ€»è®¡å¤„ç†å®Œæˆ {total_processed} æ¡æ•°æ®")
            result_preview = generate_enhanced_result_preview(current_dataframe, columns_to_process, total_processed)
            return "\n".join(processing_log), result_preview, overall_progress
            
    except Exception as e:
        logger.error(f"æ•°æ®å¤„ç†å¤±è´¥: {str(e)}")
        return f"âŒ å¤„ç†å¤±è´¥: {str(e)}", "", 0.0

def generate_result_preview(df: pd.DataFrame, processed_columns: List[str]) -> str:
    """ç”Ÿæˆå¤„ç†ç»“æœé¢„è§ˆ"""
    try:
        preview_lines = []
        preview_lines.append("=== å¤„ç†ç»“æœé¢„è§ˆ ===")
        preview_lines.append(f"æ•°æ®æ€»è¡Œæ•°: {len(df)}")
        preview_lines.append(f"æ•°æ®æ€»åˆ—æ•°: {len(df.columns)}")
        preview_lines.append("")
        
        # æ˜¾ç¤ºå¤„ç†çš„åˆ—ä¿¡æ¯
        for col in processed_columns:
            result_col = f"{col}_processed"
            if result_col in df.columns:
                non_empty_count = df[result_col].astype(str).str.strip().ne('').sum()
                preview_lines.append(f"åˆ— '{col}' -> '{result_col}': {non_empty_count} æ¡æœ‰æ•ˆç»“æœ")
        
        preview_lines.append("")
        preview_lines.append("=== å‰5è¡Œæ•°æ®é¢„è§ˆ ===")
        
        # æ˜¾ç¤ºå‰5è¡Œçš„åŸå§‹æ•°æ®å’Œå¤„ç†ç»“æœ
        for i in range(min(5, len(df))):
            preview_lines.append(f"--- ç¬¬ {i+1} è¡Œ ---")
            for col in processed_columns:
                original_value = str(df.iloc[i][col])[:100]
                result_col = f"{col}_processed"
                if result_col in df.columns:
                    processed_value = str(df.iloc[i][result_col])[:100]
                    preview_lines.append(f"{col}: {original_value}")
                    preview_lines.append(f"{result_col}: {processed_value}")
                    preview_lines.append("")
        
        return "\n".join(preview_lines)
        
    except Exception as e:
        return f"ç”Ÿæˆé¢„è§ˆå¤±è´¥: {str(e)}"

def generate_enhanced_result_preview(df: pd.DataFrame, processed_columns: List[str], total_processed: int) -> str:
    """ç”Ÿæˆå¢å¼ºçš„å¤„ç†ç»“æœé¢„è§ˆ"""
    try:
        preview_lines = []
        preview_lines.append("ğŸ“Š === å¤„ç†ç»“æœè¯¦ç»†é¢„è§ˆ === ğŸ“Š")
        preview_lines.append(f"ğŸ“‹ æ•°æ®æ€»è¡Œæ•°: {len(df)}")
        preview_lines.append(f"ğŸ“‹ æ•°æ®æ€»åˆ—æ•°: {len(df.columns)}")
        preview_lines.append(f"âœ… æˆåŠŸå¤„ç†: {total_processed} æ¡")
        preview_lines.append("")
        
        # æ˜¾ç¤ºæ¯åˆ—çš„è¯¦ç»†ç»Ÿè®¡
        preview_lines.append("ğŸ“ˆ å„åˆ—å¤„ç†ç»Ÿè®¡:")
        for col in processed_columns:
            result_col = f"{col}_processed"
            if result_col in df.columns:
                # ç»Ÿè®¡æœ‰æ•ˆç»“æœ
                valid_results = df[result_col].astype(str).str.strip()
                non_empty_count = valid_results.ne('').sum()
                failed_count = valid_results.str.startswith('å¤„ç†å¤±è´¥').sum()
                success_count = non_empty_count - failed_count
                
                # è®¡ç®—å¹³å‡é•¿åº¦
                valid_lengths = valid_results[valid_results.ne('')].str.len()
                avg_length = valid_lengths.mean() if len(valid_lengths) > 0 else 0
                
                preview_lines.append(f"   ğŸ”¹ åˆ— '{col}':")
                preview_lines.append(f"      âœ… æˆåŠŸ: {success_count} æ¡")
                preview_lines.append(f"      âŒ å¤±è´¥: {failed_count} æ¡")
                preview_lines.append(f"      ğŸ“ å¹³å‡ç»“æœé•¿åº¦: {avg_length:.0f} å­—ç¬¦")
                preview_lines.append("")
        
        preview_lines.append("ğŸ” === æ•°æ®æ ·æœ¬é¢„è§ˆ === ğŸ”")
        
        # æ˜¾ç¤ºå‰3è¡Œçš„åŸå§‹æ•°æ®å’Œå¤„ç†ç»“æœ
        for i in range(min(3, len(df))):
            preview_lines.append(f"ğŸ“„ ç¬¬ {i+1} è¡Œæ ·æœ¬:")
            for col in processed_columns:
                original_value = str(df.iloc[i][col])
                result_col = f"{col}_processed"
                if result_col in df.columns:
                    processed_value = str(df.iloc[i][result_col])
                    
                    # é™åˆ¶æ˜¾ç¤ºé•¿åº¦
                    original_display = original_value[:150] + "..." if len(original_value) > 150 else original_value
                    processed_display = processed_value[:150] + "..." if len(processed_value) > 150 else processed_value
                    
                    preview_lines.append(f"   ğŸ“ åŸå§‹[{col}]: {original_display}")
                    preview_lines.append(f"   ğŸ¤– å¤„ç†[{result_col}]: {processed_display}")
                    preview_lines.append("   " + "-"*50)
            preview_lines.append("")
        
        # æ·»åŠ æ•°æ®è´¨é‡åˆ†æ
        preview_lines.append("ğŸ“Š === æ•°æ®è´¨é‡åˆ†æ === ğŸ“Š")
        for col in processed_columns:
            result_col = f"{col}_processed"
            if result_col in df.columns:
                results = df[result_col].astype(str)
                
                # åˆ†æç»“æœç±»å‹
                empty_count = results.str.strip().eq('').sum()
                error_count = results.str.contains('å¤„ç†å¤±è´¥|é”™è¯¯|å¤±è´¥', na=False).sum()
                valid_count = len(results) - empty_count - error_count
                
                preview_lines.append(f"   ğŸ“Š åˆ— '{col}' è´¨é‡åˆ†æ:")
                preview_lines.append(f"      ğŸŸ¢ æœ‰æ•ˆç»“æœ: {valid_count} æ¡ ({valid_count/len(results)*100:.1f}%)")
                preview_lines.append(f"      ğŸ”´ å¤„ç†é”™è¯¯: {error_count} æ¡ ({error_count/len(results)*100:.1f}%)")
                preview_lines.append(f"      âšª ç©ºç™½ç»“æœ: {empty_count} æ¡ ({empty_count/len(results)*100:.1f}%)")
                preview_lines.append("")
        
        return "\n".join(preview_lines)
        
    except Exception as e:
        return f"ç”Ÿæˆå¢å¼ºé¢„è§ˆå¤±è´¥: {str(e)}"

def save_to_output_dir(df: pd.DataFrame, original_path: str, suffix: str = "_processed") -> str:
    """ä¿å­˜æ–‡ä»¶åˆ°å½“å‰ä»£ç æ–‡ä»¶æ‰€åœ¨çš„outputç›®å½•"""
    try:
        # è·å–å½“å‰ä»£ç æ–‡ä»¶æ‰€åœ¨ç›®å½•
        current_script_dir = Path(__file__).parent
        
        # åˆ›å»ºoutputç›®å½•ï¼ˆåœ¨å½“å‰ä»£ç æ–‡ä»¶ç›®å½•ä¸‹ï¼‰
        output_dir = current_script_dir / "output"
        output_dir.mkdir(parents=True, exist_ok=True)
        
        original_path = Path(original_path)
        
        # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶å
        base_name = original_path.stem
        extension = original_path.suffix.lower()
        timestamp = time.strftime("%Y%m%d_%H%M%S")
        
        output_file = output_dir / f"{base_name}{suffix}_{timestamp}{extension}"
        
        # æ ¹æ®æ–‡ä»¶ç±»å‹ä¿å­˜
        if extension == '.csv':
            df.to_csv(output_file, index=False, encoding='utf-8-sig')
        elif extension in ['.xlsx', '.xls']:
            df.to_excel(output_file, index=False)
        else:  # txt, mdç­‰æ–‡æœ¬æ–‡ä»¶
            # å¦‚æœåªæœ‰ä¸€åˆ—ï¼Œç›´æ¥ä¿å­˜å†…å®¹
            if len(df.columns) == 1:
                content = '\n'.join(df.iloc[:, 0].astype(str))
            else:
                # å¤šåˆ—æ—¶ä¿å­˜ä¸ºCSVæ ¼å¼
                content = df.to_csv(index=False, sep='\t')
            
            with open(output_file, 'w', encoding='utf-8') as f:
                f.write(content)
        
        return str(output_file)
        
    except Exception as e:
        raise Exception(f"ä¿å­˜æ–‡ä»¶å¤±è´¥: {str(e)}")

def get_processed_data() -> str:
    """è·å–å¤„ç†åçš„æ•°æ®é¢„è§ˆ"""
    global current_dataframe
    
    if current_dataframe is None:
        return "æš‚æ— æ•°æ®"
    
    try:
        # æ˜¾ç¤ºå‰10è¡Œæ•°æ®
        preview = current_dataframe.head(10).to_string(index=False, max_cols=10)
        return preview
    except Exception as e:
        return f"è·å–æ•°æ®é¢„è§ˆå¤±è´¥: {str(e)}"

# ==================== æ–‡æœ¬ç›´æ¥å¤„ç† ====================

def process_single_text(input_text: str, task_name: str) -> Tuple[str, str]:
    """
    å¤„ç†å•ä¸ªæ–‡æœ¬è¾“å…¥
    
    Args:
        input_text: è¾“å…¥çš„æ–‡æœ¬å†…å®¹
        task_name: å¤„ç†ä»»åŠ¡åç§°
    
    Returns:
        Tuple[str, str]: (å¤„ç†çŠ¶æ€, å¤„ç†ç»“æœ)
    """
    global current_model_client
    
    if not input_text or not input_text.strip():
        return "âš ï¸ è¾“å…¥æ–‡æœ¬ä¸èƒ½ä¸ºç©º", ""
    
    if current_model_client is None:
        return "âŒ è¯·å…ˆåŠ è½½AIæ¨¡å‹", ""
    
    if not task_name:
        return "âŒ è¯·é€‰æ‹©å¤„ç†ä»»åŠ¡", ""
    
    try:
        # è·å–ä»»åŠ¡æç¤ºè¯
        prompt = task_manager.get_task_prompt(task_name)
        if not prompt:
            return f"âŒ ä»»åŠ¡ '{task_name}' ä¸å­˜åœ¨", ""
        
        logger.info(f"å¼€å§‹å¤„ç†æ–‡æœ¬ï¼Œä»»åŠ¡: {task_name}")
        
        # è°ƒç”¨AIæ¨¡å‹å¤„ç†
        result = current_model_client.process_text(input_text, prompt)
        
        if result is not None and str(result).strip() != "":
            logger.info(f"æ–‡æœ¬å¤„ç†å®Œæˆï¼Œç»“æœé•¿åº¦: {len(str(result))}")
            return f"âœ… å¤„ç†å®Œæˆ\nä»»åŠ¡: {task_name}\nè¾“å…¥é•¿åº¦: {len(input_text)} å­—ç¬¦\nè¾“å‡ºé•¿åº¦: {len(str(result))} å­—ç¬¦", str(result)
        else:
            return "âŒ å¤„ç†å¤±è´¥ï¼šAIæ¨¡å‹è¿”å›ç©ºç»“æœ", ""
    
    except Exception as e:
        error_msg = f"å¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}"
        logger.error(error_msg)
        return f"âŒ {error_msg}", ""

# ==================== Gradioç•Œé¢ ====================

def create_interface():
    """åˆ›å»ºGradioç•Œé¢"""
    
    with gr.Blocks(title="AIæ–‡æœ¬å¤„ç†å·¥å…·", theme=gr.themes.Ocean()) as interface:
        gr.Markdown("# ğŸ¤– AIæ–‡æœ¬å¤„ç†å·¥å…·")
        gr.Markdown("æ”¯æŒå¤šç§æ–‡ä»¶æ ¼å¼çš„AIæ™ºèƒ½æ–‡æœ¬å¤„ç†")
        
        with gr.Tabs() as tabs:
            # ç¬¬ä¸€ä¸ªæ ‡ç­¾é¡µï¼šæ¨¡å‹é…ç½®
            with gr.TabItem("ğŸ”§ æ¨¡å‹é…ç½®"):
                with gr.Row():
                    with gr.Column(scale=1):
                        gr.Markdown("### AIæ¨¡å‹é…ç½®")
                        preset_dropdown = gr.Dropdown(
                            choices=list(AIModelClient.get_all_configs().keys()) + ["è‡ªå®šä¹‰"],
                            value="deepseek",
                            label="é€‰æ‹©æ¨¡å‹é…ç½®"
                        )
                        
                        # è‡ªå®šä¹‰é…ç½®ï¼ˆé»˜è®¤éšè—ï¼‰
                        with gr.Group(visible=False) as custom_config:
                            custom_name = gr.Textbox(label="æ¨¡å‹åç§°", placeholder="ä¾‹å¦‚ï¼šæˆ‘çš„æ¨¡å‹")
                            custom_base_url = gr.Textbox(label="APIåœ°å€", placeholder="ä¾‹å¦‚ï¼šhttps://api.example.com/v1")
                            custom_api_key = gr.Textbox(label="APIå¯†é’¥", type="password")
                            custom_model_name = gr.Textbox(label="æ¨¡å‹åç§°", placeholder="ä¾‹å¦‚ï¼šgpt-3.5-turbo")
                            save_custom_config = gr.Checkbox(label="ä¿å­˜æ­¤é…ç½®", value=False)
                        
                        load_model_btn = gr.Button("ğŸš€ åŠ è½½æ¨¡å‹", variant="primary")
                        model_status = gr.Textbox(label="æ¨¡å‹çŠ¶æ€", interactive=False)
                    
                    with gr.Column(scale=1):
                        gr.Markdown("### æ¨¡å‹é…ç½®è¯´æ˜")
                        gr.Markdown("""
                        **é¢„è®¾é…ç½®ï¼š**
                        - DeepSeek: é«˜æ€§èƒ½ä¸­æ–‡å¤§æ¨¡å‹
                        - OpenAI: GPTç³»åˆ—æ¨¡å‹
                        - æœ¬åœ°æ¨¡å‹: æœ¬åœ°éƒ¨ç½²çš„æ¨¡å‹
                        
                        **è‡ªå®šä¹‰é…ç½®ï¼š**
                        - æ”¯æŒä»»ä½•å…¼å®¹OpenAI APIçš„æ¨¡å‹
                        - å¯ä¿å­˜é…ç½®ä¾›ä¸‹æ¬¡ä½¿ç”¨
                        - è‡ªåŠ¨æµ‹è¯•è¿æ¥çŠ¶æ€
                        """)
            
            # ç¬¬äºŒä¸ªæ ‡ç­¾é¡µï¼šä»»åŠ¡ç®¡ç†
            with gr.TabItem("ğŸ“ ä»»åŠ¡ç®¡ç†"):
                with gr.Row():
                    with gr.Column(scale=1):
                        gr.Markdown("### æç¤ºè¯ä»»åŠ¡ç®¡ç†")
                        task_dropdown = gr.Dropdown(
                            choices=task_manager.get_task_names(),
                            value=task_manager.get_task_names()[0] if task_manager.get_task_names() else None,
                            label="é€‰æ‹©ä»»åŠ¡"
                        )
                        
                        task_prompt_display = gr.Textbox(
                            label="å½“å‰ä»»åŠ¡æç¤ºè¯",
                            lines=5,
                            interactive=True
                        )
                        
                        # ä»»åŠ¡ç®¡ç†æŒ‰é’®
                        with gr.Row():
                            edit_task_btn = gr.Button("âœï¸ ä¿å­˜ä¿®æ”¹", variant="primary")
                            delete_task_btn = gr.Button("ğŸ—‘ï¸ åˆ é™¤/é‡ç½®", variant="secondary")
                        with gr.Row():
                            reload_tasks_btn = gr.Button("ğŸ”„ é‡æ–°åŠ è½½", variant="secondary")
                        
                        task_status = gr.Textbox(label="ä»»åŠ¡çŠ¶æ€", interactive=False)
                    
                    with gr.Column(scale=1):
                        # è‡ªå®šä¹‰ä»»åŠ¡
                        gr.Markdown("### æ·»åŠ è‡ªå®šä¹‰ä»»åŠ¡")
                        new_task_name = gr.Textbox(label="ä»»åŠ¡åç§°", placeholder="è¾“å…¥æ–°ä»»åŠ¡åç§°")
                        new_task_prompt = gr.Textbox(label="ä»»åŠ¡æç¤ºè¯", lines=8, placeholder="è¾“å…¥ä»»åŠ¡æç¤ºè¯")
                        add_task_btn = gr.Button("â• æ·»åŠ ä»»åŠ¡", variant="primary")
                        
                        gr.Markdown("### ä»»åŠ¡è¯´æ˜")
                        gr.Markdown("""
                        **é»˜è®¤ä»»åŠ¡ï¼š**
                        - æ–‡æœ¬æ‘˜è¦ã€å…³é”®è¯æå–ã€æƒ…æ„Ÿåˆ†æç­‰
                        - å¯ä»¥ä¿®æ”¹æç¤ºè¯å†…å®¹
                        - åˆ é™¤æ—¶ä¼šé‡ç½®ä¸ºåŸå§‹å€¼
                        
                        **è‡ªå®šä¹‰ä»»åŠ¡ï¼š**
                        - å¯æ·»åŠ ã€åˆ é™¤ã€ä¿®æ”¹
                        - æ”¯æŒå¤æ‚çš„æç¤ºè¯æ¨¡æ¿
                        - åˆ é™¤æ—¶ä¼šå®Œå…¨ç§»é™¤
                        
                        **æ“ä½œè¯´æ˜ï¼š**
                        - é€‰æ‹©ä»»åŠ¡åå¯ç›´æ¥ç¼–è¾‘æç¤ºè¯
                        - ç‚¹å‡»"ä¿å­˜ä¿®æ”¹"åº”ç”¨æ›´æ”¹
                        - "åˆ é™¤/é‡ç½®"å¯¹é»˜è®¤ä»»åŠ¡æ˜¯é‡ç½®ï¼Œå¯¹è‡ªå®šä¹‰ä»»åŠ¡æ˜¯åˆ é™¤
                        """)
            
            # ç¬¬ä¸‰ä¸ªæ ‡ç­¾é¡µï¼šæ–‡æœ¬è¾“å…¥å¤„ç†ï¼ˆæ–°å¢ï¼‰
            with gr.TabItem("âœï¸ æ–‡æœ¬å¤„ç†"):
                with gr.Row():
                    with gr.Column(scale=1):
                        gr.Markdown("### è¾“å…¥æ–‡æœ¬")
                        input_text = gr.Textbox(label="å¾…å¤„ç†æ–‡æœ¬", lines=10, placeholder="åœ¨æ­¤è¾“å…¥è¦å¤„ç†çš„æ–‡æœ¬...")
                        with gr.Row():
                            copy_input_btn = gr.Button("ğŸ“‹ ä¸€é”®å¤åˆ¶è¾“å…¥")
                            paste_to_input_btn = gr.Button("ğŸ“¥ ç²˜è´´åˆ°è¾“å…¥")
                        
                        gr.Markdown("### å¤„ç†ä»»åŠ¡")
                        single_selected_task = gr.Dropdown(
                            choices=task_manager.get_task_names(),
                            value=task_manager.get_task_names()[0] if task_manager.get_task_names() else None,
                            label="é€‰æ‹©å¤„ç†ä»»åŠ¡"
                        )
                        
                        run_single_btn = gr.Button("ğŸš€ å¤„ç†æ–‡æœ¬", variant="primary")
                        single_status = gr.Textbox(label="å¤„ç†çŠ¶æ€", interactive=False)
                    
                    with gr.Column(scale=1):
                        gr.Markdown("### è¾“å‡ºç»“æœ")
                        output_text = gr.Textbox(label="å¤„ç†ç»“æœ", lines=12)
                        with gr.Row():
                            paste_output_btn = gr.Button("ğŸ“¥ ä¸€é”®ç²˜è´´åˆ°è¾“å‡º")
                            copy_output_btn = gr.Button("ğŸ“‹ ä¸€é”®å¤åˆ¶è¾“å‡º")
                        
                # å¤åˆ¶/ç²˜è´´å‰ç«¯äº¤äº’ï¼ˆä½¿ç”¨JSï¼‰
                copy_input_js = """
                function() {
                    const textarea = document.querySelector('textarea[data-testid="textbox"]');
                    if (textarea && textarea.value) {
                        navigator.clipboard.writeText(textarea.value).then(() => {
                            console.log('æ–‡æœ¬å·²å¤åˆ¶åˆ°å‰ªè´´æ¿');
                        }).catch(err => {
                            console.error('å¤åˆ¶å¤±è´¥:', err);
                        });
                    }
                }
                """
                
                paste_to_input_js = """
                async function() {
                    try {
                        const text = await navigator.clipboard.readText();
                        const textarea = document.querySelector('textarea[data-testid="textbox"]');
                        if (textarea) {
                            textarea.value = text;
                            textarea.dispatchEvent(new Event('input', { bubbles: true }));
                        }
                        return text;
                    } catch (err) {
                        console.error('ç²˜è´´å¤±è´¥:', err);
                        return '';
                    }
                }
                """
                
                copy_output_js = """
                function() {
                    const textareas = document.querySelectorAll('textarea[data-testid="textbox"]');
                    const outputTextarea = textareas[textareas.length - 1]; // è·å–æœ€åä¸€ä¸ªtextareaï¼ˆè¾“å‡ºæ¡†ï¼‰
                    if (outputTextarea && outputTextarea.value) {
                        navigator.clipboard.writeText(outputTextarea.value).then(() => {
                            console.log('è¾“å‡ºç»“æœå·²å¤åˆ¶åˆ°å‰ªè´´æ¿');
                        }).catch(err => {
                            console.error('å¤åˆ¶å¤±è´¥:', err);
                        });
                    }
                }
                """
                
                paste_output_js = """
                async function() {
                    try {
                        const text = await navigator.clipboard.readText();
                        const textareas = document.querySelectorAll('textarea[data-testid="textbox"]');
                        const outputTextarea = textareas[textareas.length - 1]; // è·å–æœ€åä¸€ä¸ªtextareaï¼ˆè¾“å‡ºæ¡†ï¼‰
                        if (outputTextarea) {
                            outputTextarea.value = text;
                            outputTextarea.dispatchEvent(new Event('input', { bubbles: true }));
                        }
                        return text;
                    } catch (err) {
                        console.error('ç²˜è´´å¤±è´¥:', err);
                        return '';
                    }
                }
                """
                
                copy_input_btn.click(None, [], [], js=copy_input_js)
                paste_to_input_btn.click(None, [], [input_text], js=paste_to_input_js)
                copy_output_btn.click(None, [], [], js=copy_output_js)
                paste_output_btn.click(None, [], [output_text], js=paste_output_js)
                
                # äº‹ä»¶ï¼šå¤„ç†æ–‡æœ¬
                run_single_btn.click(
                    fn=process_single_text,
                    inputs=[input_text, single_selected_task],
                    outputs=[single_status, output_text]
                )
            
            # ç¬¬å››ä¸ªæ ‡ç­¾é¡µï¼šæ–‡ä»¶å¤„ç†ä¸ç»“æœ
            with gr.TabItem("ğŸ“ æ–‡ä»¶å¤„ç†"):
                with gr.Row():
                    with gr.Column(scale=1):
                        # æ–‡ä»¶ä¸Šä¼ 
                        gr.Markdown("### æ–‡ä»¶ä¸Šä¼ ")
                        
                        # ä¸Šä¼ æ¨¡å¼é€‰æ‹©
                        upload_mode = gr.Radio(
                            choices=["å•ä¸ªæ–‡ä»¶", "ç›®å½•ä¸Šä¼ "],
                            value="å•ä¸ªæ–‡ä»¶",
                            label="ä¸Šä¼ æ¨¡å¼"
                        )
                        
                        # å•ä¸ªæ–‡ä»¶ä¸Šä¼ 
                        file_upload = gr.File(
                            label="ä¸Šä¼ æ–‡ä»¶",
                            file_types=[".txt", ".md", ".csv", ".xlsx", ".xls", ".pdf", ".docx", ".doc"],
                            visible=True
                        )
                        
                        # ç›®å½•ä¸Šä¼ 
                        directory_upload = gr.File(
                            label="é€‰æ‹©ç›®å½•ä¸­çš„æ–‡ä»¶",
                            file_count="multiple",
                            file_types=[".txt", ".md", ".csv", ".xlsx", ".xls", ".pdf", ".docx", ".doc"],
                            visible=False
                        )
                        
                        # æ–‡ä»¶åŒ¹é…è¿‡æ»¤
                        with gr.Group(visible=False) as file_filter_group:
                            gr.Markdown("#### æ–‡ä»¶è¿‡æ»¤è®¾ç½®")
                            file_pattern = gr.Textbox(
                                label="æ–‡ä»¶ååŒ¹é…æ¨¡å¼",
                                placeholder="ä¾‹å¦‚: *.txt æˆ– data_*.csv æˆ–ç•™ç©ºå¤„ç†æ‰€æœ‰æ–‡ä»¶",
                                value=""
                            )
                            file_extension_filter = gr.CheckboxGroup(
                                choices=[".txt", ".md", ".csv", ".xlsx", ".xls", ".pdf", ".docx", ".doc"],
                                value=[".txt", ".md", ".csv", ".xlsx", ".xls", ".pdf", ".docx", ".doc"],
                                label="å…è®¸çš„æ–‡ä»¶ç±»å‹"
                            )
                            apply_filter_btn = gr.Button("ğŸ” åº”ç”¨è¿‡æ»¤å™¨", variant="secondary")
                        
                        file_info = gr.Textbox(label="æ–‡ä»¶ä¿¡æ¯", interactive=False)
                        
                        # åŒ¹é…çš„æ–‡ä»¶åˆ—è¡¨ï¼ˆä»…ç›®å½•æ¨¡å¼æ˜¾ç¤ºï¼‰
                        matched_files_display = gr.Textbox(
                            label="åŒ¹é…çš„æ–‡ä»¶åˆ—è¡¨",
                            lines=3,
                            interactive=False,
                            visible=False
                        )
                        
                        # åˆ—é€‰æ‹©ï¼ˆä»…å¯¹è¡¨æ ¼æ–‡ä»¶æ˜¾ç¤ºï¼‰
                        column_dropdown = gr.Dropdown(
                            label="é€‰æ‹©è¦å¤„ç†çš„åˆ—",
                            visible=False,
                            multiselect=True
                        )
                        
                        # ä»»åŠ¡é€‰æ‹©
                        gr.Markdown("### å¤„ç†ä»»åŠ¡")
                        selected_task = gr.Dropdown(
                            choices=task_manager.get_task_names(),
                            value=task_manager.get_task_names()[0] if task_manager.get_task_names() else None,
                            label="é€‰æ‹©å¤„ç†ä»»åŠ¡"
                        )
                        
                        # å¤„ç†å‚æ•°
                        gr.Markdown("### å¤„ç†å‚æ•°")
                        with gr.Row():
                            batch_size = gr.Slider(
                                minimum=1, maximum=8000, value=10, step=1,
                                label="æ‰¹æ¬¡å¤§å°"
                            )
                            max_workers = gr.Slider(
                                minimum=1, maximum=200, value=3, step=1,
                                label="å¹¶å‘æ•°"
                            )
                        
                        # ä¿å­˜è®¾ç½®
                        gr.Markdown("### ä¿å­˜è®¾ç½®")
                        save_location = gr.Radio(
                            choices=["å½“å‰æ–‡ä»¶çš„outputç›®å½•", "è‡ªå®šä¹‰ç›®å½•"],
                            value="å½“å‰æ–‡ä»¶çš„outputç›®å½•",
                            label="ä¿å­˜ä½ç½®"
                        )
                        
                        custom_save_path = gr.Textbox(
                            label="è‡ªå®šä¹‰ä¿å­˜è·¯å¾„",
                            placeholder="è¾“å…¥è‡ªå®šä¹‰ä¿å­˜è·¯å¾„...",
                            visible=False
                        )
                        
                        # å¤„ç†æŒ‰é’®
                        with gr.Row():
                            process_btn = gr.Button("ğŸ”„ å¼€å§‹å¤„ç†", variant="primary", size="lg")
                            interrupt_btn = gr.Button("â¹ï¸ ä¸­æ–­å¤„ç†", variant="stop", size="lg")
                            clear_btn = gr.Button("ğŸ—‘ï¸ æ¸…é™¤ç»“æœ", variant="secondary", size="lg")
                    
                    with gr.Column(scale=1):
                        # æ–‡ä»¶é¢„è§ˆ
                        gr.Markdown("### æ–‡ä»¶é¢„è§ˆ")
                        file_preview = gr.Textbox(label="æ–‡ä»¶å†…å®¹é¢„è§ˆ", lines=8, interactive=False)
                        
                        # å¤„ç†çŠ¶æ€
                        gr.Markdown("### å¤„ç†çŠ¶æ€")
                        
                        # æ·»åŠ è¿›åº¦æ¡
                        progress_bar = gr.Progress()
                        processing_progress = gr.Slider(
                            minimum=0, maximum=100, value=0, step=0.1,
                            label="å¤„ç†è¿›åº¦ (%)",
                            interactive=False,
                            visible=True
                        )
                        
                        process_output = gr.Textbox(label="å¤„ç†æ—¥å¿—", lines=10, interactive=False)
                        
                        # ç»“æœé¢„è§ˆ
                        gr.Markdown("### ç»“æœé¢„è§ˆ")
                        result_preview = gr.Textbox(label="å¤„ç†ç»“æœè¯¦æƒ…", lines=12, interactive=False)
        
        # æ˜¾ç¤º/éšè—è‡ªå®šä¹‰é…ç½®
        def toggle_custom_config(preset):
            return gr.Group(visible=(preset == "è‡ªå®šä¹‰"))
        
        preset_dropdown.change(
            toggle_custom_config,
            inputs=[preset_dropdown],
            outputs=[custom_config]
        )
        
        # æ˜¾ç¤º/éšè—è‡ªå®šä¹‰ä¿å­˜è·¯å¾„
        save_location.change(
            fn=lambda x: gr.update(visible=(x == "è‡ªå®šä¹‰ç›®å½•")),
            inputs=[save_location],
            outputs=[custom_save_path]
        )
        
        # åŠ è½½æ¨¡å‹
        load_model_btn.click(
            load_model,
            inputs=[preset_dropdown, custom_name, custom_base_url, custom_api_key, custom_model_name, save_custom_config],
            outputs=[model_status, preset_dropdown]
        )
        
        # ä¸Šä¼ æ¨¡å¼åˆ‡æ¢å¤„ç†
        def toggle_upload_mode(mode):
            """åˆ‡æ¢ä¸Šä¼ æ¨¡å¼æ˜¾ç¤º"""
            if mode == "å•ä¸ªæ–‡ä»¶":
                return gr.update(visible=True), gr.update(visible=False), gr.update(visible=False)
            else:  # ç›®å½•ä¸Šä¼ 
                return gr.update(visible=False), gr.update(visible=True), gr.update(visible=True)
        
        upload_mode.change(
            fn=toggle_upload_mode,
            inputs=[upload_mode],
            outputs=[file_upload, directory_upload, file_filter_group]
        )
        
        # å•ä¸ªæ–‡ä»¶ä¸Šä¼ å¤„ç†
        file_upload.change(
            fn=handle_file_upload,
            inputs=[file_upload],
            outputs=[file_info, file_preview, column_dropdown]
        )
        
        # ç›®å½•ä¸Šä¼ å¤„ç†
        def handle_directory_change(files):
            """å¤„ç†ç›®å½•ä¸Šä¼ å˜åŒ–"""
            if not files:
                return "", "", gr.update(choices=[], visible=False), ""
            
            # è·å–æ‰€æœ‰æ–‡ä»¶è·¯å¾„
            file_paths = [f.name for f in files]
            
            # æ˜¾ç¤ºæ–‡ä»¶åˆ—è¡¨
            file_list = "\n".join([f"ğŸ“„ {os.path.basename(path)}" for path in file_paths])
            matched_files_display = f"ğŸ“ å·²é€‰æ‹© {len(file_paths)} ä¸ªæ–‡ä»¶:\n{file_list}"
            
            return "", "", gr.update(choices=[], visible=False), matched_files_display
        
        directory_upload.change(
            fn=handle_directory_change,
            inputs=[directory_upload],
            outputs=[file_info, file_preview, column_dropdown, matched_files_display]
        )
        
        # åº”ç”¨æ–‡ä»¶è¿‡æ»¤å™¨
        def apply_file_filter(files, pattern, extensions):
            """åº”ç”¨æ–‡ä»¶è¿‡æ»¤å™¨"""
            if not files:
                return "", "", gr.update(choices=[], visible=False), "âŒ è¯·å…ˆé€‰æ‹©ç›®å½•"
            
            try:
                # è·å–æ–‡ä»¶è·¯å¾„åˆ—è¡¨
                file_paths = [f.name for f in files]
                
                # åº”ç”¨è¿‡æ»¤å™¨
                info, preview, dropdown_update = handle_directory_upload(
                    files, pattern, extensions
                )
                
                return info, preview, dropdown_update, info.split("\n")[0] if info else "âœ… è¿‡æ»¤å™¨å·²åº”ç”¨"
            except Exception as e:
                return "", "", gr.update(choices=[], visible=False), f"âŒ è¿‡æ»¤å™¨åº”ç”¨å¤±è´¥: {str(e)}"
        
        apply_filter_btn.click(
            fn=apply_file_filter,
            inputs=[directory_upload, file_pattern, file_extension_filter],
            outputs=[file_info, file_preview, column_dropdown, matched_files_display]
        )
        
        # ä»»åŠ¡é€‰æ‹©æ—¶æ˜¾ç¤ºæç¤ºè¯
        task_dropdown.change(
            get_task_prompt,
            inputs=[task_dropdown],
            outputs=[task_prompt_display]
        )
        
        # ç¼–è¾‘ä»»åŠ¡
        edit_task_btn.click(
            edit_task,
            inputs=[task_dropdown, task_prompt_display],
            outputs=[task_status, task_dropdown, selected_task, single_selected_task]
        )
        
        # æ·»åŠ è‡ªå®šä¹‰ä»»åŠ¡
        add_task_btn.click(
            add_custom_task,
            inputs=[new_task_name, new_task_prompt],
            outputs=[task_status, task_dropdown, selected_task, single_selected_task]
        )
        
        # åˆ é™¤ä»»åŠ¡
        delete_task_btn.click(
            delete_task,
            inputs=[task_dropdown],
            outputs=[task_status, task_dropdown, selected_task, single_selected_task]
        )
        
        # é‡æ–°åŠ è½½ä»»åŠ¡
        reload_tasks_btn.click(
            reload_tasks,
            outputs=[task_status, task_dropdown, selected_task, single_selected_task]
        )
        
        # å¤„ç†æ•°æ®
        def process_with_progress(file_upload, column_dropdown, selected_task, batch_size, max_workers, save_location, custom_save_path):
            """å¸¦è¿›åº¦æ›´æ–°çš„å¤„ç†å‡½æ•°"""
            log, preview, progress = process_data(file_upload, column_dropdown, selected_task, batch_size, max_workers, save_location, custom_save_path)
            return log, preview, progress
        
        # å®æ—¶è¿›åº¦æ›´æ–°å‡½æ•°
        def start_processing(upload_mode, single_file, directory_files, column_dropdown, selected_task, batch_size, max_workers, save_location, custom_save_path):
            """å¼€å§‹å¤„ç†å¹¶æ˜¾ç¤ºå®æ—¶è¿›åº¦"""
            # é‡ç½®è¿›åº¦
            yield "ğŸš€ å¼€å§‹å¤„ç†...", "", 0.0
            
            # æ ¹æ®ä¸Šä¼ æ¨¡å¼é€‰æ‹©æ–‡ä»¶è¾“å…¥
            if upload_mode == "å•ä¸ªæ–‡ä»¶":
                file_input = single_file
            else:
                file_input = directory_files
            
            if not file_input:
                yield "âŒ è¯·å…ˆä¸Šä¼ æ–‡ä»¶", "", 0.0
                return
            
            # è°ƒç”¨æµå¼å¤„ç†å‡½æ•°
            try:
                for log, preview, progress in process_data_stream(file_input, column_dropdown, selected_task, batch_size, max_workers, save_location, custom_save_path):
                    yield log, preview, progress
            except Exception as e:
                yield f"âŒ å¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}", "", 0.0
        
        process_btn.click(
             fn=start_processing,
             inputs=[upload_mode, file_upload, directory_upload, column_dropdown, selected_task, batch_size, max_workers, save_location, custom_save_path],
             outputs=[process_output, result_preview, processing_progress]
         )
         
        # ä¸­æ–­å¤„ç†åŠŸèƒ½
        interrupt_btn.click(
            fn=interrupt_processing,
            outputs=[process_output]
        )
        
         # æ¸…é™¤ç»“æœåŠŸèƒ½
        def clear_results():
            return "", "", 0.0
            
        clear_btn.click(
            fn=clear_results,
            outputs=[process_output, result_preview, processing_progress]
        )
    
    return interface

# ==================== ä¸»ç¨‹åº ====================

if __name__ == "__main__":
    # åˆ›å»ºå¹¶å¯åŠ¨ç•Œé¢
    interface = create_interface()
    
    # å¯åŠ¨æœåŠ¡
    interface.launch(
        server_name="0.0.0.0",
        server_port=7863,
        share=False,
        debug=True
    )
