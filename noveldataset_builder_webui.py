# -*- coding: utf-8 -*-
"""
å°è¯´æ•°æ®é›†æ„å»º WebUIï¼ˆJSONL messages æ ¼å¼ï¼‰
- æ”¯æŒä¸Šä¼ TXTæ–‡ä»¶æˆ–æŒ‡å®šç›®å½•
- æ™ºèƒ½åˆ†æ®µï¼ŒæŒ‰æ®µè½åˆå¹¶è‡³é˜ˆå€¼
- åŠ©æ‰‹å†…å®¹å¯é€‰æ‹©ï¼šAIç”Ÿæˆ / ä½¿ç”¨åŸæ–‡ / ç•™ç©º
- å¯é…ç½® system æç¤ºè¯ä¸ä»»åŠ¡ç±»å‹ï¼ˆå†™ä½œ/ç»­å†™/ä¿®æ”¹ï¼‰
- å†…ç½®æ¨¡å‹è¿æ¥å’Œæç¤ºè¯ä»»åŠ¡ç®¡ç†ï¼Œç‹¬ç«‹è¿è¡Œ
"""
import os, re, sys, json, time, logging
from pathlib import Path
from typing import List, Dict, Optional, Tuple
from dataclasses import dataclass
from urllib.parse import urlparse
import gradio as gr
import openai
from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type

# å°è¯•å¯¼å…¥ chardetï¼ˆå¯é€‰ï¼‰
try:
    import chardet
    HAS_CHARDET = True
except Exception:
    HAS_CHARDET = False

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# ==================== AIæ¨¡å‹é…ç½®å’Œå®¢æˆ·ç«¯ ====================

@dataclass
class ModelConfig:
    """AIæ¨¡å‹é…ç½®ç±»"""
    name: str
    base_url: str
    api_key: str
    model_name: str
    timeout: int = 30
    max_retries: int = 3
    temperature: float = 0.7
    max_tokens: Optional[int] = 2048
    
    def __post_init__(self):
        """é…ç½®éªŒè¯"""
        if not self.base_url or not self.api_key or not self.model_name:
            raise ValueError("base_url, api_key, model_name ä¸èƒ½ä¸ºç©º")
        
        # éªŒè¯URLæ ¼å¼
        parsed = urlparse(self.base_url)
        if not parsed.scheme or not parsed.netloc:
            raise ValueError("base_url æ ¼å¼æ— æ•ˆ")

class AIModelClient:
    """AIæ¨¡å‹å®¢æˆ·ç«¯"""
    
    @classmethod
    def get_preset_configs(cls) -> Dict[str, Dict]:
        """è·å–é¢„è®¾æ¨¡å‹é…ç½®"""
        return {
            "DeepSeek": {
                "name": "DeepSeek",
                "base_url": "https://api.deepseek.com/v1",
                "api_key": "",
                "model_name": "deepseek-chat",
                "timeout": 30,
                "temperature": 0.7
            },
            "OpenAI": {
                "name": "OpenAI",
                "base_url": "https://api.openai.com/v1",
                "api_key": "",
                "model_name": "gpt-4o-mini",
                "timeout": 30,
                "temperature": 0.7
            },
            "æœ¬åœ°æ¨¡å‹": {
                "name": "æœ¬åœ°æ¨¡å‹",
                "base_url": "http://localhost:11434/v1",
                "api_key": "ollama",
                "model_name": "qwen2.5:7b",
                "timeout": 60,
                "temperature": 0.7
            }
        }
    
    @classmethod
    def load_custom_configs(cls) -> Dict[str, Dict]:
        """åŠ è½½è‡ªå®šä¹‰æ¨¡å‹é…ç½®"""
        config_file = "model_configs.json"
        try:
            if os.path.exists(config_file):
                with open(config_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
        except Exception as e:
            logger.warning(f"åŠ è½½æ¨¡å‹é…ç½®å¤±è´¥: {str(e)}")
        return {}
    
    @classmethod
    def save_custom_config(cls, config: Dict):
        """ä¿å­˜è‡ªå®šä¹‰æ¨¡å‹é…ç½®"""
        config_file = "model_configs.json"
        try:
            existing = cls.load_custom_configs()
            existing[config['name']] = config
            with open(config_file, 'w', encoding='utf-8') as f:
                json.dump(existing, f, ensure_ascii=False, indent=2)
        except Exception as e:
            logger.error(f"ä¿å­˜æ¨¡å‹é…ç½®å¤±è´¥: {str(e)}")
    
    @classmethod
    def get_all_configs(cls) -> Dict[str, Dict]:
        """è·å–æ‰€æœ‰é…ç½®ï¼ˆé¢„è®¾+è‡ªå®šä¹‰ï¼‰"""
        configs = cls.get_preset_configs()
        configs.update(cls.load_custom_configs())
        return configs
    
    def __init__(self, config: ModelConfig):
        self.config = config
        self.client = openai.OpenAI(
            base_url=config.base_url,
            api_key=config.api_key,
            timeout=config.timeout,
            max_retries=config.max_retries
        )
        logger.info(f"AIæ¨¡å‹å®¢æˆ·ç«¯å·²åˆå§‹åŒ–: {config.name}")
    
    @retry(
        stop=stop_after_attempt(5),
        wait=wait_exponential(multiplier=1, min=4, max=10),
        retry=retry_if_exception_type((
            openai.RateLimitError, 
            openai.APITimeoutError,
            openai.APIConnectionError,
            ConnectionError,
            TimeoutError,
            Exception
        ))
    )
    def process_text(self, text: str, prompt: str) -> str:
        """ä½¿ç”¨AIæ¨¡å‹å¤„ç†æ–‡æœ¬"""
        start_time = time.time()
        
        try:
            # æ£€æŸ¥æ–‡æœ¬é•¿åº¦ï¼Œè¿‡é•¿çš„æ–‡æœ¬è¿›è¡Œæˆªæ–­
            if len(text) > 8000:
                text = text[:8000] + "...[æ–‡æœ¬è¿‡é•¿å·²æˆªæ–­]"
                logger.warning(f"è¾“å…¥æ–‡æœ¬è¿‡é•¿ï¼Œå·²æˆªæ–­è‡³8000å­—ç¬¦")
            
            messages = [
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡æœ¬å¤„ç†åŠ©æ‰‹ï¼Œè¯·æ ¹æ®ç”¨æˆ·çš„è¦æ±‚å¤„ç†æ–‡æœ¬å†…å®¹ã€‚è¯·ä¿æŒå›å¤ç®€æ´æ˜äº†ã€‚"},
                {"role": "user", "content": f"è¯·æ ¹æ®ä»¥ä¸‹è¦æ±‚å¤„ç†æ–‡æœ¬ï¼š\n\nè¦æ±‚ï¼š{prompt}\n\næ–‡æœ¬å†…å®¹ï¼š{text}"}
            ]
            
            # è®¾ç½®åˆç†çš„max_tokensé™åˆ¶
            max_tokens = self.config.max_tokens
            if max_tokens is None or max_tokens > 4096:
                estimated_tokens = min(len(text), 2048)
                max_tokens = max(estimated_tokens, 256)
            
            max_tokens = min(max_tokens, 2048)
            
            response = self.client.chat.completions.create(
                model=self.config.model_name,
                messages=messages,
                temperature=self.config.temperature,
                max_tokens=max_tokens,
                stop=["<|im_end|>", "<|endoftext|>", "\n\n\n", "<|im_start|>", "\n\n"],
                timeout=self.config.timeout
            )
            
            result = response.choices[0].message.content.strip()
            
            # è®°å½•å¤„ç†æ—¶é—´
            elapsed_time = time.time() - start_time
            if elapsed_time > 20:
                logger.warning(f"æ–‡æœ¬å¤„ç†è€—æ—¶è¾ƒé•¿: {elapsed_time:.2f}ç§’")
            
            return result
            
        except (openai.APIConnectionError, ConnectionError) as e:
            logger.warning(f"è¿æ¥é”™è¯¯ï¼Œæ­£åœ¨é‡è¯•: {str(e)}")
            raise
        except (openai.APITimeoutError, TimeoutError) as e:
            logger.warning(f"è¯·æ±‚è¶…æ—¶ï¼Œæ­£åœ¨é‡è¯•: {str(e)}")
            raise
        except openai.RateLimitError as e:
            logger.warning(f"è¯·æ±‚é¢‘ç‡é™åˆ¶ï¼Œæ­£åœ¨é‡è¯•: {str(e)}")
            raise
        except Exception as e:
            logger.error(f"AIå¤„ç†å¤±è´¥: {str(e)}ï¼Œæ­£åœ¨é‡è¯•")
            raise
    
    def test_connection(self) -> bool:
        """æµ‹è¯•è¿æ¥"""
        try:
            response = self.process_text("æµ‹è¯•", "è¯·å›å¤'è¿æ¥æˆåŠŸ'")
            return "è¿æ¥æˆåŠŸ" in response or "æˆåŠŸ" in response
        except Exception:
            return False

# ==================== æç¤ºè¯ä»»åŠ¡ç®¡ç† ====================

class ReversePromptManager:
    """åæ¨æç¤ºè¯ç®¡ç†å™¨"""
    
    def __init__(self):
        self.custom_prompts_file = "custom_reverse_prompts.json"
        self._load_prompts()
    
    def _load_prompts(self):
        """åŠ è½½åæ¨æç¤ºè¯"""
        # å†…ç½®ä¼˜ç§€åæ¨æç¤ºè¯æ¨¡æ¿
        self.predefined_prompts = {
            "å°è¯´å†™ä½œ": {
                "åŸºç¡€åˆ›ä½œ": "è¯·æ ¹æ®ä»¥ä¸‹å°è¯´å†…å®¹ï¼Œåˆ†æå…¶ä¸»é¢˜ã€é£æ ¼ã€æƒ…èŠ‚ç‰¹ç‚¹å’Œå†™ä½œæ‰‹æ³•ï¼Œç„¶åç”Ÿæˆä¸€ä¸ªè‡ªç„¶çš„ç”¨æˆ·åˆ›ä½œè¯·æ±‚ã€‚ç”¨æˆ·å¸Œæœ›åˆ›ä½œå…·æœ‰ç›¸ä¼¼ç‰¹è‰²çš„å°è¯´ç‰‡æ®µï¼Œè¯·æ¨¡æ‹Ÿç”¨æˆ·å¯èƒ½æå‡ºçš„å…·ä½“å†™ä½œéœ€æ±‚ã€‚",
                "é£æ ¼æ¨¡ä»¿": "è¯·ä»”ç»†åˆ†æä»¥ä¸‹å°è¯´ç‰‡æ®µçš„å†™ä½œé£æ ¼ï¼ˆå¦‚å™è¿°è§†è§’ã€è¯­è¨€ç‰¹è‰²ã€èŠ‚å¥æ„Ÿã€ä¿®è¾æ‰‹æ³•ç­‰ï¼‰ï¼Œç„¶åç”Ÿæˆä¸€ä¸ªç”¨æˆ·è¯·æ±‚ï¼Œè¯¢é—®å¦‚ä½•å†™å‡ºå…·æœ‰ç›¸åŒé£æ ¼ç‰¹ç‚¹çš„å†…å®¹ã€‚è¯·ç¡®ä¿ç”Ÿæˆçš„è¯·æ±‚å…·ä½“ä¸”å®ç”¨ã€‚",
                "æƒ…èŠ‚æ„æ€": "è¯·æ ¹æ®ä»¥ä¸‹å°è¯´å†…å®¹ï¼Œè¯†åˆ«å…¶æƒ…èŠ‚ç±»å‹ã€å†²çªè®¾ç½®ã€æ•…äº‹ç»“æ„ç­‰ç‰¹ç‚¹ï¼Œç„¶åç”Ÿæˆä¸€ä¸ªç”¨æˆ·å…³äºæƒ…èŠ‚åˆ›ä½œçš„å…·ä½“è¯¢é—®ï¼ŒåŒ…æ‹¬åœºæ™¯è®¾å®šã€äººç‰©å…³ç³»ã€å†²çªå‘å±•ç­‰æ–¹é¢ã€‚",
                "äººç‰©å¡‘é€ ": "è¯·åˆ†æä»¥ä¸‹å°è¯´ä¸­çš„äººç‰©åˆ»ç”»æ‰‹æ³•ï¼ˆå¦‚å¤–è²Œæå†™ã€å¿ƒç†æå†™ã€å¯¹è¯ç‰¹ç‚¹ã€è¡Œä¸ºç‰¹å¾ç­‰ï¼‰ï¼Œç”Ÿæˆä¸€ä¸ªç”¨æˆ·è¯¢é—®å¦‚ä½•å¡‘é€ ç±»ä¼¼äººç‰©çš„è¯¦ç»†è¯·æ±‚ã€‚",
                "åœºæ™¯æå†™": "è¯·åˆ†æä»¥ä¸‹å°è¯´ä¸­çš„åœºæ™¯æå†™æŠ€å·§ï¼ˆå¦‚ç¯å¢ƒæ¸²æŸ“ã€æ°›å›´è¥é€ ã€ç»†èŠ‚åˆ»ç”»ç­‰ï¼‰ï¼Œç”Ÿæˆä¸€ä¸ªç”¨æˆ·è¯¢é—®å¦‚ä½•æå†™ç±»ä¼¼åœºæ™¯çš„è¯·æ±‚ã€‚",
                "å¯¹è¯å†™ä½œ": "è¯·åˆ†æä»¥ä¸‹å°è¯´ä¸­çš„å¯¹è¯å†™ä½œç‰¹ç‚¹ï¼ˆå¦‚è¯­è¨€é£æ ¼ã€äººç‰©æ€§æ ¼ä½“ç°ã€æ¨åŠ¨æƒ…èŠ‚ç­‰ï¼‰ï¼Œç”Ÿæˆä¸€ä¸ªç”¨æˆ·è¯¢é—®å¦‚ä½•å†™å¥½å¯¹è¯çš„è¯·æ±‚ã€‚",
                "æƒ…æ„Ÿè¡¨è¾¾": "è¯·åˆ†æä»¥ä¸‹å°è¯´ä¸­çš„æƒ…æ„Ÿè¡¨è¾¾æ–¹å¼ï¼ˆå¦‚å†…å¿ƒç‹¬ç™½ã€æƒ…æ„Ÿæ¸²æŸ“ã€æƒ…ç»ªå˜åŒ–ç­‰ï¼‰ï¼Œç”Ÿæˆä¸€ä¸ªç”¨æˆ·è¯¢é—®å¦‚ä½•è¡¨è¾¾ç±»ä¼¼æƒ…æ„Ÿçš„è¯·æ±‚ã€‚"
            },
            "å°è¯´ç»­å†™": {
                "æƒ…èŠ‚å»¶ç»­": "è¯·æ ¹æ®ä»¥ä¸‹å°è¯´å‰åŠéƒ¨åˆ†çš„å†…å®¹å’ŒååŠéƒ¨åˆ†çš„å‘å±•ï¼Œåˆ†ææƒ…èŠ‚çš„é€»è¾‘è¿æ¥ç‚¹ã€è½¬æŠ˜æ–¹å¼å’Œå‘å±•è„‰ç»œï¼Œç„¶åç”Ÿæˆä¸€ä¸ªç”¨æˆ·ç»­å†™è¯·æ±‚ï¼Œè¦æ±‚ä»å‰åŠéƒ¨åˆ†è‡ªç„¶è¿‡æ¸¡åˆ°ååŠéƒ¨åˆ†ã€‚",
                "é£æ ¼ä¿æŒ": "è¯·åˆ†æä»¥ä¸‹å°è¯´çš„å†™ä½œé£æ ¼ã€å™è¿°ç‰¹ç‚¹å’Œè¯­è¨€ç‰¹è‰²ï¼Œç”Ÿæˆä¸€ä¸ªç”¨æˆ·è¯·æ±‚ï¼Œè¯¢é—®å¦‚ä½•åœ¨ç»­å†™ä¸­ä¿æŒåŸæ–‡çš„é£æ ¼ä¸€è‡´æ€§ï¼ŒåŒ…æ‹¬è¯­è°ƒã€èŠ‚å¥ã€ç”¨è¯ä¹ æƒ¯ç­‰ã€‚",
                "æƒ…èŠ‚å‘å±•": "è¯·æ ¹æ®å°è¯´å‰æ–‡çš„é“ºå«å’Œåæ–‡çš„ç»“æœï¼Œæ¨æµ‹ä¸­é—´çš„æƒ…èŠ‚å‘å±•é€»è¾‘ï¼Œç”Ÿæˆä¸€ä¸ªç”¨æˆ·è¯¢é—®å¦‚ä½•åˆç†å‘å±•æƒ…èŠ‚çš„ç»­å†™è¯·æ±‚ã€‚",
                "äººç‰©å‘å±•": "è¯·åˆ†æå°è¯´ä¸­äººç‰©åœ¨å‰åæ–‡ä¸­çš„å˜åŒ–å’Œæˆé•¿ï¼Œç”Ÿæˆä¸€ä¸ªç”¨æˆ·è¯¢é—®å¦‚ä½•åœ¨ç»­å†™ä¸­å±•ç°äººç‰©å‘å±•çš„è¯·æ±‚ã€‚",
                "å†²çªæ¨è¿›": "è¯·åˆ†æå°è¯´ä¸­å†²çªçš„å‘å±•è„‰ç»œï¼Œç”Ÿæˆä¸€ä¸ªç”¨æˆ·è¯¢é—®å¦‚ä½•åœ¨ç»­å†™ä¸­æ¨è¿›å†²çªã€åˆ¶é€ æ‚¬å¿µçš„è¯·æ±‚ã€‚",
                "æ°›å›´è¥é€ ": "è¯·åˆ†æå°è¯´çš„æ•´ä½“æ°›å›´å’Œæƒ…ç»ªåŸºè°ƒï¼Œç”Ÿæˆä¸€ä¸ªç”¨æˆ·è¯¢é—®å¦‚ä½•åœ¨ç»­å†™ä¸­ä¿æŒæˆ–å‘å±•è¿™ç§æ°›å›´çš„è¯·æ±‚ã€‚"
            },
            "å°è¯´ä¿®æ”¹": {
                "æ–‡æœ¬æ¶¦è‰²": "è¯·å¯¹æ¯”ä¿®æ”¹å‰åçš„æ–‡æœ¬ï¼Œåˆ†æå…·ä½“çš„æ”¹è¿›æ–¹å‘ï¼ˆå¦‚è¯­è¨€æµç•…åº¦ã€è¡¨è¾¾å‡†ç¡®æ€§ã€æ–‡å­—ä¼˜ç¾åº¦ç­‰ï¼‰ï¼Œç„¶åç”Ÿæˆä¸€ä¸ªç”¨æˆ·å…³äºæ–‡æœ¬æ¶¦è‰²çš„å…·ä½“è¯·æ±‚ã€‚",
                "é£æ ¼è°ƒæ•´": "è¯·åˆ†ææ–‡æœ¬ä¿®æ”¹çš„é£æ ¼å˜åŒ–æ–¹å‘ï¼ˆå¦‚ä»å¹³å®åˆ°åä¸½ã€ä»ä¸¥è‚ƒåˆ°è½»æ¾ç­‰ï¼‰ï¼Œç”Ÿæˆä¸€ä¸ªç”¨æˆ·è¯¢é—®å¦‚ä½•è°ƒæ•´å†™ä½œé£æ ¼çš„è¯¦ç»†è¯·æ±‚ã€‚",
                "å†…å®¹å®Œå–„": "è¯·æ ¹æ®ä¿®æ”¹åå†…å®¹çš„æ”¹è¿›ç‚¹ï¼ˆå¦‚å¢åŠ ç»†èŠ‚ã€å®Œå–„é€»è¾‘ã€ä¸°å¯Œæƒ…æ„Ÿç­‰ï¼‰ï¼Œæ¨æµ‹ç”¨æˆ·æƒ³è¦å®Œå–„çš„å…·ä½“æ–¹é¢ï¼Œç”Ÿæˆç›¸åº”çš„ä¿®æ”¹è¯·æ±‚ã€‚",
                "ç»“æ„ä¼˜åŒ–": "è¯·åˆ†ææ–‡æœ¬åœ¨ç»“æ„æ–¹é¢çš„è°ƒæ•´ï¼ˆå¦‚æ®µè½é‡ç»„ã€é€»è¾‘æ¢³ç†ã€å±‚æ¬¡åˆ†æ˜ç­‰ï¼‰ï¼Œç”Ÿæˆä¸€ä¸ªç”¨æˆ·è¯¢é—®å¦‚ä½•ä¼˜åŒ–æ–‡æœ¬ç»“æ„çš„è¯·æ±‚ã€‚",
                "è¯­è¨€ç²¾ç‚¼": "è¯·å¯¹æ¯”ä¿®æ”¹å‰åçš„è¯­è¨€è¡¨è¾¾ï¼Œåˆ†æç²¾ç‚¼å’Œä¼˜åŒ–çš„æ–¹å‘ï¼Œç”Ÿæˆä¸€ä¸ªç”¨æˆ·è¯¢é—®å¦‚ä½•è®©è¯­è¨€æ›´åŠ ç²¾ç‚¼æœ‰åŠ›çš„è¯·æ±‚ã€‚",
                "æƒ…æ„Ÿå¼ºåŒ–": "è¯·åˆ†æä¿®æ”¹åæ–‡æœ¬åœ¨æƒ…æ„Ÿè¡¨è¾¾æ–¹é¢çš„å¢å¼ºï¼Œç”Ÿæˆä¸€ä¸ªç”¨æˆ·è¯¢é—®å¦‚ä½•å¼ºåŒ–æƒ…æ„Ÿè¡¨è¾¾æ•ˆæœçš„è¯·æ±‚ã€‚",
                "é€»è¾‘å®Œå–„": "è¯·åˆ†æä¿®æ”¹åæ–‡æœ¬åœ¨é€»è¾‘æ–¹é¢çš„å®Œå–„ï¼Œç”Ÿæˆä¸€ä¸ªç”¨æˆ·è¯¢é—®å¦‚ä½•è®©æ–‡æœ¬é€»è¾‘æ›´åŠ æ¸…æ™°ä¸¥å¯†çš„è¯·æ±‚ã€‚"
            }
        }
        
        # åŠ è½½è‡ªå®šä¹‰åæ¨æç¤ºè¯
        self.custom_prompts = {}
        try:
            if os.path.exists(self.custom_prompts_file):
                with open(self.custom_prompts_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    if isinstance(data, dict):
                        self.custom_prompts = data
                    else:
                        logger.warning(f"è‡ªå®šä¹‰åæ¨æç¤ºè¯æ–‡ä»¶æ ¼å¼é”™è¯¯ï¼Œå·²é‡ç½®ä¸ºç©ºå­—å…¸")
                        self.custom_prompts = {}
        except Exception as e:
            logger.warning(f"åŠ è½½è‡ªå®šä¹‰åæ¨æç¤ºè¯å¤±è´¥: {str(e)}")
            self.custom_prompts = {}
    
    def get_prompt_categories(self, task_type: str) -> List[str]:
        """è·å–æŒ‡å®šä»»åŠ¡ç±»å‹çš„æç¤ºè¯åˆ†ç±»"""
        categories = []
        if task_type in self.predefined_prompts:
            categories.extend(list(self.predefined_prompts[task_type].keys()))
        if task_type in self.custom_prompts:
            categories.extend(list(self.custom_prompts[task_type].keys()))
        return categories
    
    def get_prompt(self, task_type: str, category: str) -> str:
        """è·å–æŒ‡å®šçš„åæ¨æç¤ºè¯"""
        # å…ˆæŸ¥æ‰¾é¢„è®¾æç¤ºè¯
        if task_type in self.predefined_prompts and category in self.predefined_prompts[task_type]:
            return self.predefined_prompts[task_type][category]
        # å†æŸ¥æ‰¾è‡ªå®šä¹‰æç¤ºè¯
        if task_type in self.custom_prompts and category in self.custom_prompts[task_type]:
            return self.custom_prompts[task_type][category]
        return ""
    
    def add_custom_prompt(self, task_type: str, category: str, prompt: str) -> bool:
        """æ·»åŠ è‡ªå®šä¹‰åæ¨æç¤ºè¯"""
        try:
            if task_type not in self.custom_prompts:
                self.custom_prompts[task_type] = {}
            self.custom_prompts[task_type][category] = prompt
            with open(self.custom_prompts_file, 'w', encoding='utf-8') as f:
                json.dump(self.custom_prompts, f, ensure_ascii=False, indent=2)
            return True
        except Exception as e:
            logger.error(f"æ·»åŠ è‡ªå®šä¹‰åæ¨æç¤ºè¯å¤±è´¥: {str(e)}")
            return False
    
    def delete_custom_prompt(self, task_type: str, category: str) -> bool:
        """åˆ é™¤è‡ªå®šä¹‰åæ¨æç¤ºè¯"""
        try:
            if task_type in self.custom_prompts and category in self.custom_prompts[task_type]:
                del self.custom_prompts[task_type][category]
                if not self.custom_prompts[task_type]:  # å¦‚æœåˆ†ç±»ä¸ºç©ºï¼Œåˆ é™¤æ•´ä¸ªä»»åŠ¡ç±»å‹
                    del self.custom_prompts[task_type]
                with open(self.custom_prompts_file, 'w', encoding='utf-8') as f:
                    json.dump(self.custom_prompts, f, ensure_ascii=False, indent=2)
                return True
            return False
        except Exception as e:
            logger.error(f"åˆ é™¤è‡ªå®šä¹‰åæ¨æç¤ºè¯å¤±è´¥: {str(e)}")
            return False

# ==================== å…¨å±€å˜é‡ ====================

# å…¨å±€å˜é‡
current_model_client = None
reverse_prompt_manager = ReversePromptManager()

# ==================== åŸºæœ¬å·¥å…·å‡½æ•° ====================

def detect_encoding(file_path: str) -> str:
    if HAS_CHARDET:
        try:
            with open(file_path, 'rb') as f:
                raw = f.read(10000)
            r = chardet.detect(raw)
            if r.get('encoding') and r.get('confidence', 0) > 0.7:
                return r['encoding']
        except Exception:
            pass
    for enc in ['utf-8', 'utf-8-sig', 'gbk', 'gb2312', 'gb18030', 'big5', 'latin1']:
        try:
            with open(file_path, 'r', encoding=enc) as f:
                f.read(1000)
            return enc
        except Exception:
            continue
    return 'utf-8'

def read_text(file_path: str) -> str:
    enc = detect_encoding(file_path)
    with open(file_path, 'r', encoding=enc, errors='ignore') as f:
        return f.read()

# ç®€æ˜“åˆ†æ®µï¼šæŒ‰åŒæ¢è¡Œåˆ†æ®µï¼Œåˆå¹¶è‡³ [min_len, max_len]

def split_text(text: str, min_len: int = 200, max_len: int = 800) -> List[str]:
    paras = [p.strip() for p in re.split(r"\n{2,}", text) if p.strip()]
    segs, cur = [], ''
    for p in paras:
        if not cur:
            cur = p
        elif len(cur) + len(p) + 2 <= max_len:
            cur += "\n\n" + p
        else:
            if len(cur) < min_len and segs:
                segs[-1] += "\n\n" + cur
            else:
                segs.append(cur)
            cur = p
    if cur:
        if len(cur) < min_len and segs:
            segs[-1] += "\n\n" + cur
        else:
            segs.append(cur)
    return segs

# æ„å»º messagesï¼ˆä¿ç•™å…¼å®¹å‡½æ•°ï¼‰

def build_messages(segment: str, system_prompt: str, task_type: str, user_extra: str) -> List[Dict[str, str]]:
    """æ„å»ºmessagesï¼ˆå…¼å®¹å‡½æ•°ï¼Œæ–°ç‰ˆæœ¬ä¸­å·²åœ¨build_jsonlä¸­ç›´æ¥æ„å»ºï¼‰"""
    if task_type == 'ç»­å†™':
        user_msg = f"è¯·ç»­å†™ä»¥ä¸‹æ®µè½ï¼š\n\nã€{segment}ã€"
    elif task_type == 'ä¿®æ”¹':
        hint = user_extra.strip() or 'è¯·ä¼˜åŒ–æ–‡ç¬”ï¼Œä½¿å…¶æ›´æµç•…ã€ç”ŸåŠ¨ä¸”ç¬¦åˆä¸­æ–‡å†™ä½œä¹ æƒ¯ã€‚'
        user_msg = f"è¯·æ ¹æ®ä»¥ä¸‹è¦æ±‚ä¼˜åŒ–è¿™æ®µæ–‡æœ¬ï¼š\n\nè¦æ±‚ï¼š{hint}\n\næ–‡æœ¬ï¼š\nã€{segment}ã€"
    else:  # å†™ä½œ
        prompt = user_extra.strip() or 'è¯·æ ¹æ®ç»™å®šé¢˜æä¸é£æ ¼åˆ›ä½œä¸€æ®µå°è¯´ç‰‡æ®µï¼ˆ300-500å­—ï¼‰ã€‚'
        user_msg = prompt
    return [
        {"role": "system", "content": system_prompt.strip() or "ä½ æ˜¯ä¸€åä¸“ä¸šçš„å°è¯´åˆ›ä½œåŠ©æ‰‹ã€‚"},
        {"role": "user", "content": user_msg}
    ]

# AI ç”ŸæˆåŠ©æ‰‹å›å¤

# AIç”ŸæˆåŠ©æ‰‹å†…å®¹çš„åŠŸèƒ½å·²ç§»é™¤ï¼Œç°åœ¨åªä½¿ç”¨åŸæ–‡å†…å®¹

def ai_generate_writing_prompt(client: AIModelClient, assistant_content: str, user_extra: str, focus_points: str = "") -> str:
    """æ ¹æ®ç”Ÿæˆçš„å†…å®¹åæ¨åˆ›ä½œè¦æ±‚ï¼ˆå…¼å®¹å‡½æ•°ï¼‰"""
    return ai_generate_writing_prompt_custom(client, assistant_content, user_extra, focus_points, "")

def ai_generate_writing_prompt_custom(client: AIModelClient, assistant_content: str, user_extra: str, focus_points: str = "", custom_prompt: str = "") -> str:
    """æ ¹æ®ç”Ÿæˆçš„å†…å®¹åæ¨åˆ›ä½œè¦æ±‚ï¼ˆæ”¯æŒè‡ªå®šä¹‰æç¤ºè¯ï¼‰"""
    try:
        prompt = custom_prompt.strip() or "è¯·æ ¹æ®ä»¥ä¸‹ç”Ÿæˆçš„å°è¯´å†…å®¹ï¼Œåæ¨å‡ºä¸€ä¸ªåˆç†çš„åˆ›ä½œè¦æ±‚æˆ–å†™ä½œæŒ‡ä»¤ï¼Œä½œä¸ºç”¨æˆ·çš„æé—®ã€‚è¦æ±‚ç®€æ´æ˜äº†ï¼Œç¬¦åˆåˆ›ä½œé€»è¾‘ã€‚æ ¼å¼å¦‚ï¼š'å†™ä¸€ä¸ª[é¢˜æ]å°è¯´çš„[åœºæ™¯]ï¼Œ[å…·ä½“è¦æ±‚]ã€‚'"
        
        text = f"ç”Ÿæˆçš„å†…å®¹ï¼š{assistant_content}\n\nç”¨æˆ·é¢å¤–è¦æ±‚ï¼š{user_extra}"
        if focus_points.strip():
            text += f"\n\nå…³æ³¨ç‚¹ï¼š{focus_points.strip()}"
            if not custom_prompt.strip():  # åªæœ‰åœ¨ä½¿ç”¨é»˜è®¤æç¤ºè¯æ—¶æ‰æ·»åŠ å…³æ³¨ç‚¹è¯´æ˜
                prompt += f"\n\nè¯·ç‰¹åˆ«å…³æ³¨ä»¥ä¸‹æ–¹é¢ï¼š{focus_points.strip()}"
        
        result = client.process_text(text, prompt)
        if not result or result.strip() == "":
            return user_extra.strip() or "è¯·åˆ›ä½œä¸€æ®µå°è¯´å†…å®¹ã€‚"
        return result.strip()
    except Exception as e:
        logger.error(f"ç”Ÿæˆå†™ä½œæç¤ºè¯å¤±è´¥: {str(e)}")
        return user_extra.strip() or "è¯·åˆ›ä½œä¸€æ®µå°è¯´å†…å®¹ã€‚"

def ai_generate_modification_prompt(client: AIModelClient, original_segment: str, modified_content: str, user_extra: str, focus_points: str = "") -> Tuple[str, str]:
    """æ ¹æ®ä¿®æ”¹åçš„å†…å®¹åæ¨åŸæ–‡å’Œä¿®æ”¹è¦æ±‚ï¼ˆå…¼å®¹å‡½æ•°ï¼‰"""
    return ai_generate_modification_prompt_custom(client, original_segment, modified_content, user_extra, focus_points, "")

def ai_generate_modification_prompt_custom(client: AIModelClient, original_segment: str, modified_content: str, user_extra: str, focus_points: str = "", custom_prompt: str = "") -> Tuple[str, str]:
    """æ ¹æ®ä¿®æ”¹åçš„å†…å®¹åæ¨åŸæ–‡å’Œä¿®æ”¹è¦æ±‚ï¼ˆæ”¯æŒè‡ªå®šä¹‰æç¤ºè¯ï¼‰"""
    try:
        # ç”Ÿæˆä¿®æ”¹è¦æ±‚
        prompt = custom_prompt.strip() or "è¯·æ ¹æ®åŸæ–‡å’Œä¿®æ”¹åçš„å†…å®¹ï¼Œæ¨æ–­å‡ºç”¨æˆ·å¯èƒ½æå‡ºçš„ä¿®æ”¹è¦æ±‚ã€‚è¦æ±‚ç®€æ´æ˜äº†ï¼Œå…·ä½“å¯æ“ä½œã€‚"
        text = f"åŸæ–‡ï¼š{original_segment}\n\nä¿®æ”¹åå†…å®¹ï¼š{modified_content}\n\nç”¨æˆ·é¢å¤–è¦æ±‚ï¼š{user_extra}"
        
        if focus_points.strip():
            text += f"\n\nå…³æ³¨ç‚¹ï¼š{focus_points.strip()}"
            if not custom_prompt.strip():  # åªæœ‰åœ¨ä½¿ç”¨é»˜è®¤æç¤ºè¯æ—¶æ‰æ·»åŠ å…³æ³¨ç‚¹è¯´æ˜
                prompt += f"\n\nè¯·ç‰¹åˆ«å…³æ³¨ä»¥ä¸‹æ–¹é¢ï¼š{focus_points.strip()}"
        
        modification_request = client.process_text(text, prompt)
        if not modification_request or modification_request.strip() == "":
            modification_request = user_extra.strip() or "è¯·ä¼˜åŒ–æ–‡ç¬”ï¼Œä½¿å…¶æ›´æµç•…ã€ç”ŸåŠ¨ä¸”ç¬¦åˆä¸­æ–‡å†™ä½œä¹ æƒ¯ã€‚"
        
        # ç”Ÿæˆç¨å¾®ç®€åŒ–çš„åŸæ–‡ï¼ˆæ¨¡æ‹Ÿæœªä¿®æ”¹å‰çš„çŠ¶æ€ï¼‰
        prompt2 = "è¯·å°†ä»¥ä¸‹æ–‡æœ¬ç¨å¾®ç®€åŒ–ï¼Œé™ä½æ–‡å­¦æ€§ï¼Œæ¨¡æ‹Ÿä¿®æ”¹å‰çš„åŸå§‹çŠ¶æ€ã€‚ä¿æŒä¸»è¦å†…å®¹ä¸å˜ï¼Œä½†è¡¨è¾¾æ›´æœ´ç´ ä¸€äº›ã€‚"
        original_text = client.process_text(modified_content, prompt2)
        if not original_text or original_text.strip() == "":
            original_text = original_segment
        
        return original_text.strip(), modification_request.strip()
    except Exception as e:
        logger.error(f"ç”Ÿæˆä¿®æ”¹æç¤ºè¯å¤±è´¥: {str(e)}")
        modification_request = user_extra.strip() or "è¯·ä¼˜åŒ–æ–‡ç¬”ï¼Œä½¿å…¶æ›´æµç•…ã€ç”ŸåŠ¨ä¸”ç¬¦åˆä¸­æ–‡å†™ä½œä¹ æƒ¯ã€‚"
        return original_segment, modification_request

def ai_generate_continuation_prompt_custom(segment: str, user_extra: str, custom_prompt: str = "") -> str:
    """ç”Ÿæˆç»­å†™ä»»åŠ¡çš„ç”¨æˆ·æé—®å†…å®¹ï¼ˆæ”¯æŒè‡ªå®šä¹‰æç¤ºè¯ï¼‰"""
    try:
        prompt_template = custom_prompt.strip() or "è¯·ç»­å†™ä»¥ä¸‹æ®µè½ï¼š"
        
        # æˆªå–å‰200å­—ç¬¦ä½œä¸ºå±•ç¤º
        display_segment = segment[:200] + "..." if len(segment) > 200 else segment
        
        if user_extra.strip():
            return f"{prompt_template}\n\nã€{display_segment}ã€\n\nç»­å†™è¦æ±‚ï¼š{user_extra.strip()}"
        else:
            return f"{prompt_template}\n\nã€{display_segment}ã€"
    except Exception as e:
        logger.error(f"ç”Ÿæˆç»­å†™æç¤ºè¯å¤±è´¥: {str(e)}")
        return f"è¯·ç»­å†™ä»¥ä¸‹æ®µè½ï¼š\n\nã€{segment[:200]}...ã€"

# ä¿ç•™åŸæœ‰çš„ai_generateå‡½æ•°ä»¥å…¼å®¹å…¶ä»–å¯èƒ½çš„è°ƒç”¨
# AIç”ŸæˆåŠ©æ‰‹å†…å®¹åŠŸèƒ½å·²ç§»é™¤ï¼Œåªä½¿ç”¨åŸæ–‡å†…å®¹

def ai_generate_user_content(client: AIModelClient, task_type: str, assistant_content: str, user_extra: str) -> str:
    """åŸºäºassistantå†…å®¹ç”Ÿæˆuserå†…å®¹ï¼ˆå…¼å®¹å‡½æ•°ï¼‰"""
    if task_type == 'å†™ä½œ':
        return ai_generate_writing_prompt(client, assistant_content, user_extra)
    elif task_type == 'ç»­å†™':
        return f"è¯·ç»­å†™ä»¥ä¸‹æ®µè½ï¼š\n\nã€{assistant_content[:200]}...ã€"
    else:  # ä¿®æ”¹
        original_text, modification_request = ai_generate_modification_prompt(client, assistant_content, assistant_content, user_extra)
        return f"è¯·æ ¹æ®ä»¥ä¸‹è¦æ±‚ä¼˜åŒ–è¿™æ®µæ–‡æœ¬ï¼š\n\nè¦æ±‚ï¼š{modification_request}\n\næ–‡æœ¬ï¼š\nã€{original_text}ã€"

def get_default_user_content(task_type: str, user_extra: str) -> str:
    """è·å–é»˜è®¤ç”¨æˆ·å†…å®¹"""
    if task_type == 'å†™ä½œ':
        return user_extra.strip() or "è¯·åˆ›ä½œä¸€æ®µå°è¯´å†…å®¹ã€‚"
    elif task_type == 'ç»­å†™':
        return "è¯·ç»­å†™ä»¥ä¸‹å†…å®¹ã€‚"
    else:  # ä¿®æ”¹
        return user_extra.strip() or "è¯·ä¼˜åŒ–ä»¥ä¸‹æ–‡æœ¬å†…å®¹ã€‚"

def get_default_prompt(task_type: str) -> str:
    """è·å–é»˜è®¤æç¤ºè¯"""
    if task_type == 'å†™ä½œ':
        return 'è¯·æ ¹æ®å‚è€ƒæ–‡æœ¬å’Œåˆ›ä½œè¦æ±‚ï¼Œç”Ÿæˆä¸€æ®µé«˜è´¨é‡çš„å°è¯´å†…å®¹ã€‚ä¿æŒæ–‡é£ä¸€è‡´ï¼Œæƒ…èŠ‚åˆç†ï¼Œå­—æ•°æ§åˆ¶åœ¨300-500å­—ã€‚'
    elif task_type == 'ç»­å†™':
        return 'è¯·æ ¹æ®ä¸Šæ–‡å†…å®¹ï¼Œè‡ªç„¶åœ°ç»­å†™æ¥ä¸‹æ¥çš„æƒ…èŠ‚ã€‚ä¿æŒåŸæœ‰çš„å†™ä½œé£æ ¼ã€äººç‰©æ€§æ ¼å’Œæ•…äº‹èŠ‚å¥ã€‚ç»­å†™å†…å®¹åº”è¯¥åœ¨300-500å­—ä¹‹é—´ï¼Œæƒ…èŠ‚è¦æœ‰é€»è¾‘æ€§å’Œè¿è´¯æ€§ã€‚'
    else:  # ä¿®æ”¹
        return 'è¯·å¯¹åŸæ–‡è¿›è¡Œæ¶¦è‰²å’Œä¼˜åŒ–ï¼Œæå‡è¯­è¨€è¡¨è¾¾çš„æµç•…æ€§ã€å‡†ç¡®æ€§å’Œæ–‡å­¦æ€§ã€‚ä¿æŒåŸæ„ä¸å˜ï¼Œä½†è®©è¡¨è¾¾æ›´åŠ ç”ŸåŠ¨ã€è‡ªç„¶ã€‚'

def get_random_reverse_prompt(reverse_prompt_manager, task_type: str, category: str) -> str:
    """éšæœºè·å–åæ¨æç¤ºè¯æ¨¡æ¿"""
    import random
    
    # è·å–æŒ‡å®šç±»åˆ«çš„æ‰€æœ‰æç¤ºè¯
    prompt = reverse_prompt_manager.get_prompt(task_type, category)
    if not prompt:
        return ""
    
    # å¦‚æœæç¤ºè¯åŒ…å«å¤šä¸ªæ¨¡æ¿ï¼ˆç”¨æ¢è¡Œç¬¦åˆ†éš”ï¼‰ï¼Œéšæœºé€‰æ‹©ä¸€ä¸ª
    templates = [template.strip() for template in prompt.split('\n') if template.strip()]
    if templates:
        return random.choice(templates)
    
    return prompt

# æ‰«æç›®å½• txt æ–‡ä»¶

def scan_dir_txts(dir_path: str) -> List[str]:
    p = Path(dir_path)
    if not p.exists():
        return []
    return [str(fp) for fp in p.rglob('*.txt') if fp.is_file()]

# ==================== æ¨¡å‹ç®¡ç†å‡½æ•° ====================

def load_model_config(config_name: str, base_url: str = "", api_key: str = "", 
                     model_name: str = "", timeout: int = 30) -> Tuple[str, str]:
    """åŠ è½½æ¨¡å‹é…ç½®"""
    global current_model_client
    
    try:
        if config_name == "è‡ªå®šä¹‰":
            if not (base_url and api_key and model_name):
                return "âŒ è‡ªå®šä¹‰é…ç½®éœ€è¦å¡«å†™å®Œæ•´çš„è¿æ¥ä¿¡æ¯", ""
            
            config = ModelConfig(
                name="è‡ªå®šä¹‰",
                base_url=base_url,
                api_key=api_key,
                model_name=model_name,
                timeout=timeout
            )
        else:
            # ä»é¢„è®¾æˆ–è‡ªå®šä¹‰é…ç½®ä¸­åŠ è½½
            all_configs = AIModelClient.get_all_configs()
            if config_name not in all_configs:
                return f"âŒ é…ç½® '{config_name}' ä¸å­˜åœ¨", ""
            
            config_dict = all_configs[config_name]
            if not config_dict.get('api_key'):
                return f"âŒ é…ç½® '{config_name}' ç¼ºå°‘ API Key", ""
            
            config = ModelConfig(
                name=config_dict['name'],
                base_url=config_dict['base_url'],
                api_key=config_dict['api_key'],
                model_name=config_dict['model_name'],
                timeout=config_dict.get('timeout', 30)
            )
        
        # åˆ›å»ºå®¢æˆ·ç«¯
        current_model_client = AIModelClient(config)
        
        # æµ‹è¯•è¿æ¥
        logger.info(f"æ­£åœ¨æµ‹è¯•æ¨¡å‹è¿æ¥: {config.name}")
        if current_model_client.test_connection():
            message = f"âœ… æ¨¡å‹ {config.name} åŠ è½½æˆåŠŸå¹¶è¿æ¥æ­£å¸¸"
            logger.info(message)
            return message, "è¿æ¥æˆåŠŸ"
        else:
            message = f"âš ï¸ æ¨¡å‹ {config.name} åŠ è½½æˆåŠŸä½†è¿æ¥æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥é…ç½®"
            logger.warning(message)
            return message, "è¿æ¥å¤±è´¥"
            
    except Exception as e:
        error_msg = f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}"
        logger.error(error_msg)
        return error_msg, "åŠ è½½å¤±è´¥"

def save_model_config(name: str, base_url: str, api_key: str, model_name: str, 
                     timeout: int) -> str:
    """ä¿å­˜æ¨¡å‹é…ç½®"""
    try:
        if not all([name, base_url, api_key, model_name]):
            return "âŒ æ‰€æœ‰å­—æ®µéƒ½å¿…é¡»å¡«å†™"
        
        config = {
            "name": name,
            "base_url": base_url,
            "api_key": api_key,
            "model_name": model_name,
            "timeout": timeout,
            "temperature": 0.7
        }
        
        AIModelClient.save_custom_config(config)
        return f"âœ… é…ç½® '{name}' ä¿å­˜æˆåŠŸ"
        
    except Exception as e:
        return f"âŒ ä¿å­˜å¤±è´¥: {str(e)}"

# ==================== ä»»åŠ¡ç®¡ç†å‡½æ•° ====================

# åæ¨æç¤ºè¯ç®¡ç†å‡½æ•°
def update_reverse_categories(task_type: str) -> gr.Dropdown:
    """æ›´æ–°åæ¨æç¤ºè¯åˆ†ç±»é€‰æ‹©"""
    categories = reverse_prompt_manager.get_prompt_categories(task_type)
    return gr.Dropdown(choices=categories, value=categories[0] if categories else None)

def update_reverse_prompt_display(task_type: str, category: str) -> str:
    """æ›´æ–°åæ¨æç¤ºè¯æ˜¾ç¤º"""
    if not task_type or not category:
        return ""
    return reverse_prompt_manager.get_prompt(task_type, category)

def save_reverse_prompt(task_type: str, category: str, prompt: str) -> str:
    """ä¿å­˜åæ¨æç¤ºè¯"""
    if not task_type or not category or not prompt:
        return "âŒ ä»»åŠ¡ç±»å‹ã€åˆ†ç±»å’Œæç¤ºè¯ä¸èƒ½ä¸ºç©º"
    
    # å¦‚æœæ˜¯é¢„è®¾æç¤ºè¯ï¼Œåˆ™æ·»åŠ ä¸ºè‡ªå®šä¹‰æç¤ºè¯
    if reverse_prompt_manager.add_custom_prompt(task_type, category, prompt):
        return f"âœ… åæ¨æç¤ºè¯ '{category}' ä¿å­˜æˆåŠŸ"
    else:
        return "âŒ ä¿å­˜åæ¨æç¤ºè¯å¤±è´¥"

def add_custom_reverse_prompt(task_type: str, category: str, prompt: str) -> Tuple[str, gr.Dropdown]:
    """æ·»åŠ è‡ªå®šä¹‰åæ¨æç¤ºè¯"""
    if not task_type or not category or not prompt:
        return "âŒ ä»»åŠ¡ç±»å‹ã€åˆ†ç±»åç§°å’Œæç¤ºè¯ä¸èƒ½ä¸ºç©º", gr.Dropdown()
    
    if reverse_prompt_manager.add_custom_prompt(task_type, category, prompt):
        # æ›´æ–°åˆ é™¤é€‰æ‹©åˆ—è¡¨
        custom_choices = []
        for t_type, categories in reverse_prompt_manager.custom_prompts.items():
            for cat in categories.keys():
                custom_choices.append(f"{t_type}:{cat}")
        return f"âœ… è‡ªå®šä¹‰åæ¨æç¤ºè¯ '{category}' æ·»åŠ æˆåŠŸ", gr.Dropdown(choices=custom_choices)
    else:
        return "âŒ æ·»åŠ è‡ªå®šä¹‰åæ¨æç¤ºè¯å¤±è´¥", gr.Dropdown()

def delete_custom_reverse_prompt(selection: str) -> Tuple[str, gr.Dropdown]:
    """åˆ é™¤è‡ªå®šä¹‰åæ¨æç¤ºè¯"""
    if not selection:
        return "âŒ è¯·é€‰æ‹©è¦åˆ é™¤çš„æç¤ºè¯", gr.Dropdown()
    
    try:
        task_type, category = selection.split(':', 1)
        if reverse_prompt_manager.delete_custom_prompt(task_type, category):
            # æ›´æ–°åˆ é™¤é€‰æ‹©åˆ—è¡¨
            custom_choices = []
            for t_type, categories in reverse_prompt_manager.custom_prompts.items():
                for cat in categories.keys():
                    custom_choices.append(f"{t_type}:{cat}")
            return f"âœ… è‡ªå®šä¹‰åæ¨æç¤ºè¯ '{category}' åˆ é™¤æˆåŠŸ", gr.Dropdown(choices=custom_choices)
        else:
            return "âŒ åˆ é™¤è‡ªå®šä¹‰åæ¨æç¤ºè¯å¤±è´¥", gr.Dropdown()
    except ValueError:
        return "âŒ é€‰æ‹©æ ¼å¼é”™è¯¯", gr.Dropdown()

def update_template_display(task_type: str) -> str:
    """æ›´æ–°å†…ç½®æ¨¡æ¿æ˜¾ç¤º"""
    if task_type in reverse_prompt_manager.predefined_prompts:
        templates = reverse_prompt_manager.predefined_prompts[task_type]
        return '\n'.join([f"â€¢ {cat}: {prompt[:80]}..." for cat, prompt in templates.items()])
    return "æš‚æ— å†…ç½®æ¨¡æ¿"

# ==================== æ ¸å¿ƒå¤„ç†å‡½æ•° ====================

def build_jsonl(dir_path: str, output_dir: str, min_len: int, max_len: int, 
                task_selection: List[str], system_prompt: str, user_extra: str,
                reverse_prompt_type: str, 
                writing_reverse_category: str, continuation_reverse_category: str, modification_reverse_category: str,
                custom_writing_reverse: str, custom_continuation_reverse: str, custom_modification_reverse: str,
                progress=gr.Progress()) -> Tuple[str, str]:
    """æ„å»ºJSONLæ•°æ®é›†"""
    global current_model_client, reverse_prompt_manager
    
    # å›ºå®šä½¿ç”¨åŸæ–‡æ¨¡å¼ï¼ŒAIç”ŸæˆåŠŸèƒ½å·²ç§»é™¤
    mode = 'ä½¿ç”¨åŸæ–‡'
    
    # æ ¹æ®åæ¨æç¤ºè¯ç±»å‹è·å–æç¤ºè¯
    if reverse_prompt_type == 'å†…ç½®':
        # ä½¿ç”¨å†…ç½®æç¤ºè¯ï¼Œæ”¯æŒéšæœºé€‰æ‹©
        writing_reverse_prompt = get_random_reverse_prompt(reverse_prompt_manager, 'å°è¯´å†™ä½œ', writing_reverse_category) if writing_reverse_category else ""
        continuation_reverse_prompt = get_random_reverse_prompt(reverse_prompt_manager, 'å°è¯´ç»­å†™', continuation_reverse_category) if continuation_reverse_category else ""
        modification_reverse_prompt = get_random_reverse_prompt(reverse_prompt_manager, 'å°è¯´ä¿®æ”¹', modification_reverse_category) if modification_reverse_category else ""
    else:
        # ä½¿ç”¨è‡ªå®šä¹‰æç¤ºè¯
        writing_reverse_prompt = custom_writing_reverse or ""
        continuation_reverse_prompt = custom_continuation_reverse or ""
        modification_reverse_prompt = custom_modification_reverse or ""
    
    # æ£€æŸ¥è¾“å…¥ç›®å½•
    if not dir_path or not os.path.exists(dir_path):
        return None, 'è¯·è¾“å…¥æœ‰æ•ˆçš„ç›®å½•è·¯å¾„'
    
    # æ‰«æç›®å½•ä¸­çš„txtæ–‡ä»¶
    all_files = scan_dir_txts(dir_path)
    if not all_files:
        return None, 'ç›®å½•ä¸­æœªæ‰¾åˆ°TXTæ–‡ä»¶'
    
    # æ£€æŸ¥ä»»åŠ¡é€‰æ‹©
    if not task_selection:
        return None, 'è¯·è‡³å°‘é€‰æ‹©ä¸€ä¸ªä»»åŠ¡ç±»å‹'

    # æ ¹æ®ä»»åŠ¡ç±»å‹å’Œæ¨¡å¼åˆ¤æ–­æ˜¯å¦éœ€è¦AIæ¨¡å‹
    needs_ai = False
    if mode == 'AIç”Ÿæˆ':
        needs_ai = True
    elif mode == 'ä½¿ç”¨åŸæ–‡':
        # å†™ä½œå’Œä¿®æ”¹ä»»åŠ¡åœ¨ä½¿ç”¨åŸæ–‡æ¨¡å¼ä¸‹éœ€è¦AIæ¥åæ¨è¦æ±‚
        if 'å†™ä½œ' in task_selection or 'ä¿®æ”¹' in task_selection:
            needs_ai = True
    
    if needs_ai and current_model_client is None:
        return None, 'å½“å‰ä»»åŠ¡éœ€è¦AIæ¨¡å‹æ”¯æŒï¼Œè¯·å…ˆåŠ è½½AIæ¨¡å‹'

    # åˆ›å»ºè¾“å‡ºç›®å½•
    out_dir = Path(output_dir)
    out_dir.mkdir(parents=True, exist_ok=True)

    # åˆ†æ‰¹å¤„ç†é…ç½®
    BATCH_SIZE = 100  # æ¯æ‰¹å¤„ç†çš„æ ·æœ¬æ•°é‡
    
    # ä¸ºæ¯ä¸ªä»»åŠ¡ç±»å‹åˆ›å»ºå•ç‹¬çš„æ•°æ®é›†ç¼“å­˜
    datasets_cache = {task: [] for task in task_selection}
    datasets_counters = {task: 0 for task in task_selection}
    
    # ç»Ÿè®¡æ€»ç‰‡æ®µæ•°ï¼ˆæ¯ä¸ªä»»åŠ¡ç±»å‹éƒ½è¦å¤„ç†ï¼‰
    total_files = len(all_files)
    total_segments = 0
    file_segments = {}
    
    progress(0.0, desc="æ­£åœ¨æ‰«ææ–‡ä»¶ï¼Œç»Ÿè®¡å¤„ç†é‡...")
    for fp in all_files:
        txt = read_text(fp)
        segs = split_text(txt, min_len, max_len)
        file_segments[fp] = segs
        total_segments += len(segs) * len(task_selection)
    
    processed_segments = 0
    processed_files = 0
    
    # åˆ›å»ºè¾“å‡ºç›®å½•å’Œæ–‡ä»¶å¥æŸ„
    out_dir = Path(output_dir)
    out_dir.mkdir(parents=True, exist_ok=True)
    ts = time.strftime('%Y%m%d_%H%M%S')
    
    # ä¸ºæ¯ä¸ªä»»åŠ¡ç±»å‹åˆ›å»ºè¾“å‡ºæ–‡ä»¶
    output_files = {}
    file_handles = {}
    for task_type in task_selection:
        task_suffix = task_type
        out_path = out_dir / f'dataset_{task_suffix}_{mode}_{ts}.jsonl'
        output_files[task_type] = str(out_path)
        file_handles[task_type] = open(out_path, 'w', encoding='utf-8')
    
    progress(0.05, desc=f"æ‰«æå®Œæˆï¼Œå…±å‘ç° {total_files} ä¸ªæ–‡ä»¶ï¼Œ{total_segments} ä¸ªå¤„ç†ä»»åŠ¡")
    
    for file_idx, fp in enumerate(all_files):
        segs = file_segments[fp]
        file_name = os.path.basename(fp)
        
        progress(processed_segments / total_segments, desc=f"æ­£åœ¨å¤„ç†æ–‡ä»¶ {file_idx+1}/{total_files}: {file_name}")
        
        for i, seg in enumerate(segs):
            # ä¸ºæ¯ä¸ªé€‰ä¸­çš„ä»»åŠ¡ç±»å‹å¤„ç†å½“å‰æ®µè½
            for task_idx, task_type in enumerate(task_selection):
                processed_segments += 1
                progress_ratio = processed_segments / total_segments
                
                # è¯¦ç»†çš„è¿›åº¦æè¿°
                desc = f"[{processed_segments}/{total_segments}] {file_name} - {task_type} (æ®µè½ {i+1}/{len(segs)})"
                progress(progress_ratio, desc=desc)
                
                # æ ¹æ®ä»»åŠ¡ç±»å‹ç”Ÿæˆä¸åŒæ ¼å¼çš„æ•°æ®
                if task_type == 'ç»­å†™':
                     # ç»­å†™ä»»åŠ¡ï¼šå°†åŸæ–‡åˆ†æˆä¸¤éƒ¨åˆ†ï¼Œå‰åŠéƒ¨åˆ†ä½œä¸ºä¸Šæ–‡ï¼ŒååŠéƒ¨åˆ†ä½œä¸ºç»­å†™å†…å®¹
                     # å°†å½“å‰æ®µè½åˆ†æˆä¸¤éƒ¨åˆ†
                     seg_length = len(seg)
                     if seg_length < 100:  # æ®µè½å¤ªçŸ­ï¼Œè·³è¿‡
                         continue
                     
                     # æ‰¾åˆ°åˆé€‚çš„åˆ†å‰²ç‚¹ï¼ˆå°½é‡åœ¨å¥å·ã€æ„Ÿå¹å·ã€é—®å·å¤„åˆ†å‰²ï¼‰
                     split_point = seg_length // 2
                     for punct in ['ã€‚', 'ï¼', 'ï¼Ÿ', '\n']:
                         # åœ¨ä¸­é—´ä½ç½®é™„è¿‘å¯»æ‰¾æ ‡ç‚¹ç¬¦å·
                         for offset in range(0, seg_length // 4):
                             if split_point + offset < seg_length and seg[split_point + offset] == punct:
                                 split_point = split_point + offset + 1
                                 break
                             if split_point - offset > 0 and seg[split_point - offset] == punct:
                                 split_point = split_point - offset + 1
                                 break
                         else:
                             continue
                         break
                     
                     first_part = seg[:split_point].strip()
                     second_part = seg[split_point:].strip()
                     
                     if len(first_part) < 50 or len(second_part) < 50:  # åˆ†å‰²åéƒ¨åˆ†å¤ªçŸ­
                         continue
                     
                     # ç»­å†™ä»»åŠ¡ï¼šä½¿ç”¨AIæ ¹æ®åæ¨æç¤ºè¯ç”Ÿæˆç”¨æˆ·å†…å®¹
                     assistant_content = second_part
                     user_content = ai_generate_continuation_prompt_custom(first_part, user_extra, continuation_reverse_prompt)
                     
                     msgs = [
                         {"role": "system", "content": system_prompt.strip() or "ä½ æ˜¯ä¸€åä¸“ä¸šçš„å°è¯´åˆ›ä½œåŠ©æ‰‹ï¼Œèƒ½å¤Ÿæ ¹æ®ç”¨æˆ·æä¾›çš„å°è¯´ä¸Šæ–‡ï¼Œè‡ªç„¶åœ°ç»­å†™æ¥ä¸‹æ¥çš„æƒ…èŠ‚ã€‚"},
                         {"role": "user", "content": user_content},
                         {"role": "assistant", "content": assistant_content}
                     ]
                     
                     # åˆ†æ‰¹ä¿å­˜æœºåˆ¶
                     datasets_cache[task_type].append({"messages": msgs})
                     datasets_counters[task_type] += 1
                     
                     # å½“ç¼“å­˜è¾¾åˆ°æ‰¹æ¬¡å¤§å°æ—¶ï¼Œå†™å…¥æ–‡ä»¶å¹¶æ¸…ç©ºç¼“å­˜
                     if len(datasets_cache[task_type]) >= BATCH_SIZE:
                         for item in datasets_cache[task_type]:
                             file_handles[task_type].write(json.dumps(item, ensure_ascii=False) + '\n')
                         file_handles[task_type].flush()  # ç¡®ä¿æ•°æ®å†™å…¥ç£ç›˜
                         datasets_cache[task_type].clear()  # æ¸…ç©ºç¼“å­˜é‡Šæ”¾å†…å­˜
                    
                elif task_type == 'å†™ä½œ':
                     # å†™ä½œä»»åŠ¡ï¼šä½¿ç”¨AIæ ¹æ®åæ¨æç¤ºè¯ç”Ÿæˆç”¨æˆ·å†…å®¹
                     assistant_content = seg
                     if current_model_client:
                         user_content = ai_generate_writing_prompt_custom(current_model_client, assistant_content, user_extra, user_extra, writing_reverse_prompt)
                     else:
                         # å¦‚æœæ²¡æœ‰AIæ¨¡å‹ï¼Œä½¿ç”¨ç®€å•çš„é»˜è®¤æç¤º
                         user_content = user_extra.strip() or "è¯·åˆ›ä½œä¸€æ®µå°è¯´å†…å®¹ã€‚"
                     
                     msgs = [
                         {"role": "system", "content": system_prompt.strip() or "ä½ æ˜¯ä¸€åä¸“ä¸šçš„å°è¯´åˆ›ä½œåŠ©æ‰‹ï¼Œæ“…é•¿æ ¹æ®ç”¨æˆ·çš„æŒ‡ä»¤ç”Ÿæˆç”ŸåŠ¨ã€æœ‰è¶£çš„æ•…äº‹ç‰‡æ®µã€‚"},
                         {"role": "user", "content": user_content},
                         {"role": "assistant", "content": assistant_content}
                     ]
                     
                     # åˆ†æ‰¹ä¿å­˜æœºåˆ¶
                     datasets_cache[task_type].append({"messages": msgs})
                     datasets_counters[task_type] += 1
                     
                     # å½“ç¼“å­˜è¾¾åˆ°æ‰¹æ¬¡å¤§å°æ—¶ï¼Œå†™å…¥æ–‡ä»¶å¹¶æ¸…ç©ºç¼“å­˜
                     if len(datasets_cache[task_type]) >= BATCH_SIZE:
                         for item in datasets_cache[task_type]:
                             file_handles[task_type].write(json.dumps(item, ensure_ascii=False) + '\n')
                         file_handles[task_type].flush()  # ç¡®ä¿æ•°æ®å†™å…¥ç£ç›˜
                         datasets_cache[task_type].clear()  # æ¸…ç©ºç¼“å­˜é‡Šæ”¾å†…å­˜
                
                else:  # ä¿®æ”¹ä»»åŠ¡
                     # ä¿®æ”¹ä»»åŠ¡ï¼šä½¿ç”¨AIæ ¹æ®åæ¨æç¤ºè¯ç”Ÿæˆç”¨æˆ·å†…å®¹
                     assistant_content = seg
                     if current_model_client:
                         original_text, modification_request = ai_generate_modification_prompt_custom(current_model_client, seg, seg, user_extra, user_extra, modification_reverse_prompt)
                         user_content = f"è¯·æ ¹æ®ä»¥ä¸‹è¦æ±‚ä¼˜åŒ–è¿™æ®µæ–‡æœ¬ï¼š\n\nè¦æ±‚ï¼š{modification_request}\n\næ–‡æœ¬ï¼š\nã€{original_text}ã€"
                     else:
                         # å¦‚æœæ²¡æœ‰AIæ¨¡å‹ï¼Œä½¿ç”¨ç®€å•çš„é»˜è®¤ä¿®æ”¹è¦æ±‚
                         modification_request = user_extra.strip() or "è¯·ä¼˜åŒ–æ–‡ç¬”ï¼Œä½¿å…¶æ›´æµç•…ã€ç”ŸåŠ¨ä¸”ç¬¦åˆä¸­æ–‡å†™ä½œä¹ æƒ¯ã€‚"
                         user_content = f"è¯·æ ¹æ®ä»¥ä¸‹è¦æ±‚ä¼˜åŒ–è¿™æ®µæ–‡æœ¬ï¼š\n\nè¦æ±‚ï¼š{modification_request}\n\næ–‡æœ¬ï¼š\nã€{seg}ã€"
                     
                     msgs = [
                         {"role": "system", "content": system_prompt.strip() or "ä½ æ˜¯ä¸€åä¸“ä¸šçš„ç¼–è¾‘åŠ©æ‰‹ï¼Œæ“…é•¿æ ¹æ®ç”¨æˆ·çš„è¦æ±‚å¯¹æ–‡å­—è¿›è¡Œæ¶¦è‰²ã€ä¿®æ”¹å’Œä¼˜åŒ–ã€‚"},
                         {"role": "user", "content": user_content},
                         {"role": "assistant", "content": assistant_content}
                     ]
                     
                     # åˆ†æ‰¹ä¿å­˜æœºåˆ¶
                     datasets_cache[task_type].append({"messages": msgs})
                     datasets_counters[task_type] += 1
                     
                     # å½“ç¼“å­˜è¾¾åˆ°æ‰¹æ¬¡å¤§å°æ—¶ï¼Œå†™å…¥æ–‡ä»¶å¹¶æ¸…ç©ºç¼“å­˜
                     if len(datasets_cache[task_type]) >= BATCH_SIZE:
                         for item in datasets_cache[task_type]:
                             file_handles[task_type].write(json.dumps(item, ensure_ascii=False) + '\n')
                         file_handles[task_type].flush()  # ç¡®ä¿æ•°æ®å†™å…¥ç£ç›˜
                         datasets_cache[task_type].clear()  # æ¸…ç©ºç¼“å­˜é‡Šæ”¾å†…å­˜

    # å¤„ç†å‰©ä½™çš„ç¼“å­˜æ•°æ®å¹¶å…³é—­æ–‡ä»¶
    progress(0.95, desc="æ­£åœ¨ä¿å­˜å‰©ä½™æ•°æ®...")
    
    total_samples = 0
    final_output_files = []
    
    try:
        for task_type in task_selection:
            # å†™å…¥å‰©ä½™çš„ç¼“å­˜æ•°æ®
            if datasets_cache[task_type]:
                for item in datasets_cache[task_type]:
                    file_handles[task_type].write(json.dumps(item, ensure_ascii=False) + '\n')
                file_handles[task_type].flush()
                datasets_cache[task_type].clear()
            
            # å…³é—­æ–‡ä»¶å¥æŸ„
            file_handles[task_type].close()
            
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦æœ‰å†…å®¹
            if datasets_counters[task_type] > 0:
                final_output_files.append(output_files[task_type])
                total_samples += datasets_counters[task_type]
    
    except Exception as e:
        # ç¡®ä¿æ‰€æœ‰æ–‡ä»¶å¥æŸ„éƒ½è¢«å…³é—­
        for handle in file_handles.values():
            try:
                handle.close()
            except:
                pass
        raise e
    
    if not final_output_files:
        progress(1.0, desc="å¤„ç†å®Œæˆï¼Œä½†æœªç”Ÿæˆä»»ä½•æ ·æœ¬")
        return None, 'æœªç”Ÿæˆä»»ä½•æ ·æœ¬ï¼Œè¯·æ£€æŸ¥è¾“å…¥å‚æ•°'
    
    progress(1.0, desc=f"âœ… å¤„ç†å®Œæˆï¼å…±ç”Ÿæˆ {total_samples} æ¡æ ·æœ¬")
    
    # è¿”å›å®Œæ•´çš„æ–‡ä»¶è·¯å¾„ä¿¡æ¯
    file_paths_text = "\n".join([f"ğŸ“ {f}" for f in final_output_files])
    return file_paths_text, f'âœ… å·²ç”Ÿæˆ {total_samples} æ¡æ ·æœ¬ï¼Œä¿å­˜åˆ° {len(final_output_files)} ä¸ªæ–‡ä»¶'

# ==================== Gradio UI ====================

def scan_directory(dir_path: str) -> List[str]:
    """æ‰«æç›®å½•ä¸­çš„txtæ–‡ä»¶"""
    if not dir_path or not os.path.exists(dir_path):
        return []
    return scan_dir_txts(dir_path)

def create_ui():
    with gr.Blocks(title='å°è¯´æ•°æ®é›†æ„å»ºå·¥å…·', theme=gr.themes.Ocean()) as demo:
        gr.Markdown('# ğŸ“š å°è¯´æ•°æ®é›†æ„å»ºå·¥å…·\n\nå°†å°è¯´æ–‡æœ¬è½¬æ¢ä¸º JSONL messages æ ¼å¼ï¼Œç”¨äºå¤§æ¨¡å‹å¾®è°ƒ')
        
        with gr.Tabs():
            # ä¸»è¦åŠŸèƒ½æ ‡ç­¾é¡µ
            with gr.TabItem("ğŸ“– æ•°æ®é›†æ„å»º"):
                with gr.Row():
                    with gr.Column(scale=2):
                        gr.Markdown('### ğŸ“ è¾“å…¥è®¾ç½®')
                        dir_path = gr.Textbox(label='è¾“å…¥ç›®å½•è·¯å¾„', placeholder='ä¾‹ï¼šD:/novels')
                        scan_btn = gr.Button('ğŸ“‚ æ‰«æç›®å½•', variant='secondary')
                        file_list = gr.Textbox(label='å¾…å¤„ç†æ–‡ä»¶åˆ—è¡¨', lines=3, interactive=False)
                        
                        gr.Markdown('### ğŸ’¾ è¾“å‡ºè®¾ç½®')
                        output_dir = gr.Textbox(
                            label='ä¿å­˜ç›®å½•',
                            value=str(Path(__file__).parent / 'è¾“å‡ºæ•°æ®é›†'),
                            placeholder='ä¾‹ï¼šD:/output'
                        )
                        
                        gr.Markdown('### âš™ï¸ åˆ†æ®µè®¾ç½®')
                        with gr.Row():
                            min_len = gr.Number(label='æœ€å°é•¿åº¦', value=200, minimum=50)
                            max_len = gr.Number(label='æœ€å¤§é•¿åº¦', value=800, minimum=100)
                        
                        gr.Markdown('### ğŸ¯ ä»»åŠ¡è®¾ç½®')
                        gr.Markdown('**ğŸ“‹ å·¥ä½œæµç¨‹è¯´æ˜ï¼š**')
                        gr.Markdown('1. **ä½¿ç”¨åŸæ–‡** â†’ ç›´æ¥ä½¿ç”¨å°è¯´åŸæ–‡ä½œä¸ºåŠ©æ‰‹å›å¤å†…å®¹\n2. **åæ¨æç¤ºè¯** â†’ æ ¹æ®åŠ©æ‰‹å›å¤ç”Ÿæˆç”¨æˆ·é—®é¢˜\n3. æœ€ç»ˆå½¢æˆå®Œæ•´çš„å¯¹è¯æ•°æ®é›†')
                        
                        # ä»»åŠ¡é€‰æ‹©
                        task_selection = gr.CheckboxGroup(
                            choices=['å†™ä½œ', 'ç»­å†™', 'ä¿®æ”¹'],
                            label='é€‰æ‹©è¦æ‰§è¡Œçš„ä»»åŠ¡ï¼ˆå¯å¤šé€‰ï¼‰',
                            value=['ç»­å†™']
                        )
                        
                        gr.Markdown('### ğŸ”„ åæ¨æç¤ºè¯è®¾ç½®')
                        gr.Markdown('**è¯´æ˜ï¼š** åæ¨æç¤ºè¯ç”¨äºæ ¹æ®åŠ©æ‰‹å†…å®¹ç”Ÿæˆç”¨æˆ·é—®é¢˜ï¼Œå½¢æˆå®Œæ•´çš„å¯¹è¯æ•°æ®é›†')
                        
                        # åæ¨æç¤ºè¯ç±»å‹é€‰æ‹©
                        reverse_prompt_type = gr.Radio(
                            choices=['å†…ç½®', 'è‡ªå®šä¹‰'],
                            label='åæ¨æç¤ºè¯ç±»å‹',
                            value='å†…ç½®',
                            info='é€‰æ‹©ä½¿ç”¨å†…ç½®æ¨¡æ¿è¿˜æ˜¯è‡ªå®šä¹‰æç¤ºè¯'
                        )
                        
                        # å†…ç½®æç¤ºè¯è®¾ç½®ï¼ˆæ‰€æœ‰ä»»åŠ¡ç±»å‹é€šç”¨ï¼‰
                        with gr.Group(visible=True) as builtin_group:
                            gr.Markdown('**å†…ç½®æç¤ºè¯è®¾ç½®**')
                            gr.Markdown('*æ‰€æœ‰ä»»åŠ¡ç±»å‹å°†ä½¿ç”¨ç›¸åŒçš„å†…ç½®åæ¨æç¤ºè¯æ¨¡æ¿*')
                            
                            # éšè—çš„å˜é‡ï¼Œç”¨äºå…¼å®¹æ€§
                            writing_reverse_category = gr.Textbox(value='åŸºç¡€åˆ›ä½œ', visible=False)
                            continuation_reverse_category = gr.Textbox(value='æƒ…èŠ‚å»¶ç»­', visible=False)
                            modification_reverse_category = gr.Textbox(value='æ–‡æœ¬æ¶¦è‰²', visible=False)
                        
                        # è‡ªå®šä¹‰æç¤ºè¯è®¾ç½®ï¼ˆæ‰€æœ‰ä»»åŠ¡ç±»å‹é€šç”¨ï¼‰
                        with gr.Group(visible=False) as custom_group:
                            gr.Markdown('**è‡ªå®šä¹‰æç¤ºè¯è®¾ç½®**')
                            gr.Markdown('*æ‰€æœ‰ä»»åŠ¡ç±»å‹å°†ä½¿ç”¨ç›¸åŒçš„è‡ªå®šä¹‰åæ¨æç¤ºè¯*')
                            
                            # é€šç”¨è‡ªå®šä¹‰åæ¨æç¤ºè¯
                            custom_universal_reverse = gr.Textbox(
                                label='é€šç”¨åæ¨æç¤ºè¯',
                                placeholder='è¯·è¾“å…¥ç”¨äºæ‰€æœ‰ä»»åŠ¡ç±»å‹çš„åæ¨æç¤ºè¯...',
                                lines=5,
                                value='è¯·æ ¹æ®ä»¥ä¸‹å†…å®¹ï¼Œåˆ†æå…¶ç‰¹ç‚¹å’Œé£æ ¼ï¼Œç„¶åç”Ÿæˆä¸€ä¸ªè‡ªç„¶çš„ç”¨æˆ·è¯·æ±‚ã€‚ç”¨æˆ·å¸Œæœ›è·å¾—å…·æœ‰ç›¸ä¼¼ç‰¹è‰²çš„å†…å®¹ï¼Œè¯·æ¨¡æ‹Ÿç”¨æˆ·å¯èƒ½æå‡ºçš„å…·ä½“éœ€æ±‚ã€‚',
                                visible=True
                            )
                            
                            # éšè—çš„å˜é‡ï¼Œç”¨äºå…¼å®¹æ€§
                            custom_writing_reverse = gr.Textbox(visible=False)
                            custom_continuation_reverse = gr.Textbox(visible=False)
                            custom_modification_reverse = gr.Textbox(visible=False)
                        
                        # æ³¨ï¼šç°åœ¨åªä½¿ç”¨åŸæ–‡å†…å®¹ï¼ŒAIç”ŸæˆåŠŸèƒ½å·²ç§»é™¤
                        
                        system_prompt = gr.Textbox(
                            label='System æç¤ºè¯',
                            value='ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å°è¯´åˆ›ä½œåŠ©æ‰‹ï¼Œæ“…é•¿å„ç§æ–‡å­¦ä½“è£çš„å†™ä½œã€‚',
                            lines=2
                        )
                        user_extra = gr.Textbox(
                            label='ç”¨æˆ·éœ€æ±‚ä¸å…³æ³¨ç‚¹',
                            placeholder='ä¾‹ï¼šç§‘å¹»é¢˜æï¼Œæ³¨é‡äººç‰©å¿ƒç†æå†™ï¼Œä¿æŒæƒ…èŠ‚è¿è´¯æ€§å’Œæ–‡ç¬”ä¼˜ç¾åº¦',
                            lines=3,
                            info='åŒ…å«å†™ä½œä¸»é¢˜ã€ä¿®æ”¹éœ€æ±‚ã€AIåæ¨æç¤ºè¯æ—¶çš„å…³æ³¨ç‚¹ç­‰'
                        )
                        
                    with gr.Column(scale=1):
                        gr.Markdown('### ğŸ¤– AIæ¨¡å‹è®¾ç½®')
                        gr.Markdown('*è¿æ¥å‚æ•°é…ç½®è¯·åœ¨æ¨¡å‹ç®¡ç†æ ‡ç­¾é¡µä¸­å®Œæˆ*')
                        
                        # æ¨¡å‹é…ç½®é€‰æ‹©
                        all_configs = AIModelClient.get_all_configs()
                        config_choices = list(all_configs.keys())
                        selected_config = gr.Dropdown(
                            choices=config_choices,
                            label='é€‰æ‹©æ¨¡å‹é…ç½®',
                            value=config_choices[0] if config_choices else None,
                            info='è¯·å…ˆåœ¨æ¨¡å‹ç®¡ç†ä¸­é…ç½®æ¨¡å‹'
                        )
                        
                        # æ¨¡å‹æ“ä½œæŒ‰é’®
                        load_model_btn = gr.Button('ğŸ”— åŠ è½½å¹¶æµ‹è¯•æ¨¡å‹', variant='primary')
                        
                        model_status = gr.Textbox(label='æ¨¡å‹çŠ¶æ€', interactive=False, lines=2)
                        
                        gr.Markdown('### ğŸš€ æ‰§è¡Œ')
                        build_btn = gr.Button('ğŸ”¨ å¼€å§‹æ„å»º', variant='primary', size='lg')
                        
                        gr.Markdown('### ğŸ“Š è¾“å‡º')
                        output_file = gr.Textbox(label='ç”Ÿæˆçš„æ–‡ä»¶è·¯å¾„', interactive=False, lines=2)
                        logs = gr.Textbox(label='å¤„ç†æ—¥å¿—', lines=6, interactive=False)
            
            # æ¨¡å‹ç®¡ç†æ ‡ç­¾é¡µ
            with gr.TabItem("ğŸ”§ æ¨¡å‹ç®¡ç†"):
                gr.Markdown('### ğŸ’¾ ä¿å­˜æ–°çš„æ¨¡å‹é…ç½®')
                with gr.Row():
                    with gr.Column():
                        new_config_name = gr.Textbox(label='é…ç½®åç§°', placeholder='ä¾‹ï¼šæˆ‘çš„DeepSeek')
                        new_base_url = gr.Textbox(label='Base URL', placeholder='https://api.deepseek.com/v1')
                        new_api_key = gr.Textbox(label='API Key', type='password')
                        new_model_name = gr.Textbox(label='Model Name', placeholder='deepseek-chat')
                        new_timeout = gr.Number(label='è¶…æ—¶æ—¶é—´(ç§’)', value=30, minimum=10)
                        
                        save_config_btn = gr.Button('ğŸ’¾ ä¿å­˜é…ç½®', variant='primary')
                        save_config_status = gr.Textbox(label='ä¿å­˜çŠ¶æ€', interactive=False)
                
                gr.Markdown('### ğŸ“‹ ç°æœ‰é…ç½®åˆ—è¡¨')
                config_list = gr.Textbox(
                    label='å·²ä¿å­˜çš„é…ç½®',
                    value='\n'.join([f"â€¢ {name}: {config.get('base_url', 'N/A')}" for name, config in all_configs.items()]),
                    lines=8,
                    interactive=False
                )
            
            # åæ¨æç¤ºè¯ç®¡ç†æ ‡ç­¾é¡µ
            with gr.TabItem("ğŸ”„ åæ¨æç¤ºè¯ç®¡ç†"):
                gr.Markdown('### ğŸ“š å†…ç½®åæ¨æç¤ºè¯æ¨¡æ¿')
                
                # æ˜¾ç¤ºæ‰€æœ‰å†…ç½®æ¨¡æ¿
                with gr.Row():
                    template_task_type = gr.Dropdown(
                        choices=["å°è¯´å†™ä½œ", "å°è¯´ç»­å†™", "å°è¯´ä¿®æ”¹"],
                        label='æŸ¥çœ‹ä»»åŠ¡ç±»å‹',
                        value='å°è¯´å†™ä½œ'
                    )
                
                template_display = gr.Textbox(
                    label='å†…ç½®æ¨¡æ¿åˆ—è¡¨',
                    lines=10,
                    interactive=False,
                    value='\n'.join([f"â€¢ {cat}: {prompt[:80]}..." for cat, prompt in reverse_prompt_manager.predefined_prompts.get('å°è¯´å†™ä½œ', {}).items()])
                )
                
                gr.Markdown('### â• è‡ªå®šä¹‰åæ¨æç¤ºè¯ç®¡ç†')
                with gr.Row():
                    with gr.Column():
                        custom_task_type = gr.Dropdown(
                            choices=["å°è¯´å†™ä½œ", "å°è¯´ç»­å†™", "å°è¯´ä¿®æ”¹"],
                            label='ä»»åŠ¡ç±»å‹',
                            value='å°è¯´å†™ä½œ'
                        )
                        custom_category_name = gr.Textbox(label='åˆ†ç±»åç§°', placeholder='ä¾‹ï¼šæƒ…æ„Ÿæå†™')
                        custom_prompt_content = gr.Textbox(
                            label='åæ¨æç¤ºè¯å†…å®¹',
                            placeholder='è¯·è¯¦ç»†æè¿°åæ¨æç¤ºè¯...',
                            lines=4
                        )
                        
                        with gr.Row():
                            add_custom_prompt_btn = gr.Button('â• æ·»åŠ æç¤ºè¯', variant='primary')
                            delete_custom_prompt_btn = gr.Button('ğŸ—‘ï¸ åˆ é™¤æç¤ºè¯', variant='secondary')
                        
                        custom_prompt_status = gr.Textbox(label='æ“ä½œçŠ¶æ€', interactive=False)
                
                # åˆ é™¤è‡ªå®šä¹‰æç¤ºè¯çš„é€‰æ‹©
                delete_custom_dropdown = gr.Dropdown(
                    label='é€‰æ‹©è¦åˆ é™¤çš„è‡ªå®šä¹‰æç¤ºè¯',
                    choices=[],
                    value=None
                )

        
        # ==================== äº‹ä»¶ç»‘å®š ====================
        
        # ç›®å½•å¤„ç†
        scan_btn.click(
            fn=lambda path: '\n'.join(scan_directory(path)) if path else 'è¯·è¾“å…¥ç›®å½•è·¯å¾„',
            inputs=[dir_path],
            outputs=[file_list]
        )
        
        # æ³¨ï¼šåæ¨æç¤ºè¯ç®¡ç†äº‹ä»¶ç»‘å®šå·²ç§»è‡³ä¸“é—¨çš„æ ‡ç­¾é¡µ
        
        # åæ¨æç¤ºè¯ç®¡ç†æ ‡ç­¾é¡µäº‹ä»¶
        template_task_type.change(
            fn=update_template_display,
            inputs=[template_task_type],
            outputs=[template_display]
        )
        
        add_custom_prompt_btn.click(
            fn=add_custom_reverse_prompt,
            inputs=[custom_task_type, custom_category_name, custom_prompt_content],
            outputs=[custom_prompt_status, delete_custom_dropdown]
        )
        
        delete_custom_prompt_btn.click(
            fn=delete_custom_reverse_prompt,
            inputs=[delete_custom_dropdown],
            outputs=[custom_prompt_status, delete_custom_dropdown]
        )
        
        # æ¨¡å‹æ“ä½œ - ç®€åŒ–çš„åŠ è½½å‡½æ•°
        def load_selected_model(config_name):
            if not config_name:
                return "âŒ è¯·é€‰æ‹©ä¸€ä¸ªæ¨¡å‹é…ç½®"
            return load_model_config(config_name)[0]  # åªè¿”å›çŠ¶æ€æ¶ˆæ¯
        
        load_model_btn.click(
            fn=load_selected_model,
            inputs=[selected_config],
            outputs=[model_status]
        )
        
        # ä¿å­˜æ¨¡å‹é…ç½®
        save_config_btn.click(
            fn=save_model_config,
            inputs=[new_config_name, new_base_url, new_api_key, new_model_name, new_timeout],
            outputs=[save_config_status]
        )
        

        
        # åæ¨æç¤ºè¯ç±»å‹åˆ‡æ¢äº‹ä»¶
        def toggle_reverse_prompt_type(prompt_type):
            if prompt_type == 'å†…ç½®':
                return gr.update(visible=True), gr.update(visible=False)
            else:
                return gr.update(visible=False), gr.update(visible=True)
        
        reverse_prompt_type.change(
            fn=toggle_reverse_prompt_type,
            inputs=[reverse_prompt_type],
            outputs=[builtin_group, custom_group]
        )
        
        # é€šç”¨è‡ªå®šä¹‰æç¤ºè¯åŒæ­¥äº‹ä»¶
        def sync_custom_prompt(universal_prompt):
            return universal_prompt, universal_prompt, universal_prompt
        
        custom_universal_reverse.change(
            fn=sync_custom_prompt,
            inputs=[custom_universal_reverse],
            outputs=[custom_writing_reverse, custom_continuation_reverse, custom_modification_reverse]
        )
        
        # ä¸»è¦æ„å»ºåŠŸèƒ½
        build_btn.click(
            fn=build_jsonl,
            inputs=[dir_path, output_dir, min_len, max_len, task_selection, system_prompt, user_extra,
                   reverse_prompt_type, 
                   writing_reverse_category, continuation_reverse_category, modification_reverse_category,
                   custom_writing_reverse, custom_continuation_reverse, custom_modification_reverse],
            outputs=[output_file, logs]
        )
    
    return demo

def ui_app():
    return create_ui()

if __name__ == '__main__':
    ui_app().launch()
